{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgllife.utils import mol_to_bigraph, PretrainAtomFeaturizer, PretrainBondFeaturizer, AttentiveFPAtomFeaturizer, AttentiveFPBondFeaturizer, CanonicalAtomFeaturizer, CanonicalBondFeaturizer\n",
    "from dgllife.model import AttentiveFPPredictor\n",
    "from dgllife.utils import Meter, EarlyStopping, SMILESToBigraph\n",
    "import pandas as pd\n",
    "from dgllife.data import MoleculeCSVDataset\n",
    "import os\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "atom_feat = AttentiveFPAtomFeaturizer()\n",
    "bond_feat = AttentiveFPBondFeaturizer()\n",
    "\n",
    "def load_dataset(df, atom_feat, bond_feat):\n",
    "    smiles_to_g = SMILESToBigraph(add_self_loop=False, node_featurizer=atom_feat,\n",
    "                                  edge_featurizer=bond_feat)\n",
    "    dataset = MoleculeCSVDataset(df=df,\n",
    "                                 smiles_to_graph=smiles_to_g,\n",
    "                                 smiles_column='smiles',\n",
    "                                 cache_file_path= 'models/graph.bin',\n",
    "                                 task_names='rt',\n",
    "                                 n_jobs=6)\n",
    "\n",
    "    return dataset\n",
    "def collate_molgraphs(data):\n",
    "    \"\"\"Batching a list of datapoints for dataloader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list of 4-tuples.\n",
    "        Each tuple is for a single datapoint, consisting of\n",
    "        a SMILES, a DGLGraph, all-task labels and a binary\n",
    "        mask indicating the existence of labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    smiles : list\n",
    "        List of smiles\n",
    "    bg : DGLGraph\n",
    "        The batched DGLGraph.\n",
    "    labels : Tensor of dtype float32 and shape (B, T)\n",
    "        Batched datapoint labels. B is len(data) and\n",
    "        T is the number of total tasks.\n",
    "    masks : Tensor of dtype float32 and shape (B, T)\n",
    "        Batched datapoint binary mask, indicating the\n",
    "        existence of labels.\n",
    "    \"\"\"\n",
    "    smiles, graphs, labels, masks = map(list, zip(*data))\n",
    "\n",
    "    bg = dgl.batch(graphs)\n",
    "    bg.set_n_initializer(dgl.init.zero_initializer)\n",
    "    bg.set_e_initializer(dgl.init.zero_initializer)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "\n",
    "    if masks is None:\n",
    "        masks = torch.ones(labels.shape)\n",
    "    else:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "\n",
    "    return smiles, bg, labels, masks\n",
    "data_dir = '/Users/fanzhoukong/Documents/GitHub/Libgen_data/metlin_smrt'\n",
    "data_subset = pd.read_csv(os.path.join(data_dir, 'SMRT_subset.csv'))\n",
    "data_all = pd.read_csv(os.path.join(data_dir, 'SMRT_parsed.csv'))\n",
    "train_df, test_df = train_test_split(data_subset, test_size=0.1, random_state=42)\n",
    "train_data = load_dataset(train_df, atom_feat, bond_feat)\n",
    "test_data = load_dataset(test_df, atom_feat, bond_feat)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True,\n",
    "                              collate_fn=collate_molgraphs\n",
    "                              )\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False,collate_fn=collate_molgraphs)\n",
    "\n",
    "node_feat_size = atom_feat.feat_size()\n",
    "edge_feat_size = bond_feat.feat_size()\n",
    "def predict( model, bg):\n",
    "    bg = bg.to('cpu')\n",
    "    \n",
    "    \n",
    "    node_feats = bg.ndata.pop('h').to('cpu')\n",
    "    edge_feats = bg.edata.pop('e').to('cpu')\n",
    "        # return model(bg, node_feats, edge_feats)\n",
    "    return model(bg, node_feats, edge_feats)\n",
    "def train(model, data_loader,epoch, optimizer, criterion, scheduler = None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        batch_n = len(batch_data[0])\n",
    "        smiles, bg, labels, masks = batch_data\n",
    "        if len(smiles) == 1:\n",
    "            # Avoid potential issues with batch normalization\n",
    "            continue\n",
    "\n",
    "        labels, masks = labels.to('cpu'), masks.to('cpu')\n",
    "        prediction = predict( model, bg)\n",
    "        loss = (criterion(prediction, labels.view(-1, 1)) )\n",
    "        total_loss += loss.item()*batch_n\n",
    "        total_items += batch_n\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "    median_loss = total_loss / total_items\n",
    "    print(f'this is epoch {epoch},the loss is {np.sqrt(median_loss)}')\n",
    "def test(model, data_loader, loss_criterion, verbose = False, return_value = True):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    refs = []\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            smiles, bg, labels, masks = batch_data\n",
    "            refs.extend(labels)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                bg.to(torch.device('cpu'))\n",
    "                labels = labels.to('cpu')\n",
    "                masks = masks.to('cpu')\n",
    "            \n",
    "            prediction = model(bg, bg.ndata['h'], bg.edata['e'])\n",
    "            predictions.extend(prediction)\n",
    "            loss = (loss_criterion(prediction, labels.view(-1, 1)).float())\n",
    "            #loss = loss_criterion(prediction, labels)\n",
    "            losses.append(loss.data.item())\n",
    "    predictions= np.array([i.item() for i in predictions])\n",
    "    refs = np.array([i.item() for i in refs])\n",
    "    mse = loss_criterion(torch.tensor(predictions), torch.tensor(refs))\n",
    "    mae = np.median(abs(predictions-refs))\n",
    "    if verbose:\n",
    "        print(f'the rmse is {np.sqrt(mse)}, the mae is {mae}')\n",
    "    if return_value:\n",
    "        return np.sqrt(mse)\n",
    "    else:\n",
    "        return ()\n",
    "def get_mae(y_true, y_pred):\n",
    "    mae = np.median(np.abs(y_pred - y_true))\n",
    "    return mae\n",
    "def get_rmse(y_true, y_pred):\n",
    "    rmse = np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
    "    return rmse\n",
    "model = AttentiveFPPredictor(node_feat_size=node_feat_size,\n",
    "                                  edge_feat_size=edge_feat_size,\n",
    "                                  num_layers=2,\n",
    "                                  num_timesteps=2,\n",
    "                                  graph_feat_size=256,\n",
    "                                  n_tasks=1,\n",
    "                                  dropout=0.0)\n",
    "model = model.to('cpu')\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0003126662000605776,)\n",
    "max_epoch = 2\n",
    "best_val_loss = float('inf')\n",
    "saved = False\n",
    "for i in range(0, max_epoch):\n",
    "    train(model, train_loader, i, optimizer, loss_fn)\n",
    "    val_rmse = test(model, test_loader, loss_fn).item()\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(model.state_dict(), 'models/best_my_trained_attfp.pth')\n",
    "        saved = True\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break\n",
    "if saved == False:\n",
    "    torch.save(model.state_dict(), 'models/last_my_trained_attfp.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
