{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from dgllife.model import AttentiveFPPredictor\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from dl_toolsets.utilities import load_dataset,collate_molgraphs\n",
    "from torch.optim import lr_scheduler\n",
    "from dgllife.utils import mol_to_bigraph, PretrainAtomFeaturizer, PretrainBondFeaturizer, AttentiveFPAtomFeaturizer, AttentiveFPBondFeaturizer\n",
    "from dgllife.model import load_pretrained\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from dl_toolsets.utilities import make_gin_embedding, make_rdkit_descriptors, make_aft_embedding,make_ecfp,  make_clf_embeddings, attentive_fp_featurizers, make_datalist\n",
    "atom_feat = AttentiveFPAtomFeaturizer()\n",
    "bond_feat = AttentiveFPBondFeaturizer()\n",
    "node_feat_size = atom_feat.feat_size()\n",
    "edge_feat_size = bond_feat.feat_size()\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.40.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1287/1287 [00:03<00:00, 333.44it/s]\n",
      "100%|██████████| 322/322 [00:00<00:00, 335.51it/s]\n",
      "100%|██████████| 1287/1287 [00:00<00:00, 11866.24it/s]\n",
      "100%|██████████| 322/322 [00:00<00:00, 12546.83it/s]\n"
     ]
    }
   ],
   "source": [
    "train_raw = pd.read_csv('mini_dataset/train_alc.csv')\n",
    "test_raw= pd.read_csv('mini_dataset/test_alc.csv')\n",
    "train_afp = attentive_fp_featurizers(train_raw['smiles'])\n",
    "test_afp = attentive_fp_featurizers(test_raw['smiles'])\n",
    "train_ecfp = make_ecfp(train_raw['smiles'])\n",
    "test_ecfp = make_ecfp(test_raw['smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_list = make_datalist(train_afp,train_ecfp , train_raw['rt'].values)\n",
    "test_list = make_datalist(test_afp,test_ecfp , test_raw['rt'].values)\n",
    "train_loader = DataLoader(train_list, batch_size=64, shuffle=True,)\n",
    "test_loader = DataLoader(test_list, batch_size=64, shuffle=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pd.read_csv('mini_dataset/c18_all_meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.dropna(subset=['smiles'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentiveFPPredictor(node_feat_size=node_feat_size,\n",
    "                                  edge_feat_size=edge_feat_size,\n",
    "                                  num_layers=2,\n",
    "                                  num_timesteps=2,\n",
    "                                  graph_feat_size=256,\n",
    "                                  n_tasks=1,\n",
    "                                  dropout=0.0)\n",
    "model = model.to('cpu')\n",
    "model.load_state_dict(torch.load('models/best_my_ft_attfp.pth'))\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3131/3131 [00:09<00:00, 338.11it/s]\n",
      "100%|██████████| 3131/3131 [00:00<00:00, 12838.18it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_afp = attentive_fp_featurizers(validation['smiles'])\n",
    "validation_ecfp = make_ecfp(validation['smiles'])\n",
    "validation_list = make_datalist(validation_afp,validation_ecfp , validation['rt'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "validation_loader = DataLoader(validation_list, batch_size=64, shuffle=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_toolsets.super_model import test\n",
    "from dl_toolsets.utilities import predict\n",
    "\n",
    "refs, preds = test(model, validation_loader, return_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation['predicted_rt']=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = abs(validation['predicted_rt']-validation['rt'])\n",
    "validation['offset'] = offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.948932831175625"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation['offset'].quantile(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki_id</th>\n",
       "      <th>scan</th>\n",
       "      <th>rt</th>\n",
       "      <th>precursor_mz</th>\n",
       "      <th>charge</th>\n",
       "      <th>binbase-splash</th>\n",
       "      <th>user_annotation-name</th>\n",
       "      <th>key</th>\n",
       "      <th>smiles</th>\n",
       "      <th>msms</th>\n",
       "      <th>hidden</th>\n",
       "      <th>ms1_inte</th>\n",
       "      <th>adduct</th>\n",
       "      <th>method</th>\n",
       "      <th>predicted_rt</th>\n",
       "      <th>offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>aYSUJ8A/TIPJ74MR</td>\n",
       "      <td>8426</td>\n",
       "      <td>344.649973</td>\n",
       "      <td>162.113016</td>\n",
       "      <td>1</td>\n",
       "      <td>splash10-03di-1900000000-c9b8fa967f7efdaa7d2f</td>\n",
       "      <td>zz_elution wash</td>\n",
       "      <td>8426</td>\n",
       "      <td>C[N+](C)(C)CC(CC(=O)[O-])O</td>\n",
       "      <td>60.08104\\t13797.0\\n66.550606\\t1840.0\\n72.15575...</td>\n",
       "      <td>False</td>\n",
       "      <td>724619.0</td>\n",
       "      <td>[M+H]+</td>\n",
       "      <td>5m splash one premier | orbitrap | beh c18 | p...</td>\n",
       "      <td>8.459267</td>\n",
       "      <td>336.190706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>aYSUJ8A/DNR87D41</td>\n",
       "      <td>8636</td>\n",
       "      <td>338.171800</td>\n",
       "      <td>432.238599</td>\n",
       "      <td>1</td>\n",
       "      <td>splash10-014r-1900000000-87eee9822abe031b4c83</td>\n",
       "      <td>zz_elution wash</td>\n",
       "      <td>8636</td>\n",
       "      <td>CC1C(C2(C(C2(C)C)C3C1(C4C=C(C(=O)C4CC(=C3)CO)C...</td>\n",
       "      <td>69.03363\\t12384.0\\n69.98429\\t2116.0\\n87.04418\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>1014588.0</td>\n",
       "      <td>[M+ACN+H]+</td>\n",
       "      <td>5m splash one premier | orbitrap | beh c18 | p...</td>\n",
       "      <td>17.133936</td>\n",
       "      <td>321.037864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>aYSUJ8A/742NDBSA</td>\n",
       "      <td>7395</td>\n",
       "      <td>319.544638</td>\n",
       "      <td>195.123319</td>\n",
       "      <td>1</td>\n",
       "      <td>splash10-000i-9600000000-99d631a0031434419d4e</td>\n",
       "      <td>zz_no peak</td>\n",
       "      <td>7395</td>\n",
       "      <td>C(COCCOCCOCCO)O</td>\n",
       "      <td>73.0648\\t2538.0\\n78.61735\\t2356.0\\n78.63006\\t2...</td>\n",
       "      <td>False</td>\n",
       "      <td>2609287.0</td>\n",
       "      <td>[M+H]+</td>\n",
       "      <td>5m splash one premier | orbitrap | beh c18 | p...</td>\n",
       "      <td>10.945576</td>\n",
       "      <td>308.599062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>aYSUJ8A/Q310O1JL</td>\n",
       "      <td>11288</td>\n",
       "      <td>308.818417</td>\n",
       "      <td>1178.779993</td>\n",
       "      <td>1</td>\n",
       "      <td>splash10-0059-0392212000-32fda3f29d849a5e912e</td>\n",
       "      <td>zz_elution wash sodium formate cluster</td>\n",
       "      <td>11288</td>\n",
       "      <td>C(=O)[O-].[Na+]</td>\n",
       "      <td>226.95148\\t30948.0\\n362.92578\\t7869.0\\n430.914...</td>\n",
       "      <td>False</td>\n",
       "      <td>374655.0</td>\n",
       "      <td>M+H</td>\n",
       "      <td>5m splash one premier | orbitrap | beh c18 | p...</td>\n",
       "      <td>5.031274</td>\n",
       "      <td>303.787143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>aYSUJ8A/4QMCM6ZW</td>\n",
       "      <td>11240</td>\n",
       "      <td>308.569414</td>\n",
       "      <td>974.814664</td>\n",
       "      <td>1</td>\n",
       "      <td>splash10-004i-0193400000-17113cfe9a0ab4d8e108</td>\n",
       "      <td>zz_elution wash sodium formate cluster</td>\n",
       "      <td>11240</td>\n",
       "      <td>C(=O)[O-].[Na+]</td>\n",
       "      <td>158.9642\\t26441.0\\n196.564\\t3296.0\\n201.7384\\t...</td>\n",
       "      <td>False</td>\n",
       "      <td>703061.0</td>\n",
       "      <td>M+H</td>\n",
       "      <td>5m splash one premier | orbitrap | beh c18 | p...</td>\n",
       "      <td>5.031274</td>\n",
       "      <td>303.538140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>aA9N54F/YXCQNY06</td>\n",
       "      <td>383</td>\n",
       "      <td>12.555635</td>\n",
       "      <td>285.043585</td>\n",
       "      <td>-1</td>\n",
       "      <td>splash10-000i-0090000000-af5dd60606f1340e87dc</td>\n",
       "      <td>oxazepam</td>\n",
       "      <td>383</td>\n",
       "      <td>C1=CC=C(C=C1)C2=NC(C(=O)NC3=C2C=C(C=C3)Cl)O</td>\n",
       "      <td>59.013607\\t23216.0\\n61.51781\\t11471.0\\n71.2598...</td>\n",
       "      <td>False</td>\n",
       "      <td>6558037.0</td>\n",
       "      <td>[M-H]-</td>\n",
       "      <td>5m splash one premier | orbitrap | beh c18 | n...</td>\n",
       "      <td>12.570807</td>\n",
       "      <td>0.015172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>aYSUJ8A/PSVAL84T</td>\n",
       "      <td>479</td>\n",
       "      <td>157.303553</td>\n",
       "      <td>798.636551</td>\n",
       "      <td>1</td>\n",
       "      <td>splash10-001i-0900000100-ad719b29c03be82c6c76</td>\n",
       "      <td>PC O-18:0_20:3</td>\n",
       "      <td>479</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCOC[C@H](COP(=O)([O-])OCC[N+]...</td>\n",
       "      <td>86.09641\\t279509.0\\n88.738846\\t31221.0\\n104.10...</td>\n",
       "      <td>False</td>\n",
       "      <td>9801641.0</td>\n",
       "      <td>M+H</td>\n",
       "      <td>5m splash one premier | orbitrap | beh c18 | p...</td>\n",
       "      <td>157.313049</td>\n",
       "      <td>0.009497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>aYSUJ8A/K0UBQU8I</td>\n",
       "      <td>497</td>\n",
       "      <td>234.947311</td>\n",
       "      <td>898.785521</td>\n",
       "      <td>1</td>\n",
       "      <td>splash10-0udi-0000009000-c9c8ec57f507a6b645bb</td>\n",
       "      <td>TG 18:1_18:2_18:2</td>\n",
       "      <td>497</td>\n",
       "      <td>CCCCCCCC/C=C\\CCCCCCCC(=O)OC[C@H](COC(=O)CCCCCC...</td>\n",
       "      <td>95.08573\\t430938.0\\n109.10109\\t226722.0\\n123.1...</td>\n",
       "      <td>False</td>\n",
       "      <td>157882784.0</td>\n",
       "      <td>M+NH4</td>\n",
       "      <td>5m splash one premier | orbitrap | beh c18 | p...</td>\n",
       "      <td>234.954529</td>\n",
       "      <td>0.007218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>aYSUJ8A/UHL1CBKE</td>\n",
       "      <td>1057</td>\n",
       "      <td>11.862608</td>\n",
       "      <td>301.136877</td>\n",
       "      <td>1</td>\n",
       "      <td>splash10-0udi-4019000000-b52168e1bff6065c27e5</td>\n",
       "      <td>promethazine sulfoxide</td>\n",
       "      <td>1057</td>\n",
       "      <td>CC(CN1C2=CC=CC=C2S(=O)C3=CC=CC=C31)N(C)C</td>\n",
       "      <td>55.635628\\t2411.0\\n71.07291\\t3630.0\\n72.08015\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>1588638.0</td>\n",
       "      <td>[M+H]+</td>\n",
       "      <td>5m splash one premier | orbitrap | beh c18 | p...</td>\n",
       "      <td>11.859269</td>\n",
       "      <td>0.003338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>aA9N54F/M41XP95Z</td>\n",
       "      <td>13642</td>\n",
       "      <td>9.300964</td>\n",
       "      <td>134.046738</td>\n",
       "      <td>-1</td>\n",
       "      <td>splash10-001i-0900000000-aa58211590ed1a3398e1</td>\n",
       "      <td>cAMP</td>\n",
       "      <td>13642</td>\n",
       "      <td>C1=NC2=NC=NC(=C2N1)N</td>\n",
       "      <td>50.40041\\t6181.0\\n68.75553\\t5640.0\\n71.6818\\t4...</td>\n",
       "      <td>False</td>\n",
       "      <td>5500001.0</td>\n",
       "      <td>[M-H]-</td>\n",
       "      <td>5m splash one premier | orbitrap | beh c18 | n...</td>\n",
       "      <td>9.300609</td>\n",
       "      <td>0.000355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3131 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               wiki_id   scan          rt  precursor_mz  charge  \\\n",
       "1285  aYSUJ8A/TIPJ74MR   8426  344.649973    162.113016       1   \n",
       "1312  aYSUJ8A/DNR87D41   8636  338.171800    432.238599       1   \n",
       "1189  aYSUJ8A/742NDBSA   7395  319.544638    195.123319       1   \n",
       "1480  aYSUJ8A/Q310O1JL  11288  308.818417   1178.779993       1   \n",
       "1478  aYSUJ8A/4QMCM6ZW  11240  308.569414    974.814664       1   \n",
       "...                ...    ...         ...           ...     ...   \n",
       "2011  aA9N54F/YXCQNY06    383   12.555635    285.043585      -1   \n",
       "295   aYSUJ8A/PSVAL84T    479  157.303553    798.636551       1   \n",
       "305   aYSUJ8A/K0UBQU8I    497  234.947311    898.785521       1   \n",
       "480   aYSUJ8A/UHL1CBKE   1057   11.862608    301.136877       1   \n",
       "3172  aA9N54F/M41XP95Z  13642    9.300964    134.046738      -1   \n",
       "\n",
       "                                     binbase-splash  \\\n",
       "1285  splash10-03di-1900000000-c9b8fa967f7efdaa7d2f   \n",
       "1312  splash10-014r-1900000000-87eee9822abe031b4c83   \n",
       "1189  splash10-000i-9600000000-99d631a0031434419d4e   \n",
       "1480  splash10-0059-0392212000-32fda3f29d849a5e912e   \n",
       "1478  splash10-004i-0193400000-17113cfe9a0ab4d8e108   \n",
       "...                                             ...   \n",
       "2011  splash10-000i-0090000000-af5dd60606f1340e87dc   \n",
       "295   splash10-001i-0900000100-ad719b29c03be82c6c76   \n",
       "305   splash10-0udi-0000009000-c9c8ec57f507a6b645bb   \n",
       "480   splash10-0udi-4019000000-b52168e1bff6065c27e5   \n",
       "3172  splash10-001i-0900000000-aa58211590ed1a3398e1   \n",
       "\n",
       "                        user_annotation-name    key  \\\n",
       "1285                         zz_elution wash   8426   \n",
       "1312                         zz_elution wash   8636   \n",
       "1189                              zz_no peak   7395   \n",
       "1480  zz_elution wash sodium formate cluster  11288   \n",
       "1478  zz_elution wash sodium formate cluster  11240   \n",
       "...                                      ...    ...   \n",
       "2011                                oxazepam    383   \n",
       "295                           PC O-18:0_20:3    479   \n",
       "305                        TG 18:1_18:2_18:2    497   \n",
       "480                   promethazine sulfoxide   1057   \n",
       "3172                                    cAMP  13642   \n",
       "\n",
       "                                                 smiles  \\\n",
       "1285                         C[N+](C)(C)CC(CC(=O)[O-])O   \n",
       "1312  CC1C(C2(C(C2(C)C)C3C1(C4C=C(C(=O)C4CC(=C3)CO)C...   \n",
       "1189                                    C(COCCOCCOCCO)O   \n",
       "1480                                    C(=O)[O-].[Na+]   \n",
       "1478                                    C(=O)[O-].[Na+]   \n",
       "...                                                 ...   \n",
       "2011        C1=CC=C(C=C1)C2=NC(C(=O)NC3=C2C=C(C=C3)Cl)O   \n",
       "295   CCCCCCCCCCCCCCCCCCOC[C@H](COP(=O)([O-])OCC[N+]...   \n",
       "305   CCCCCCCC/C=C\\CCCCCCCC(=O)OC[C@H](COC(=O)CCCCCC...   \n",
       "480            CC(CN1C2=CC=CC=C2S(=O)C3=CC=CC=C31)N(C)C   \n",
       "3172                               C1=NC2=NC=NC(=C2N1)N   \n",
       "\n",
       "                                                   msms hidden     ms1_inte  \\\n",
       "1285  60.08104\\t13797.0\\n66.550606\\t1840.0\\n72.15575...  False     724619.0   \n",
       "1312  69.03363\\t12384.0\\n69.98429\\t2116.0\\n87.04418\\...  False    1014588.0   \n",
       "1189  73.0648\\t2538.0\\n78.61735\\t2356.0\\n78.63006\\t2...  False    2609287.0   \n",
       "1480  226.95148\\t30948.0\\n362.92578\\t7869.0\\n430.914...  False     374655.0   \n",
       "1478  158.9642\\t26441.0\\n196.564\\t3296.0\\n201.7384\\t...  False     703061.0   \n",
       "...                                                 ...    ...          ...   \n",
       "2011  59.013607\\t23216.0\\n61.51781\\t11471.0\\n71.2598...  False    6558037.0   \n",
       "295   86.09641\\t279509.0\\n88.738846\\t31221.0\\n104.10...  False    9801641.0   \n",
       "305   95.08573\\t430938.0\\n109.10109\\t226722.0\\n123.1...  False  157882784.0   \n",
       "480   55.635628\\t2411.0\\n71.07291\\t3630.0\\n72.08015\\...  False    1588638.0   \n",
       "3172  50.40041\\t6181.0\\n68.75553\\t5640.0\\n71.6818\\t4...  False    5500001.0   \n",
       "\n",
       "          adduct                                             method  \\\n",
       "1285      [M+H]+  5m splash one premier | orbitrap | beh c18 | p...   \n",
       "1312  [M+ACN+H]+  5m splash one premier | orbitrap | beh c18 | p...   \n",
       "1189      [M+H]+  5m splash one premier | orbitrap | beh c18 | p...   \n",
       "1480         M+H  5m splash one premier | orbitrap | beh c18 | p...   \n",
       "1478         M+H  5m splash one premier | orbitrap | beh c18 | p...   \n",
       "...          ...                                                ...   \n",
       "2011      [M-H]-  5m splash one premier | orbitrap | beh c18 | n...   \n",
       "295          M+H  5m splash one premier | orbitrap | beh c18 | p...   \n",
       "305        M+NH4  5m splash one premier | orbitrap | beh c18 | p...   \n",
       "480       [M+H]+  5m splash one premier | orbitrap | beh c18 | p...   \n",
       "3172      [M-H]-  5m splash one premier | orbitrap | beh c18 | n...   \n",
       "\n",
       "      predicted_rt      offset  \n",
       "1285      8.459267  336.190706  \n",
       "1312     17.133936  321.037864  \n",
       "1189     10.945576  308.599062  \n",
       "1480      5.031274  303.787143  \n",
       "1478      5.031274  303.538140  \n",
       "...            ...         ...  \n",
       "2011     12.570807    0.015172  \n",
       "295     157.313049    0.009497  \n",
       "305     234.954529    0.007218  \n",
       "480      11.859269    0.003338  \n",
       "3172      9.300609    0.000355  \n",
       "\n",
       "[3131 rows x 16 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.sort_values(by = 'offset', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(validation['offset'], kde=True)\n",
    "plt.xlabel('Predicted RT offset')\n",
    "plt.axvline(validation['offset'].quantile(0.8), color='red',\n",
    "            #  dashes = '--'\n",
    "             )\n",
    "plt.savefig('mass_wiki_rt_offset.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = []\n",
    "for index, row in validation.iterrows():\n",
    "    if row['offset'] < 13:\n",
    "        tag.append(True)\n",
    "    else:\n",
    "        tag.append(False)\n",
    "validation['tag'] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = validation[validation['user_annotation-name'].str.contains('zz|yy', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       253\n",
       "unique        2\n",
       "top       False\n",
       "freq        211\n",
       "Name: tag, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatives['tag'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8611111111111112"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "217/252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '/Users/fanzhoukong/Documents/GitHub/Libgen_data/metlin_smrt'\n",
    "# data_subset = pd.read_csv(os.path.join(data_dir, 'SMRT_parsed.csv'))\n",
    "# # data_all = pd.read_csv(os.path.join(data_dir, 'SMRT_parsed.csv'))\n",
    "# train_df, test_df = train_test_split(data_subset, test_size=0.1, random_state=42)\n",
    "# train_data = load_dataset(train_df, atom_feat, bond_feat)\n",
    "# test_data = load_dataset(test_df, atom_feat, bond_feat)\n",
    "# train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True,\n",
    "#                               collate_fn=collate_molgraphs\n",
    "#                               )\n",
    "# test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False,collate_fn=collate_molgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_toolsets.super_model import train, test\n",
    "from dl_toolsets.utilities import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw= pd.read_csv('mini_dataset/train_alc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,9):\n",
    "    _, dataset = train_test_split(train_raw, test_size=((i+1 )/ 10))\n",
    "    dataset.to_csv('mini_dataset/train_alc_'+str(i+1)+'.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_raw.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCCCCCCC=CCCCCCCCC(=O)OCC(COC(=O)CCCCCCCC=CCC...</td>\n",
       "      <td>248.437120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCCCCCCCCCCCCCCCC(=O)OC(COCCCCCCCCCCCCCC)COP(=...</td>\n",
       "      <td>123.921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCCCC=CCCCCCCCCCCCCOCC(O)COP(=O)(O)OCCN</td>\n",
       "      <td>95.132667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C1=CC=C(C(=C1)C(=O)O)C(=O)O</td>\n",
       "      <td>9.680105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCCCN(CCC(=O)OCC)C(=O)C</td>\n",
       "      <td>13.259751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>CCC=CCC=CCC=CCC=CCCCCCCCCC(=O)OC(COC(=O)CCCCCC...</td>\n",
       "      <td>114.439427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>C(COCCOCCOCCOCCOCCO)O</td>\n",
       "      <td>11.729926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>CCCCCCCCCCCCCCCC=CC(O)C(CO)NC(=O)CCCCCCCCCCCCC...</td>\n",
       "      <td>224.795132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>CCCC=CCC=CCCCCCCCC(=O)OCCCCCCCCC=CCC=CCCC(=O)N...</td>\n",
       "      <td>177.715541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>CCCCCCCCCCCCCCCCCCC(=O)OCC(COP(=O)(O)OCCN)OC(=...</td>\n",
       "      <td>141.913978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1287 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles          rt\n",
       "0     CCCCCCCCC=CCCCCCCCC(=O)OCC(COC(=O)CCCCCCCC=CCC...  248.437120\n",
       "1     CCCCCCCCCCCCCCCCC(=O)OC(COCCCCCCCCCCCCCC)COP(=...  123.921500\n",
       "2         CCCCCCCCCCC=CCCCCCCCCCCCCOCC(O)COP(=O)(O)OCCN   95.132667\n",
       "3                           C1=CC=C(C(=C1)C(=O)O)C(=O)O    9.680105\n",
       "4                               CCCCN(CCC(=O)OCC)C(=O)C   13.259751\n",
       "...                                                 ...         ...\n",
       "1282  CCC=CCC=CCC=CCC=CCCCCCCCCC(=O)OC(COC(=O)CCCCCC...  114.439427\n",
       "1283                              C(COCCOCCOCCOCCOCCO)O   11.729926\n",
       "1284  CCCCCCCCCCCCCCCC=CC(O)C(CO)NC(=O)CCCCCCCCCCCCC...  224.795132\n",
       "1285  CCCC=CCC=CCCCCCCCC(=O)OCCCCCCCCC=CCC=CCCC(=O)N...  177.715541\n",
       "1286  CCCCCCCCCCCCCCCCCCC(=O)OCC(COP(=O)(O)OCCN)OC(=...  141.913978\n",
       "\n",
       "[1287 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_size = len(df) // 10\n",
    "\n",
    "# Create a list to hold the 10 parts\n",
    "parts = []\n",
    "for i in range(10):\n",
    "    if i < 9:\n",
    "        part = df.iloc[i * part_size:(i + 1) * part_size]\n",
    "    else:\n",
    "        part = df.iloc[i * part_size:]  # Last part gets the remaining rows\n",
    "    parts.append(part)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(parts)):\n",
    "    out = pd.DataFrame()\n",
    "    for j in range(i+1):\n",
    "        out = pd.concat([out, parts[j]])\n",
    "        out.to_csv('mini_dataset/train_alc_'+str(i+1)+'.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets try fine tunning first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_toolsets.super_model import train, test\n",
    "from dl_toolsets.utilities import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:03<00:00, 328.22it/s]\n",
      "100%|██████████| 1152/1152 [00:00<00:00, 12767.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, RMSE: 1452.0852331407946\n",
      "Epoch 1, RMSE: 491.72786871907107\n",
      "Epoch 2, RMSE: 255.71157144153102\n",
      "Epoch 3, RMSE: 163.52027527296798\n",
      "Epoch 4, RMSE: 80.4475106813529\n",
      "Epoch 5, RMSE: 49.81752163070521\n",
      "Epoch 6, RMSE: 38.46147714358928\n",
      "Epoch 7, RMSE: 32.88017574513935\n",
      "Epoch 8, RMSE: 29.146620715478225\n",
      "Epoch 9, RMSE: 26.533211455712205\n",
      "Epoch 10, RMSE: 24.807573787674254\n",
      "Epoch 11, RMSE: 23.639668355309084\n",
      "Epoch 12, RMSE: 22.096131422590723\n",
      "Epoch 13, RMSE: 20.946230863055895\n",
      "Epoch 14, RMSE: 20.18838760424529\n",
      "Epoch 15, RMSE: 19.451939318506728\n",
      "Epoch 16, RMSE: 18.728870789790168\n",
      "Epoch 17, RMSE: 18.21664029718918\n",
      "Epoch 18, RMSE: 17.96495357785006\n",
      "Epoch 19, RMSE: 18.029360361618473\n",
      "Epoch 20, RMSE: 17.230901689975127\n",
      "Epoch 21, RMSE: 17.08046962647285\n",
      "Epoch 22, RMSE: 16.57198358127293\n",
      "Epoch 23, RMSE: 16.352538203905166\n",
      "Epoch 24, RMSE: 16.148502199826375\n",
      "Epoch 25, RMSE: 15.73826043194998\n",
      "Epoch 26, RMSE: 15.771056294177464\n",
      "Epoch 27, RMSE: 16.64612912331797\n",
      "Epoch 28, RMSE: 15.378360563663177\n",
      "Epoch 29, RMSE: 15.310462522003318\n",
      "Epoch 30, RMSE: 14.773938410861932\n",
      "Epoch 31, RMSE: 14.532757402668388\n",
      "Epoch 32, RMSE: 14.697208855210588\n",
      "Epoch 33, RMSE: 14.554040520972556\n",
      "Epoch 34, RMSE: 14.381840118778062\n",
      "Epoch 35, RMSE: 14.307638409697715\n",
      "Epoch 36, RMSE: 14.13888016363015\n",
      "Epoch 37, RMSE: 14.013581575759206\n",
      "Epoch 38, RMSE: 13.828497642769115\n",
      "Epoch 39, RMSE: 13.727818964645069\n",
      "Epoch 40, RMSE: 13.671724810062955\n",
      "Epoch 41, RMSE: 13.46273243659281\n",
      "Epoch 42, RMSE: 14.025179638527197\n",
      "Epoch 43, RMSE: 13.632603210572151\n",
      "Epoch 44, RMSE: 13.701330651526153\n",
      "Epoch 45, RMSE: 13.225776281971857\n",
      "Epoch 46, RMSE: 13.487835273743386\n",
      "Epoch 47, RMSE: 13.878707970695444\n",
      "Epoch 48, RMSE: 13.359606828468937\n",
      "Epoch 49, RMSE: 13.316778292484505\n",
      "Epoch 50, RMSE: 12.856774880090068\n",
      "Epoch 51, RMSE: 12.871897977029654\n",
      "Epoch 52, RMSE: 13.210508462285855\n",
      "Epoch 53, RMSE: 12.9030875333176\n",
      "Epoch 54, RMSE: 12.84354797517903\n",
      "Epoch 55, RMSE: 13.109135302864468\n",
      "Epoch 56, RMSE: 13.658833546194119\n",
      "Epoch 57, RMSE: 12.549209860979346\n",
      "Epoch 58, RMSE: 12.638392949883249\n",
      "Epoch 59, RMSE: 12.97304720890773\n",
      "Epoch 60, RMSE: 12.853306012474139\n",
      "Epoch 61, RMSE: 12.603817988004899\n",
      "Epoch 62, RMSE: 13.070940388597897\n",
      "Epoch 63, RMSE: 12.620268536273425\n",
      "Epoch 64, RMSE: 12.3054775085347\n",
      "Epoch 65, RMSE: 12.323652420157188\n",
      "Epoch 66, RMSE: 12.239109033363317\n",
      "early stopping triggered\n",
      "9\n",
      "the testing rmse is 15.049352747322406, the mae is 5.749393463134766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(15.0494, dtype=torch.float64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AttentiveFPPredictor(node_feat_size=node_feat_size,\n",
    "                                  edge_feat_size=edge_feat_size,\n",
    "                                  num_layers=2,\n",
    "                                  num_timesteps=2,\n",
    "                                  graph_feat_size=256,\n",
    "                                  n_tasks=1,\n",
    "                                  dropout=0.0)\n",
    "model = model.to('cpu')\n",
    "loss_fn = nn.MSELoss()\n",
    "partition_size = 9\n",
    "train_raw = pd.read_csv('mini_dataset/train_alc_'+str(partition_size)+'.csv')\n",
    "print(len(train_raw))\n",
    "train_afp = attentive_fp_featurizers(train_raw['smiles'])\n",
    "\n",
    "train_ecfp = make_ecfp(train_raw['smiles'])\n",
    "train_list = make_datalist(train_afp,train_ecfp , train_raw['rt'].values)\n",
    "train_loader = DataLoader(train_list, batch_size=64, shuffle=True,)\n",
    "\n",
    "model.load_state_dict(torch.load('models/best_my_trained_attfp.pth'))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, \n",
    "                             weight_decay=0.001,\n",
    "                             )\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "max_epoch = 500\n",
    "best_val_loss = float('inf')\n",
    "for i in range(0, max_epoch):\n",
    "    train(i, model, train_loader, loss_fn, optimizer)\n",
    "    val_rmse = test(model, test_loader)\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(model.state_dict(), 'models/best_my_ft_attfp0903_'+str(partition_size)+'.pth')\n",
    "        saved = True\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break\n",
    "print(partition_size)\n",
    "model.load_state_dict(torch.load('models/best_my_ft_attfp0903_'+str(partition_size)+'.pth'))\n",
    "# model.load_state_dict(torch.load('models/best_my_ft_attfp.pth'))\n",
    "# test(model, train_loader, verbose=True)\n",
    "test(model, test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the testing rmse is 12.233930440224375, the mae is 4.697955846786499\n",
      "the testing rmse is 14.870879551849672, the mae is 5.755428314208984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14.8709, dtype=torch.float64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/best_my_ft_attfp0903_'+str(partition_size)+'.pth'))\n",
    "# model.load_state_dict(torch.load('models/best_my_ft_attfp.pth'))\n",
    "test(model, train_loader, verbose=True)\n",
    "test(model, test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the testing rmse is 11.602343619646685, the mae is 3.8681640625\n",
      "the testing rmse is 13.233695869261458, the mae is 3.962677001953125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(13.2337, dtype=torch.float64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('models/best_my_ft_attfp0903.pth'))\n",
    "model.load_state_dict(torch.load('models/best_my_ft_attfp.pth'))\n",
    "test(model, train_loader, verbose=True)\n",
    "test(model, test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m exp_lr_scheduler \u001b[38;5;241m=\u001b[39m lr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, max_epoch):\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     val_rmse \u001b[38;5;241m=\u001b[39m test(model, test_loader, loss_fn)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_rmse \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "File \u001b[0;32m~/Documents/GitHub/Rhoeto/dl_toolsets/utilities.py:105\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, epoch, optimizer, criterion, scheduler)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m    104\u001b[0m     batch_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_data[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 105\u001b[0m     smiles, bg, labels, masks \u001b[38;5;241m=\u001b[39m batch_data\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(smiles) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;66;03m# Avoid potential issues with batch normalization\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "num_ftrs = 256\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
    "model.predict = nn.Sequential(\n",
    "    nn.Dropout(p = 0.0, inplace=False),\n",
    "    nn.Linear(num_ftrs, 1, bias = True),\n",
    "                            )\n",
    "\n",
    "model = model.to('cpu')\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0003126662000605776,)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "max_epoch = 500\n",
    "best_val_loss = float('inf')\n",
    "saved = False\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "for i in range(0, max_epoch):\n",
    "    train(model, train_loader, i, optimizer, loss_fn)\n",
    "    val_rmse = test(model, test_loader, loss_fn).item()\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(model.state_dict(), 'models/best_my_ft_attfp.pth')\n",
    "        saved = True\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training statistics are: \n",
      "the rmse is 11.602343746135222, the mae is 3.8681640625\n",
      "the testing statistics are: \n",
      "the rmse is 13.233696220919093, the mae is 3.962677001953125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/best_my_ft_attfp.pth'))\n",
    "print('the training statistics are: ')\n",
    "test(model, train_loader, loss_fn, verbose = True, return_rmse=False)\n",
    "print('the testing statistics are: ')\n",
    "test(model, test_loader, loss_fn, verbose = True,return_rmse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this time we use our pretrained model as an fixed embedding maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(os.path.join('mini_dataset/train_alc.csv'))\n",
    "test_raw = pd.read_csv(os.path.join('mini_dataset/test_alc.csv'))\n",
    "# train_alc = load_dataset(train_raw, atom_feat, bond_feat)\n",
    "# test_alc = load_dataset(test_raw, atom_feat, bond_feat)\n",
    "# train_loader = DataLoader(dataset=train_alc, batch_size=64, shuffle=True,\n",
    "#                               collate_fn=collate_molgraphs\n",
    "#                               )\n",
    "# test_loader = DataLoader(dataset=test_alc, batch_size=64, shuffle=False,collate_fn=collate_molgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is epoch 0,the loss is 136.75684578578029\n",
      "this is epoch 1,the loss is 122.01509109763951\n",
      "this is epoch 2,the loss is 107.01716494516268\n",
      "this is epoch 3,the loss is 92.00433059819386\n",
      "this is epoch 4,the loss is 77.82362066037066\n",
      "this is epoch 5,the loss is 64.47863516052294\n",
      "this is epoch 6,the loss is 53.624322587434925\n",
      "this is epoch 7,the loss is 45.87867326921208\n",
      "this is epoch 8,the loss is 41.56333635254691\n",
      "this is epoch 9,the loss is 39.35598471541167\n",
      "this is epoch 10,the loss is 38.368926748996415\n",
      "this is epoch 11,the loss is 37.89249257321862\n",
      "this is epoch 12,the loss is 37.58098209947162\n",
      "this is epoch 13,the loss is 37.21399956552332\n",
      "this is epoch 14,the loss is 36.82313715251263\n",
      "this is epoch 15,the loss is 36.42177296199461\n",
      "this is epoch 16,the loss is 36.02282124285961\n",
      "this is epoch 17,the loss is 35.6455347405272\n",
      "this is epoch 18,the loss is 35.23198071232383\n",
      "this is epoch 19,the loss is 34.80233925736848\n",
      "this is epoch 20,the loss is 34.40833646677312\n",
      "this is epoch 21,the loss is 33.93181920158493\n",
      "this is epoch 22,the loss is 33.46050061491725\n",
      "this is epoch 23,the loss is 33.01711728069439\n",
      "this is epoch 24,the loss is 32.564987662943125\n",
      "this is epoch 25,the loss is 32.12878573300867\n",
      "this is epoch 26,the loss is 31.727116085129545\n",
      "this is epoch 27,the loss is 31.285561997838258\n",
      "this is epoch 28,the loss is 30.873691904654418\n",
      "this is epoch 29,the loss is 30.512503142270965\n",
      "this is epoch 30,the loss is 30.038257906782874\n",
      "this is epoch 31,the loss is 29.650653249836033\n",
      "this is epoch 32,the loss is 29.273680304445644\n",
      "this is epoch 33,the loss is 28.85742790346253\n",
      "this is epoch 34,the loss is 28.464718028859746\n",
      "this is epoch 35,the loss is 28.08055915895128\n",
      "this is epoch 36,the loss is 27.740377806527317\n",
      "this is epoch 37,the loss is 27.42526692055518\n",
      "this is epoch 38,the loss is 27.129325204725653\n",
      "this is epoch 39,the loss is 26.838306830718096\n",
      "this is epoch 40,the loss is 26.52131451556313\n",
      "this is epoch 41,the loss is 26.223540123629093\n",
      "this is epoch 42,the loss is 25.980486188127387\n",
      "this is epoch 43,the loss is 25.77167831905086\n",
      "this is epoch 44,the loss is 25.50041966404114\n",
      "this is epoch 45,the loss is 25.34288291154733\n",
      "this is epoch 46,the loss is 25.06145464445832\n",
      "this is epoch 47,the loss is 24.95670745616638\n",
      "this is epoch 48,the loss is 24.731516042591306\n",
      "this is epoch 49,the loss is 24.703980326099142\n",
      "this is epoch 50,the loss is 24.488670281077752\n",
      "this is epoch 51,the loss is 24.315820933331185\n",
      "this is epoch 52,the loss is 24.18220162758039\n",
      "this is epoch 53,the loss is 24.086768768331538\n",
      "this is epoch 54,the loss is 23.973166908750713\n",
      "this is epoch 55,the loss is 23.90436941225817\n",
      "this is epoch 56,the loss is 23.77035843422742\n",
      "this is epoch 57,the loss is 23.720295467554077\n",
      "this is epoch 58,the loss is 23.658942736691696\n",
      "this is epoch 59,the loss is 23.64457508326544\n",
      "this is epoch 60,the loss is 23.50630575890078\n",
      "this is epoch 61,the loss is 23.498793098662084\n",
      "this is epoch 62,the loss is 23.49897698980788\n",
      "this is epoch 63,the loss is 23.33124749473321\n",
      "this is epoch 64,the loss is 23.32481183042591\n",
      "this is epoch 65,the loss is 23.222726773567505\n",
      "this is epoch 66,the loss is 23.200448344049935\n",
      "this is epoch 67,the loss is 23.121588978058462\n",
      "this is epoch 68,the loss is 23.062071147129664\n",
      "this is epoch 69,the loss is 23.0490670472646\n",
      "this is epoch 70,the loss is 23.0653738209334\n",
      "this is epoch 71,the loss is 22.996506438508366\n",
      "this is epoch 72,the loss is 22.93263268630113\n",
      "this is epoch 73,the loss is 22.896811940468623\n",
      "this is epoch 74,the loss is 22.87186035760667\n",
      "this is epoch 75,the loss is 22.831542861164575\n",
      "this is epoch 76,the loss is 22.852138581535815\n",
      "this is epoch 77,the loss is 22.77480362118251\n",
      "this is epoch 78,the loss is 22.753161813813623\n",
      "this is epoch 79,the loss is 22.74995712077806\n",
      "this is epoch 80,the loss is 22.737730278487973\n",
      "this is epoch 81,the loss is 22.662969619613307\n",
      "this is epoch 82,the loss is 22.633281313579666\n",
      "this is epoch 83,the loss is 22.682480607158755\n",
      "this is epoch 84,the loss is 22.582242058574167\n",
      "this is epoch 85,the loss is 22.56088199136889\n",
      "this is epoch 86,the loss is 22.51168800101886\n",
      "this is epoch 87,the loss is 22.55804507692661\n",
      "this is epoch 88,the loss is 22.48690107776582\n",
      "this is epoch 89,the loss is 22.500990391251708\n",
      "this is epoch 90,the loss is 22.457999794161925\n",
      "this is epoch 91,the loss is 22.611619979558046\n",
      "this is epoch 92,the loss is 22.462699154811578\n",
      "this is epoch 93,the loss is 22.479361596043326\n",
      "this is epoch 94,the loss is 22.399616481994535\n",
      "this is epoch 95,the loss is 22.350107828441665\n",
      "this is epoch 96,the loss is 22.357622668233347\n",
      "this is epoch 97,the loss is 22.30540904934369\n",
      "this is epoch 98,the loss is 22.426576317847566\n",
      "this is epoch 99,the loss is 22.482218656377228\n",
      "this is epoch 100,the loss is 22.411223353427125\n",
      "this is epoch 101,the loss is 22.39817052666631\n",
      "this is epoch 102,the loss is 22.25982277104788\n",
      "this is epoch 103,the loss is 22.307083563114116\n",
      "this is epoch 104,the loss is 22.18037581512236\n",
      "this is epoch 105,the loss is 22.178187218881867\n",
      "this is epoch 106,the loss is 22.166298402012078\n",
      "this is epoch 107,the loss is 22.18698864137188\n",
      "this is epoch 108,the loss is 22.147970410825856\n",
      "this is epoch 109,the loss is 22.088634277834768\n",
      "this is epoch 110,the loss is 22.090553758956133\n",
      "this is epoch 111,the loss is 22.122337959344158\n",
      "this is epoch 112,the loss is 22.065928251830623\n",
      "this is epoch 113,the loss is 22.0821986499705\n",
      "this is epoch 114,the loss is 22.141295950670322\n",
      "this is epoch 115,the loss is 22.155727429521484\n",
      "this is epoch 116,the loss is 22.03628332604269\n",
      "this is epoch 117,the loss is 21.942771590161747\n",
      "this is epoch 118,the loss is 21.97195166060372\n",
      "this is epoch 119,the loss is 21.979987400129154\n",
      "this is epoch 120,the loss is 21.891255064057308\n",
      "this is epoch 121,the loss is 21.895945313747117\n",
      "this is epoch 122,the loss is 21.902401669932473\n",
      "this is epoch 123,the loss is 21.900470038473724\n",
      "this is epoch 124,the loss is 21.850262507600295\n",
      "this is epoch 125,the loss is 21.858679488368203\n",
      "this is epoch 126,the loss is 22.015254322666443\n",
      "this is epoch 127,the loss is 21.79821672538562\n",
      "this is epoch 128,the loss is 21.789910890762734\n",
      "this is epoch 129,the loss is 21.76547473540153\n",
      "this is epoch 130,the loss is 21.768085899523868\n",
      "this is epoch 131,the loss is 21.761942726005145\n",
      "this is epoch 132,the loss is 21.738050566035003\n",
      "this is epoch 133,the loss is 21.753297829397525\n",
      "this is epoch 134,the loss is 21.71230901447042\n",
      "this is epoch 135,the loss is 21.746684481183454\n",
      "this is epoch 136,the loss is 21.701346110977894\n",
      "this is epoch 137,the loss is 21.6724086365798\n",
      "this is epoch 138,the loss is 21.758708960918433\n",
      "this is epoch 139,the loss is 21.69370788408538\n",
      "this is epoch 140,the loss is 21.706110708507765\n",
      "this is epoch 141,the loss is 21.614770928401033\n",
      "this is epoch 142,the loss is 21.607808961819124\n",
      "this is epoch 143,the loss is 21.703144750377767\n",
      "this is epoch 144,the loss is 21.554745215803234\n",
      "this is epoch 145,the loss is 21.547568901980696\n",
      "this is epoch 146,the loss is 21.57119167599157\n",
      "this is epoch 147,the loss is 21.5263691873653\n",
      "this is epoch 148,the loss is 21.60514271485622\n",
      "this is epoch 149,the loss is 21.524253631973043\n",
      "this is epoch 150,the loss is 21.491248663133483\n",
      "this is epoch 151,the loss is 21.48650361400107\n",
      "this is epoch 152,the loss is 21.565481678387496\n",
      "this is epoch 153,the loss is 21.582376698627506\n",
      "this is epoch 154,the loss is 21.439263469770964\n",
      "this is epoch 155,the loss is 21.59774614657582\n",
      "this is epoch 156,the loss is 21.638048150160603\n",
      "this is epoch 157,the loss is 21.48826698927095\n",
      "this is epoch 158,the loss is 21.43056757037492\n",
      "this is epoch 159,the loss is 21.43323114662554\n",
      "this is epoch 160,the loss is 21.39250368830258\n",
      "this is epoch 161,the loss is 21.366894563505426\n",
      "this is epoch 162,the loss is 21.521851788169446\n",
      "this is epoch 163,the loss is 21.328800807254094\n",
      "this is epoch 164,the loss is 21.34723835723742\n",
      "this is epoch 165,the loss is 21.374310403127133\n",
      "this is epoch 166,the loss is 21.33565200487832\n",
      "this is epoch 167,the loss is 21.32166802755584\n",
      "this is epoch 168,the loss is 21.34602200909145\n",
      "this is epoch 169,the loss is 21.287507761409262\n",
      "this is epoch 170,the loss is 21.273498305088182\n",
      "this is epoch 171,the loss is 21.254836823633784\n",
      "this is epoch 172,the loss is 21.251295021021775\n",
      "this is epoch 173,the loss is 21.3772971128904\n",
      "this is epoch 174,the loss is 21.21347380023255\n",
      "this is epoch 175,the loss is 21.232613098457193\n",
      "this is epoch 176,the loss is 21.311777857394805\n",
      "this is epoch 177,the loss is 21.249824692047337\n",
      "this is epoch 178,the loss is 21.281241936324555\n",
      "this is epoch 179,the loss is 21.20632457414847\n",
      "this is epoch 180,the loss is 21.166549659468476\n",
      "this is epoch 181,the loss is 21.178954748820633\n",
      "this is epoch 182,the loss is 21.131600934314864\n",
      "this is epoch 183,the loss is 21.24035769316377\n",
      "this is epoch 184,the loss is 21.200116468274814\n",
      "this is epoch 185,the loss is 21.168360390303352\n",
      "this is epoch 186,the loss is 21.178473491338405\n",
      "this is epoch 187,the loss is 21.289887207531354\n",
      "this is epoch 188,the loss is 21.183580974958666\n",
      "this is epoch 189,the loss is 21.43228651980299\n",
      "this is epoch 190,the loss is 21.20009026602212\n",
      "this is epoch 191,the loss is 21.130108367541027\n",
      "this is epoch 192,the loss is 21.13159709919052\n",
      "this is epoch 193,the loss is 21.046116358178324\n",
      "this is epoch 194,the loss is 21.05141677172805\n",
      "this is epoch 195,the loss is 21.019891227751348\n",
      "this is epoch 196,the loss is 21.055209211514377\n",
      "this is epoch 197,the loss is 21.0704702767685\n",
      "this is epoch 198,the loss is 21.210254257312872\n",
      "this is epoch 199,the loss is 21.010214711546745\n",
      "this is epoch 200,the loss is 20.988211832408656\n",
      "this is epoch 201,the loss is 21.04370518242699\n",
      "this is epoch 202,the loss is 20.981605912443964\n",
      "this is epoch 203,the loss is 21.11717429374047\n",
      "this is epoch 204,the loss is 21.047388906805935\n",
      "this is epoch 205,the loss is 21.10367538086807\n",
      "this is epoch 206,the loss is 20.925433576329606\n",
      "this is epoch 207,the loss is 20.941767115214038\n",
      "this is epoch 208,the loss is 20.946879738298332\n",
      "this is epoch 209,the loss is 20.93905901041586\n",
      "this is epoch 210,the loss is 20.970774546314825\n",
      "this is epoch 211,the loss is 20.947395027732057\n",
      "this is epoch 212,the loss is 20.998352521375597\n",
      "this is epoch 213,the loss is 21.03518069879974\n",
      "this is epoch 214,the loss is 20.872090278127654\n",
      "this is epoch 215,the loss is 20.92107399474084\n",
      "this is epoch 216,the loss is 20.86027272566578\n",
      "this is epoch 217,the loss is 20.895273794284314\n",
      "this is epoch 218,the loss is 20.931568299963335\n",
      "this is epoch 219,the loss is 20.903457812367236\n",
      "this is epoch 220,the loss is 20.88435224664784\n",
      "this is epoch 221,the loss is 20.835913753592152\n",
      "this is epoch 222,the loss is 20.887488529011026\n",
      "this is epoch 223,the loss is 20.890269402637863\n",
      "this is epoch 224,the loss is 20.796521254136564\n",
      "this is epoch 225,the loss is 20.79204548998426\n",
      "this is epoch 226,the loss is 20.844504053369285\n",
      "this is epoch 227,the loss is 20.88045755313575\n",
      "this is epoch 228,the loss is 20.751385979312392\n",
      "this is epoch 229,the loss is 20.936892973499944\n",
      "this is epoch 230,the loss is 20.792022553344093\n",
      "this is epoch 231,the loss is 20.743329356368175\n",
      "this is epoch 232,the loss is 20.728699575842064\n",
      "this is epoch 233,the loss is 20.796770125404755\n",
      "this is epoch 234,the loss is 20.826343850590767\n",
      "this is epoch 235,the loss is 20.751238901950636\n",
      "this is epoch 236,the loss is 20.749832470187272\n",
      "this is epoch 237,the loss is 20.72560389948063\n",
      "this is epoch 238,the loss is 20.725173639570578\n",
      "this is epoch 239,the loss is 20.693500154418818\n",
      "this is epoch 240,the loss is 20.694024462175168\n",
      "this is epoch 241,the loss is 20.855322818814987\n",
      "this is epoch 242,the loss is 20.65449766043024\n",
      "this is epoch 243,the loss is 20.681080075235165\n",
      "this is epoch 244,the loss is 20.636293026170204\n",
      "this is epoch 245,the loss is 20.621183939857026\n",
      "this is epoch 246,the loss is 20.64401143954974\n",
      "this is epoch 247,the loss is 20.60093139531462\n",
      "this is epoch 248,the loss is 20.62538016974132\n",
      "this is epoch 249,the loss is 20.67444243935249\n",
      "this is epoch 250,the loss is 20.623433297043263\n",
      "this is epoch 251,the loss is 20.65863428020074\n",
      "this is epoch 252,the loss is 20.62001041834445\n",
      "this is epoch 253,the loss is 20.610961869365433\n",
      "this is epoch 254,the loss is 20.57994053859781\n",
      "this is epoch 255,the loss is 20.636268523475135\n",
      "this is epoch 256,the loss is 20.542243587783176\n",
      "this is epoch 257,the loss is 20.566381210064772\n",
      "this is epoch 258,the loss is 20.54831267349664\n",
      "this is epoch 259,the loss is 20.56461581058054\n",
      "this is epoch 260,the loss is 20.583715254992132\n",
      "this is epoch 261,the loss is 20.68333475700428\n",
      "this is epoch 262,the loss is 20.834498523059565\n",
      "this is epoch 263,the loss is 20.519657560598976\n",
      "this is epoch 264,the loss is 20.502281917551706\n",
      "early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "model = AttentiveFPPredictor(node_feat_size=node_feat_size,\n",
    "                                  edge_feat_size=edge_feat_size,\n",
    "                                  num_layers=2,\n",
    "                                  num_timesteps=2,\n",
    "                                  graph_feat_size=256,\n",
    "                                  n_tasks=1,\n",
    "                                  dropout=0.0)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "model.load_state_dict(torch.load('models/best_my_trained_attfp.pth'))\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = 256\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
    "model.predict = nn.Sequential(\n",
    "    nn.Dropout(p = 0.0, inplace=False),\n",
    "    nn.Linear(num_ftrs, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1, bias = True),\n",
    "                            )\n",
    "model = model.to('cpu')\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.predict.parameters(), lr=0.0001, weight_decay=0.0003126662000605776,)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "max_epoch = 500\n",
    "best_val_loss = float('inf')\n",
    "saved = False\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "for i in range(0, max_epoch):\n",
    "    train(model, train_loader, i, optimizer, loss_fn)\n",
    "    val_rmse = test(model, test_loader, loss_fn).item()\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(model.state_dict(), 'models/best_my_tsf_attfp.pth')\n",
    "        saved = True\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training statistics are: \n",
      "the rmse is 20.675861917971062, the mae is 11.010482788085938\n",
      "the testing statistics are: \n",
      "the rmse is 19.307567672383776, the mae is 10.767457485198975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/best_my_tsf_attfp.pth'))\n",
    "print('the training statistics are: ')\n",
    "test(model, train_loader, loss_fn, verbose = True, return_rmse=False)\n",
    "print('the testing statistics are: ')\n",
    "test(model, test_loader, loss_fn, verbose = True,return_rmse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use pretrained as featurizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(os.path.join('mini_dataset/train_alc.csv'))\n",
    "test_raw = pd.read_csv(os.path.join('mini_dataset/test_alc.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gin_supervised_contextpred_pre_trained.pth from https://data.dgl.ai/dgllife/pre_trained/gin_supervised_contextpred.pth...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b0997be5974ce4af15c067de8c37da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gin_supervised_contextpred_pre_trained.pth:   0%|          | 0.00/7.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded\n",
      "Processing batch 1/11\n",
      "Processing batch 2/11\n",
      "Processing batch 3/11\n",
      "Processing batch 4/11\n",
      "Processing batch 5/11\n",
      "Processing batch 6/11\n",
      "Processing batch 7/11\n",
      "Processing batch 8/11\n",
      "Processing batch 9/11\n",
      "Processing batch 10/11\n",
      "Processing batch 11/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "afp_embeding_train = make_aft_embedding(train_raw['smiles'])\n",
    "\n",
    "gin_embeding_train = make_gin_embedding(train_raw['smiles'])\n",
    "rdkit_des_train = make_rdkit_descriptors(train_raw['smiles'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gin_supervised_contextpred_pre_trained.pth from https://data.dgl.ai/dgllife/pre_trained/gin_supervised_contextpred.pth...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046812bac21c48f7854558ae9ca7593f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gin_supervised_contextpred_pre_trained.pth:   0%|          | 0.00/7.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded\n",
      "Processing batch 1/3\n",
      "Processing batch 2/3\n",
      "Processing batch 3/3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "afp_embeding_test = make_aft_embedding(test_raw['smiles'])\n",
    "\n",
    "gin_embeding_test = make_gin_embedding(test_raw['smiles'])\n",
    "rdkit_des_test = make_rdkit_descriptors(test_raw['smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(afp_embeding_train, dtype=torch.float32)\n",
    "x2 = torch.tensor(gin_embeding_train, dtype=torch.float32)\n",
    "x3 = torch.tensor(rdkit_des_train, dtype=torch.float32)\n",
    "y = torch.from_numpy(train_raw['rt'].values.astype(np.float32))\n",
    "y  = y.view(y.shape[0],1)\n",
    "train_multi_data = MultiInputDataset(x1, x2, x3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(afp_embeding_test, dtype=torch.float32)\n",
    "x2 = torch.tensor(gin_embeding_test, dtype=torch.float32)\n",
    "x3 = torch.tensor(rdkit_des_test, dtype=torch.float32)\n",
    "y = torch.from_numpy(test_raw['rt'].values.astype(np.float32)).view(y.shape[0],1)\n",
    "y  = y.view(y.shape[0],1)\n",
    "test_multi_data = MultiInputDataset(x1, x2, x3, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is afp featurizer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "def train_simple_nn(model, train_loader, epoch, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_items = 0\n",
    "    for data in (train_loader):\n",
    "        x, y = data\n",
    "        x, y = x.to('cpu'), y.to('cpu')\n",
    "\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y.view(-1, 1))\n",
    "        train_loss += loss.item()*len(y)\n",
    "        train_items += len(y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch}, training rmse: {np.sqrt(train_loss / train_items)}')\n",
    "def test_simple_nn(model, test_loader, loss_fn, verbose = False, return_rmse = True, return_values=False):\n",
    "    model.eval()\n",
    "    refs = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            x, y = data\n",
    "            x, y = x.to('cpu'), y.to('cpu')\n",
    "            y_pred = model(x)\n",
    "            preds.extend(y_pred)\n",
    "            refs.extend(y)\n",
    "    preds= np.array([i.item() for i in preds])\n",
    "    refs = np.array([i.item() for i in refs])\n",
    "    mse = loss_fn(torch.tensor(preds), torch.tensor(refs))\n",
    "    mae = np.median(abs(preds-refs))\n",
    "    if verbose:\n",
    "        print(f'the rmse is {np.sqrt(mse)}, the mae is {mae}')\n",
    "    if return_values == True:\n",
    "        return preds, refs\n",
    "    elif return_rmse:\n",
    "        return np.sqrt(mse)\n",
    "    else:\n",
    "        return ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = make_dataset(afp_embeding_train, train_raw['rt'].values)\n",
    "test_df = make_dataset(afp_embeding_test, test_raw['rt'].values)\n",
    "train_loader = DataLoader(train_df, batch_size=64, shuffle=True, num_workers=6)\n",
    "test_loader = DataLoader(test_df, batch_size=64, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training rmse: 93.50006414713621\n",
      "Epoch 1, training rmse: 44.78855009727486\n",
      "Epoch 2, training rmse: 37.65559651005242\n",
      "Epoch 3, training rmse: 34.6529309648631\n",
      "Epoch 4, training rmse: 32.57488996952491\n",
      "Epoch 5, training rmse: 29.418243529372425\n",
      "Epoch 6, training rmse: 26.785040117945428\n",
      "Epoch 7, training rmse: 24.494184330250334\n",
      "Epoch 8, training rmse: 23.324078322247416\n",
      "Epoch 9, training rmse: 22.864654694034186\n",
      "Epoch 10, training rmse: 22.975757494780883\n",
      "Epoch 11, training rmse: 22.485025812889457\n",
      "Epoch 12, training rmse: 22.766221815394566\n",
      "Epoch 13, training rmse: 22.81970751542837\n",
      "Epoch 14, training rmse: 22.43339486770341\n",
      "Epoch 15, training rmse: 21.83890523887797\n",
      "Epoch 16, training rmse: 21.748589673073603\n",
      "Epoch 17, training rmse: 21.77213496387049\n",
      "Epoch 18, training rmse: 21.736052723731703\n",
      "Epoch 19, training rmse: 21.806973965379022\n",
      "Epoch 20, training rmse: 22.504599362678167\n",
      "Epoch 21, training rmse: 21.743081844783624\n",
      "Epoch 22, training rmse: 22.349070629789185\n",
      "Epoch 23, training rmse: 21.97532429330568\n",
      "Epoch 24, training rmse: 21.27221039028232\n",
      "Epoch 25, training rmse: 21.304249128113337\n",
      "Epoch 26, training rmse: 21.094536587753126\n",
      "Epoch 27, training rmse: 21.113453369784125\n",
      "Epoch 28, training rmse: 21.366437538375312\n",
      "Epoch 29, training rmse: 22.33847477559047\n",
      "Epoch 30, training rmse: 21.277721915394736\n",
      "Epoch 31, training rmse: 20.687966994331646\n",
      "Epoch 32, training rmse: 21.39873795428111\n",
      "Epoch 33, training rmse: 21.914835614471023\n",
      "Epoch 34, training rmse: 21.86349985982009\n",
      "Epoch 35, training rmse: 22.270460159599782\n",
      "Epoch 36, training rmse: 20.565756140600612\n",
      "Epoch 37, training rmse: 20.41027354593493\n",
      "Epoch 38, training rmse: 21.822670290929413\n",
      "Epoch 39, training rmse: 20.36893562259037\n",
      "Epoch 40, training rmse: 20.473013978994604\n",
      "Epoch 41, training rmse: 20.189422476606556\n",
      "Epoch 42, training rmse: 20.61243984462891\n",
      "Epoch 43, training rmse: 20.09362596314807\n",
      "Epoch 44, training rmse: 20.611370758762433\n",
      "Epoch 45, training rmse: 20.098163101051203\n",
      "Epoch 46, training rmse: 20.232002987697907\n",
      "Epoch 47, training rmse: 21.486386436135646\n",
      "Epoch 48, training rmse: 22.160098393304075\n",
      "Epoch 49, training rmse: 20.48085934777208\n",
      "Epoch 50, training rmse: 19.978003952503105\n",
      "Epoch 51, training rmse: 20.11594166762393\n",
      "Epoch 52, training rmse: 19.656250089570978\n",
      "Epoch 53, training rmse: 19.729563242538404\n",
      "Epoch 54, training rmse: 19.83358822649234\n",
      "Epoch 55, training rmse: 19.906620676692825\n",
      "Epoch 56, training rmse: 20.1433203730948\n",
      "Epoch 57, training rmse: 19.41560446090553\n",
      "Epoch 58, training rmse: 20.549936903090938\n",
      "Epoch 59, training rmse: 19.581388111406373\n",
      "Epoch 60, training rmse: 19.359214854877955\n",
      "Epoch 61, training rmse: 20.054547177485848\n",
      "Epoch 62, training rmse: 19.202226487863975\n",
      "Epoch 63, training rmse: 19.38390265519006\n",
      "Epoch 64, training rmse: 19.552742951498434\n",
      "Epoch 65, training rmse: 18.876509340948047\n",
      "Epoch 66, training rmse: 18.886410731551653\n",
      "Epoch 67, training rmse: 18.744632514622477\n",
      "Epoch 68, training rmse: 18.602394522635077\n",
      "Epoch 69, training rmse: 18.843517215875682\n",
      "Epoch 70, training rmse: 19.051754885486222\n",
      "Epoch 71, training rmse: 19.12984163539295\n",
      "Epoch 72, training rmse: 19.545134655356165\n",
      "Epoch 73, training rmse: 19.2050600048621\n",
      "Epoch 74, training rmse: 18.766167855405456\n",
      "Epoch 75, training rmse: 19.494318295271746\n",
      "Epoch 76, training rmse: 18.62501495999365\n",
      "Epoch 77, training rmse: 18.48954959782772\n",
      "Epoch 78, training rmse: 18.199433999761055\n",
      "Epoch 79, training rmse: 18.378413977018976\n",
      "Epoch 80, training rmse: 18.74242584705072\n",
      "Epoch 81, training rmse: 18.812982418391954\n",
      "Epoch 82, training rmse: 21.09700983855472\n",
      "Epoch 83, training rmse: 20.239115631831105\n",
      "Epoch 84, training rmse: 18.432153782741114\n",
      "Epoch 85, training rmse: 17.8593228060713\n",
      "Epoch 86, training rmse: 18.599527680893342\n",
      "Epoch 87, training rmse: 18.275837675837384\n",
      "Epoch 88, training rmse: 17.919697815080845\n",
      "Epoch 89, training rmse: 17.710200643858496\n",
      "Epoch 90, training rmse: 18.89972183940534\n",
      "Epoch 91, training rmse: 18.045959269539253\n",
      "Epoch 92, training rmse: 17.81875320019778\n",
      "Epoch 93, training rmse: 19.6504585460367\n",
      "Epoch 94, training rmse: 17.716987982350403\n",
      "Epoch 95, training rmse: 17.907958045134517\n",
      "Epoch 96, training rmse: 17.577488176299056\n",
      "Epoch 97, training rmse: 17.155336175723292\n",
      "Epoch 98, training rmse: 16.992883218574676\n",
      "Epoch 99, training rmse: 16.91337792664799\n",
      "Epoch 100, training rmse: 17.07472491381019\n",
      "Epoch 101, training rmse: 16.62716478376387\n",
      "Epoch 102, training rmse: 17.25752339659556\n",
      "Epoch 103, training rmse: 16.954803318748542\n",
      "Epoch 104, training rmse: 16.67267987665243\n",
      "Epoch 105, training rmse: 16.916972656874737\n",
      "Epoch 106, training rmse: 17.22972704407938\n",
      "Epoch 107, training rmse: 16.736295525342115\n",
      "Epoch 108, training rmse: 17.101410828369215\n",
      "Epoch 109, training rmse: 16.35820380077356\n",
      "Epoch 110, training rmse: 16.607810455172316\n",
      "Epoch 111, training rmse: 16.4066632467273\n",
      "Epoch 112, training rmse: 16.55690893500638\n",
      "Epoch 113, training rmse: 16.1267358834905\n",
      "Epoch 114, training rmse: 16.397009910735076\n",
      "Epoch 115, training rmse: 17.347409315036884\n",
      "Epoch 116, training rmse: 16.195590439068877\n",
      "Epoch 117, training rmse: 16.679192406461365\n",
      "Epoch 118, training rmse: 16.389821819740597\n",
      "Epoch 119, training rmse: 18.34575709351343\n",
      "Epoch 120, training rmse: 16.23430094862687\n",
      "Epoch 121, training rmse: 16.140214245242365\n",
      "Epoch 122, training rmse: 16.297788950898326\n",
      "Epoch 123, training rmse: 16.2185093580378\n",
      "Epoch 124, training rmse: 17.295552068071167\n",
      "Epoch 125, training rmse: 16.253412959105397\n",
      "Epoch 126, training rmse: 17.009264371897064\n",
      "Epoch 127, training rmse: 15.726127168753063\n",
      "Epoch 128, training rmse: 15.800926491646377\n",
      "Epoch 129, training rmse: 15.851772579798078\n",
      "Epoch 130, training rmse: 16.065294351800063\n",
      "Epoch 131, training rmse: 16.457213020312757\n",
      "Epoch 132, training rmse: 16.393088077439543\n",
      "Epoch 133, training rmse: 16.319906492995976\n",
      "Epoch 134, training rmse: 16.259608879960137\n",
      "Epoch 135, training rmse: 15.40609379162643\n",
      "Epoch 136, training rmse: 15.416086225938798\n",
      "Epoch 137, training rmse: 16.037204642807527\n",
      "Epoch 138, training rmse: 16.27001827355144\n",
      "early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "fc_model = SimpleMLP(input_dim=256)\n",
    "fc_model = fc_model.to('cpu')\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(fc_model.parameters(), lr=0.001, weight_decay=0.0003126662000605776,)\n",
    "max_epoch = 500\n",
    "best_val_loss = float('inf')\n",
    "for i in range(max_epoch):\n",
    "    train_simple_nn(fc_model, train_loader, i, optimizer, loss_fn)\n",
    "    val_rmse = test_simple_nn(fc_model, test_loader, loss_fn)\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(fc_model.state_dict(), 'models/best_afp_featurizer.pth')\n",
    "        saved = True\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the training statistics\n",
      "the rmse is 15.757693206970032, the mae is 7.5129852294921875\n",
      "this is the testing statistics\n",
      "the rmse is 16.235132213696666, the mae is 7.2605438232421875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(16.2351, dtype=torch.float64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.load_state_dict(torch.load('models/best_afp_featurizer.pth'))\n",
    "print('this is the training statistics')\n",
    "test_simple_nn(fc_model, train_loader, loss_fn, verbose = True)\n",
    "print('this is the testing statistics')\n",
    "test_simple_nn(fc_model, test_loader, loss_fn, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets try gin featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = make_dataset(gin_embeding_train, train_raw['rt'].values)\n",
    "test_df = make_dataset(gin_embeding_test, test_raw['rt'].values)\n",
    "train_loader = DataLoader(train_df, batch_size=64, shuffle=True, num_workers=6)\n",
    "test_loader = DataLoader(test_df, batch_size=64, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training rmse: 137.62884338425002\n",
      "Epoch 1, training rmse: 126.92388452557742\n",
      "Epoch 2, training rmse: 89.66154194183896\n",
      "Epoch 3, training rmse: 50.15734302931893\n",
      "Epoch 4, training rmse: 45.96680292368452\n",
      "Epoch 5, training rmse: 44.28031541490007\n",
      "Epoch 6, training rmse: 42.67744165360733\n",
      "Epoch 7, training rmse: 41.581873835563776\n",
      "Epoch 8, training rmse: 40.564073722944464\n",
      "Epoch 9, training rmse: 39.52769133809849\n",
      "Epoch 10, training rmse: 38.59505143346867\n",
      "Epoch 11, training rmse: 37.544758307970916\n",
      "Epoch 12, training rmse: 36.638660229825405\n",
      "Epoch 13, training rmse: 35.72815595793434\n",
      "Epoch 14, training rmse: 34.77606615067024\n",
      "Epoch 15, training rmse: 34.17305962905904\n",
      "Epoch 16, training rmse: 33.29732299205147\n",
      "Epoch 17, training rmse: 32.45674102570495\n",
      "Epoch 18, training rmse: 31.74788639677904\n",
      "Epoch 19, training rmse: 31.115183143284735\n",
      "Epoch 20, training rmse: 30.469578054134278\n",
      "Epoch 21, training rmse: 29.864018561979666\n",
      "Epoch 22, training rmse: 29.52606757968239\n",
      "Epoch 23, training rmse: 28.852167812528425\n",
      "Epoch 24, training rmse: 28.487039418912534\n",
      "Epoch 25, training rmse: 28.357419163498232\n",
      "Epoch 26, training rmse: 27.483611514834443\n",
      "Epoch 27, training rmse: 27.08374994875113\n",
      "Epoch 28, training rmse: 26.800509212349017\n",
      "Epoch 29, training rmse: 26.57820292907248\n",
      "Epoch 30, training rmse: 26.044959452737157\n",
      "Epoch 31, training rmse: 26.004528389799766\n",
      "Epoch 32, training rmse: 25.44744697826595\n",
      "Epoch 33, training rmse: 25.047014686047593\n",
      "Epoch 34, training rmse: 24.78576765389053\n",
      "Epoch 35, training rmse: 24.488005208235776\n",
      "Epoch 36, training rmse: 24.1607709517388\n",
      "Epoch 37, training rmse: 23.972715797060772\n",
      "Epoch 38, training rmse: 23.85028938521212\n",
      "Epoch 39, training rmse: 23.406302711519533\n",
      "Epoch 40, training rmse: 23.334494202070598\n",
      "Epoch 41, training rmse: 23.01488831095612\n",
      "Epoch 42, training rmse: 22.91095678993709\n",
      "Epoch 43, training rmse: 22.39990757461485\n",
      "Epoch 44, training rmse: 22.113541586695103\n",
      "Epoch 45, training rmse: 21.959960194485248\n",
      "Epoch 46, training rmse: 21.627824978585277\n",
      "Epoch 47, training rmse: 21.421900920598826\n",
      "Epoch 48, training rmse: 21.61305419031302\n",
      "Epoch 49, training rmse: 20.960871461425146\n",
      "Epoch 50, training rmse: 20.734317899128946\n",
      "Epoch 51, training rmse: 20.573076841771933\n",
      "Epoch 52, training rmse: 20.61472755604659\n",
      "Epoch 53, training rmse: 20.363469311105927\n",
      "Epoch 54, training rmse: 19.983513227556738\n",
      "Epoch 55, training rmse: 19.7992497211876\n",
      "Epoch 56, training rmse: 19.56295537465356\n",
      "Epoch 57, training rmse: 19.474362622459868\n",
      "Epoch 58, training rmse: 19.46223337117536\n",
      "Epoch 59, training rmse: 19.106923038912026\n",
      "Epoch 60, training rmse: 19.01459069832366\n",
      "Epoch 61, training rmse: 18.783209579170272\n",
      "Epoch 62, training rmse: 19.23244389946493\n",
      "Epoch 63, training rmse: 18.7316038813173\n",
      "Epoch 64, training rmse: 18.686696798003112\n",
      "Epoch 65, training rmse: 18.19514656275859\n",
      "Epoch 66, training rmse: 18.131349751514545\n",
      "Epoch 67, training rmse: 17.985391959247206\n",
      "Epoch 68, training rmse: 17.853668497708387\n",
      "Epoch 69, training rmse: 17.70888699531041\n",
      "Epoch 70, training rmse: 18.016859803551093\n",
      "Epoch 71, training rmse: 17.392310003080116\n",
      "Epoch 72, training rmse: 17.454742277760232\n",
      "Epoch 73, training rmse: 17.494203495837912\n",
      "Epoch 74, training rmse: 17.414532510846346\n",
      "Epoch 75, training rmse: 17.173298469369662\n",
      "Epoch 76, training rmse: 17.018043029686932\n",
      "Epoch 77, training rmse: 16.852495580911512\n",
      "Epoch 78, training rmse: 16.984867388819737\n",
      "Epoch 79, training rmse: 16.657876966291212\n",
      "Epoch 80, training rmse: 16.6578176400974\n",
      "Epoch 81, training rmse: 16.377105181340088\n",
      "Epoch 82, training rmse: 16.342060054691103\n",
      "Epoch 83, training rmse: 16.243814885041985\n",
      "Epoch 84, training rmse: 16.343536197599974\n",
      "Epoch 85, training rmse: 16.241156033915217\n",
      "Epoch 86, training rmse: 16.026569932883504\n",
      "Epoch 87, training rmse: 16.01088327154628\n",
      "Epoch 88, training rmse: 15.841729490746921\n",
      "Epoch 89, training rmse: 15.805303470797918\n",
      "Epoch 90, training rmse: 15.767379550812539\n",
      "Epoch 91, training rmse: 15.798993083957823\n",
      "Epoch 92, training rmse: 16.29860274977131\n",
      "Epoch 93, training rmse: 15.538813998733678\n",
      "Epoch 94, training rmse: 15.635366281626991\n",
      "Epoch 95, training rmse: 15.314841775389\n",
      "Epoch 96, training rmse: 15.159611063885238\n",
      "Epoch 97, training rmse: 15.183096457907729\n",
      "Epoch 98, training rmse: 14.91446823756324\n",
      "Epoch 99, training rmse: 15.21820587214152\n",
      "Epoch 100, training rmse: 14.910073470313288\n",
      "Epoch 101, training rmse: 15.066481190390917\n",
      "Epoch 102, training rmse: 15.433535428701106\n",
      "Epoch 103, training rmse: 15.11468598846211\n",
      "Epoch 104, training rmse: 16.128006528053117\n",
      "Epoch 105, training rmse: 14.918156163351922\n",
      "Epoch 106, training rmse: 15.0043724506512\n",
      "Epoch 107, training rmse: 14.49991514693523\n",
      "Epoch 108, training rmse: 14.39366809235949\n",
      "Epoch 109, training rmse: 14.590102042613005\n",
      "Epoch 110, training rmse: 14.201278974789554\n",
      "Epoch 111, training rmse: 14.137206455789165\n",
      "Epoch 112, training rmse: 14.402529677208072\n",
      "Epoch 113, training rmse: 14.217132700733952\n",
      "Epoch 114, training rmse: 13.985803704089621\n",
      "Epoch 115, training rmse: 13.840178104779152\n",
      "Epoch 116, training rmse: 13.828535068803493\n",
      "Epoch 117, training rmse: 14.126854103787709\n",
      "Epoch 118, training rmse: 14.076902726867655\n",
      "Epoch 119, training rmse: 15.653627634296113\n",
      "Epoch 120, training rmse: 13.767824060804363\n",
      "early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "fc_model = SimpleMLP(input_dim=gin_embeding_train[0].shape[0])\n",
    "fc_model = fc_model.to('cpu')\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(fc_model.parameters(), lr=0.001)\n",
    "max_epoch = 500\n",
    "best_val_loss = float('inf')\n",
    "for i in range(max_epoch):\n",
    "    train_simple_nn(fc_model, train_loader, i, optimizer, loss_fn)\n",
    "    val_rmse = test_simple_nn(fc_model, test_loader, loss_fn)\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(fc_model.state_dict(), 'models/best_gin_featurizer.pth')\n",
    "        saved = True\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the training statistics\n",
      "the rmse is 13.945336224278513, the mae is 5.2879638671875\n",
      "this is the testing statistics\n",
      "the rmse is 19.715803901658568, the mae is 7.7261199951171875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(19.7158, dtype=torch.float64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.load_state_dict(torch.load('models/best_gin_featurizer.pth'))\n",
    "print('this is the training statistics')\n",
    "test_simple_nn(fc_model, train_loader, loss_fn, verbose = True)\n",
    "print('this is the testing statistics')\n",
    "test_simple_nn(fc_model, test_loader, loss_fn, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets try rdkit descirptors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = make_dataset(rdkit_des_train, train_raw['rt'].values)\n",
    "test_df = make_dataset(rdkit_des_test, test_raw['rt'].values)\n",
    "train_loader = DataLoader(train_df, batch_size=64, shuffle=True, num_workers=6)\n",
    "test_loader = DataLoader(test_df, batch_size=64, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training rmse: 71.19587368385241\n",
      "Epoch 1, training rmse: 42.33739886952783\n",
      "Epoch 2, training rmse: 27.695124163659127\n",
      "Epoch 3, training rmse: 22.686126226920294\n",
      "Epoch 4, training rmse: 21.13158070725923\n",
      "Epoch 5, training rmse: 20.851550647068482\n",
      "Epoch 6, training rmse: 21.42709641028066\n",
      "Epoch 7, training rmse: 20.335070440983635\n",
      "Epoch 8, training rmse: 19.682056176071722\n",
      "Epoch 9, training rmse: 19.468168724825443\n",
      "Epoch 10, training rmse: 19.06062126945776\n",
      "Epoch 11, training rmse: 20.07518751798812\n",
      "Epoch 12, training rmse: 19.3215138555245\n",
      "Epoch 13, training rmse: 18.886477447645625\n",
      "Epoch 14, training rmse: 19.08598809006234\n",
      "Epoch 15, training rmse: 18.299610877830418\n",
      "Epoch 16, training rmse: 18.563048266410483\n",
      "Epoch 17, training rmse: 17.631450836743383\n",
      "Epoch 18, training rmse: 17.89233441000554\n",
      "Epoch 19, training rmse: 17.19799384469063\n",
      "Epoch 20, training rmse: 17.05473018543683\n",
      "Epoch 21, training rmse: 17.266595848401\n",
      "Epoch 22, training rmse: 18.386421519936174\n",
      "Epoch 23, training rmse: 16.69453460655716\n",
      "Epoch 24, training rmse: 16.541133450420688\n",
      "Epoch 25, training rmse: 16.892436630064655\n",
      "Epoch 26, training rmse: 16.59166088153163\n",
      "Epoch 27, training rmse: 17.273808212381304\n",
      "Epoch 28, training rmse: 16.282373233800175\n",
      "Epoch 29, training rmse: 16.24625634129731\n",
      "Epoch 30, training rmse: 15.977294561050394\n",
      "Epoch 31, training rmse: 15.799433396078173\n",
      "Epoch 32, training rmse: 16.24416934266499\n",
      "Epoch 33, training rmse: 15.839453294587624\n",
      "Epoch 34, training rmse: 15.279885916073523\n",
      "Epoch 35, training rmse: 15.304958838355855\n",
      "Epoch 36, training rmse: 15.140554985717925\n",
      "Epoch 37, training rmse: 15.520421493102385\n",
      "Epoch 38, training rmse: 15.118329963722692\n",
      "Epoch 39, training rmse: 15.69641601052623\n",
      "Epoch 40, training rmse: 15.846692150619083\n",
      "Epoch 41, training rmse: 16.883132571612574\n",
      "Epoch 42, training rmse: 14.827474883173991\n",
      "Epoch 43, training rmse: 14.455364223660672\n",
      "Epoch 44, training rmse: 14.433001399198217\n",
      "Epoch 45, training rmse: 14.450451153861154\n",
      "Epoch 46, training rmse: 14.288546654768474\n",
      "Epoch 47, training rmse: 14.049948027028801\n",
      "Epoch 48, training rmse: 14.223830587048015\n",
      "Epoch 49, training rmse: 14.866361491469505\n",
      "Epoch 50, training rmse: 15.446459894973733\n",
      "Epoch 51, training rmse: 14.314323300535598\n",
      "Epoch 52, training rmse: 13.781317654291472\n",
      "Epoch 53, training rmse: 14.796255502848915\n",
      "Epoch 54, training rmse: 15.510752513712118\n",
      "Epoch 55, training rmse: 14.374528581309631\n",
      "Epoch 56, training rmse: 14.76524899879714\n",
      "Epoch 57, training rmse: 13.781478619347595\n",
      "Epoch 58, training rmse: 13.555975276591871\n",
      "Epoch 59, training rmse: 13.5052016059488\n",
      "Epoch 60, training rmse: 14.68606396122879\n",
      "Epoch 61, training rmse: 13.49676487818829\n",
      "Epoch 62, training rmse: 13.493639377958738\n",
      "Epoch 63, training rmse: 13.29190176864114\n",
      "Epoch 64, training rmse: 13.721397286929456\n",
      "Epoch 65, training rmse: 13.628627956587774\n",
      "Epoch 66, training rmse: 13.386714850435286\n",
      "Epoch 67, training rmse: 14.083870940042267\n",
      "Epoch 68, training rmse: 13.595519898969245\n",
      "Epoch 69, training rmse: 19.102845295394232\n",
      "Epoch 70, training rmse: 14.420839446339388\n",
      "Epoch 71, training rmse: 13.582869162496442\n",
      "Epoch 72, training rmse: 13.688176780478914\n",
      "Epoch 73, training rmse: 13.148825861425241\n",
      "early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "fc_model = SimpleMLP(input_dim=rdkit_des_train[0].shape[0])\n",
    "fc_model = fc_model.to('cpu')\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(fc_model.parameters(), lr=0.001)\n",
    "max_epoch = 500\n",
    "best_val_loss = float('inf')\n",
    "for i in range(max_epoch):\n",
    "    train_simple_nn(fc_model, train_loader, i, optimizer, loss_fn)\n",
    "    val_rmse = test_simple_nn(fc_model, test_loader, loss_fn)\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(fc_model.state_dict(), 'models/best_rdkit_featurizer.pth')\n",
    "        saved = True\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the training statistics\n",
      "the rmse is 13.019986079193677, the mae is 5.598901748657227\n",
      "this is the testing statistics\n",
      "the rmse is 14.73400650043799, the mae is 6.334228515625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14.7340, dtype=torch.float64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.load_state_dict(torch.load('models/best_rdkit_featurizer.pth'))\n",
    "print('this is the training statistics')\n",
    "test_simple_nn(fc_model, train_loader, loss_fn, verbose = True)\n",
    "print('this is the testing statistics')\n",
    "test_simple_nn(fc_model, test_loader, loss_fn, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# maybe bert as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1287/1287 [00:46<00:00, 27.47it/s]\n",
      "100%|██████████| 322/322 [00:12<00:00, 25.89it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_bert_embedding = make_clf_embeddings(train_raw['smiles'], attention_pooling=True)\n",
    "test_bert_embedding = make_clf_embeddings(test_raw['smiles'], attention_pooling=True)\n",
    "train_df = make_dataset(train_bert_embedding, train_raw['rt'].values)\n",
    "test_df = make_dataset(test_bert_embedding, test_raw['rt'].values)\n",
    "train_loader = DataLoader(train_df, batch_size=64, shuffle=True, num_workers=6)\n",
    "test_loader = DataLoader(test_df, batch_size=64, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training rmse: 117.04436826869265\n",
      "Epoch 1, training rmse: 76.40097919272168\n",
      "Epoch 2, training rmse: 67.00573752194056\n",
      "Epoch 3, training rmse: 61.859084054880576\n",
      "Epoch 4, training rmse: 58.990069252401284\n",
      "Epoch 5, training rmse: 57.858466115519974\n",
      "Epoch 6, training rmse: 55.82698502313838\n",
      "Epoch 7, training rmse: 54.60157872500633\n",
      "Epoch 8, training rmse: 53.072203488502055\n",
      "Epoch 9, training rmse: 51.22257966673793\n",
      "Epoch 10, training rmse: 50.24812381467169\n",
      "Epoch 11, training rmse: 48.8437107118571\n",
      "Epoch 12, training rmse: 48.40138102309951\n",
      "Epoch 13, training rmse: 46.51430188928604\n",
      "Epoch 14, training rmse: 45.16404345897855\n",
      "Epoch 15, training rmse: 44.155365336719434\n",
      "Epoch 16, training rmse: 43.041430005126806\n",
      "Epoch 17, training rmse: 42.210616329716096\n",
      "Epoch 18, training rmse: 41.23515965079172\n",
      "Epoch 19, training rmse: 41.0055817811317\n",
      "Epoch 20, training rmse: 39.16492540995183\n",
      "Epoch 21, training rmse: 38.62436921336199\n",
      "Epoch 22, training rmse: 37.58043791959486\n",
      "Epoch 23, training rmse: 37.22899062849809\n",
      "Epoch 24, training rmse: 37.128480827279574\n",
      "Epoch 25, training rmse: 38.927711068979335\n",
      "Epoch 26, training rmse: 39.45671207786708\n",
      "Epoch 27, training rmse: 36.20189851943291\n",
      "Epoch 28, training rmse: 36.18186906883973\n",
      "Epoch 29, training rmse: 34.435121202641135\n",
      "Epoch 30, training rmse: 33.95726841284529\n",
      "Epoch 31, training rmse: 33.75481597641968\n",
      "Epoch 32, training rmse: 33.61637176159622\n",
      "Epoch 33, training rmse: 33.09704744707905\n",
      "Epoch 34, training rmse: 32.61915390406906\n",
      "Epoch 35, training rmse: 31.922106752809178\n",
      "Epoch 36, training rmse: 31.508311402709438\n",
      "Epoch 37, training rmse: 31.517591976896288\n",
      "Epoch 38, training rmse: 31.02317301886831\n",
      "Epoch 39, training rmse: 30.865238696447694\n",
      "Epoch 40, training rmse: 32.147531413631874\n",
      "Epoch 41, training rmse: 31.151859434716705\n",
      "Epoch 42, training rmse: 30.481475111376977\n",
      "Epoch 43, training rmse: 29.567313353526966\n",
      "Epoch 44, training rmse: 29.49642847198646\n",
      "Epoch 45, training rmse: 29.561813531883065\n",
      "Epoch 46, training rmse: 28.989136152112618\n",
      "Epoch 47, training rmse: 28.487843961677804\n",
      "Epoch 48, training rmse: 28.970662797694683\n",
      "Epoch 49, training rmse: 28.86499942742387\n",
      "Epoch 50, training rmse: 28.398603130705872\n",
      "Epoch 51, training rmse: 28.069673927492108\n",
      "Epoch 52, training rmse: 27.35103761353752\n",
      "Epoch 53, training rmse: 27.83289178188374\n",
      "Epoch 54, training rmse: 27.662096661798557\n",
      "Epoch 55, training rmse: 27.39178321601586\n",
      "Epoch 56, training rmse: 27.395159433995104\n",
      "Epoch 57, training rmse: 26.817978409129417\n",
      "Epoch 58, training rmse: 26.23235796872931\n",
      "Epoch 59, training rmse: 26.970630327290962\n",
      "Epoch 60, training rmse: 26.154568387922556\n",
      "Epoch 61, training rmse: 25.93967858606616\n",
      "Epoch 62, training rmse: 25.72896322709203\n",
      "Epoch 63, training rmse: 25.104421015189626\n",
      "Epoch 64, training rmse: 25.171718891345098\n",
      "Epoch 65, training rmse: 25.496341073477016\n",
      "Epoch 66, training rmse: 25.306747781391444\n",
      "Epoch 67, training rmse: 25.083931801970877\n",
      "Epoch 68, training rmse: 24.635872262642316\n",
      "Epoch 69, training rmse: 25.442908558927975\n",
      "Epoch 70, training rmse: 25.106538436209316\n",
      "Epoch 71, training rmse: 23.948101147432762\n",
      "Epoch 72, training rmse: 24.730978561795062\n",
      "Epoch 73, training rmse: 24.643451575836117\n",
      "Epoch 74, training rmse: 24.790441311655528\n",
      "Epoch 75, training rmse: 24.304125087306694\n",
      "Epoch 76, training rmse: 24.73824149264039\n",
      "Epoch 77, training rmse: 23.48738509341815\n",
      "Epoch 78, training rmse: 25.12791628425909\n",
      "Epoch 79, training rmse: 23.431563957881913\n",
      "Epoch 80, training rmse: 23.20153037828754\n",
      "Epoch 81, training rmse: 24.582493219763666\n",
      "Epoch 82, training rmse: 23.04409459165427\n",
      "Epoch 83, training rmse: 23.035175412619697\n",
      "Epoch 84, training rmse: 22.989304487728667\n",
      "Epoch 85, training rmse: 22.499231716404562\n",
      "Epoch 86, training rmse: 22.3793682612805\n",
      "Epoch 87, training rmse: 22.133250071524603\n",
      "Epoch 88, training rmse: 22.005035699800242\n",
      "Epoch 89, training rmse: 21.761885891510204\n",
      "Epoch 90, training rmse: 22.0490284608015\n",
      "Epoch 91, training rmse: 21.48756905912073\n",
      "Epoch 92, training rmse: 22.35054397451362\n",
      "Epoch 93, training rmse: 21.40506173317558\n",
      "Epoch 94, training rmse: 21.84277284953711\n",
      "Epoch 95, training rmse: 21.63462283743719\n",
      "Epoch 96, training rmse: 22.021117033472336\n",
      "Epoch 97, training rmse: 22.35645145204118\n",
      "Epoch 98, training rmse: 22.145251141513505\n",
      "Epoch 99, training rmse: 23.88064557720696\n",
      "Epoch 100, training rmse: 23.20191893533906\n",
      "Epoch 101, training rmse: 23.46384778254841\n",
      "Epoch 102, training rmse: 22.719485533583505\n",
      "Epoch 103, training rmse: 20.709930304670337\n",
      "Epoch 104, training rmse: 21.455833127896007\n",
      "Epoch 105, training rmse: 22.58420733467147\n",
      "Epoch 106, training rmse: 20.5664727354458\n",
      "Epoch 107, training rmse: 20.46172858755948\n",
      "Epoch 108, training rmse: 21.375292891419015\n",
      "Epoch 109, training rmse: 21.09623821172363\n",
      "Epoch 110, training rmse: 21.07421862313653\n",
      "Epoch 111, training rmse: 20.837225782296574\n",
      "Epoch 112, training rmse: 20.488933838492606\n",
      "Epoch 113, training rmse: 21.15148743912435\n",
      "Epoch 114, training rmse: 20.444740560478785\n",
      "Epoch 115, training rmse: 19.934736186338938\n",
      "Epoch 116, training rmse: 20.00363356655081\n",
      "Epoch 117, training rmse: 20.053146609440343\n",
      "Epoch 118, training rmse: 19.817951636549637\n",
      "Epoch 119, training rmse: 20.533860105146452\n",
      "Epoch 120, training rmse: 21.895110231062752\n",
      "Epoch 121, training rmse: 20.728588862280382\n",
      "Epoch 122, training rmse: 19.52250879568674\n",
      "Epoch 123, training rmse: 20.09436993735679\n",
      "Epoch 124, training rmse: 19.88610398600558\n",
      "Epoch 125, training rmse: 20.3748884890134\n",
      "Epoch 126, training rmse: 21.06872997863114\n",
      "Epoch 127, training rmse: 19.965837817118448\n",
      "Epoch 128, training rmse: 20.124307395603115\n",
      "Epoch 129, training rmse: 21.84698007314883\n",
      "Epoch 130, training rmse: 19.986703047223614\n",
      "Epoch 131, training rmse: 19.241219367659614\n",
      "Epoch 132, training rmse: 22.002060847766316\n",
      "Epoch 133, training rmse: 22.00142780533593\n",
      "Epoch 134, training rmse: 19.788215778489803\n",
      "Epoch 135, training rmse: 19.016206686113854\n",
      "Epoch 136, training rmse: 19.0017334467846\n",
      "Epoch 137, training rmse: 20.44350402496895\n",
      "Epoch 138, training rmse: 19.757682728212213\n",
      "Epoch 139, training rmse: 20.94423485827563\n",
      "Epoch 140, training rmse: 19.312943329528892\n",
      "Epoch 141, training rmse: 19.133571898761577\n",
      "Epoch 142, training rmse: 18.947136215812904\n",
      "Epoch 143, training rmse: 18.438937487033844\n",
      "Epoch 144, training rmse: 18.903695303145398\n",
      "Epoch 145, training rmse: 19.491886429945993\n",
      "Epoch 146, training rmse: 19.809060380043423\n",
      "Epoch 147, training rmse: 19.31877154291721\n",
      "Epoch 148, training rmse: 19.10764979268034\n",
      "Epoch 149, training rmse: 19.63424427180523\n",
      "Epoch 150, training rmse: 20.128968570800264\n",
      "early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "fc_model = SimpleMLP(input_dim=train_bert_embedding[0].shape[0])\n",
    "fc_model = fc_model.to('cpu')\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(fc_model.parameters(), lr=0.001)\n",
    "max_epoch = 500\n",
    "best_val_loss = float('inf')\n",
    "for i in range(max_epoch):\n",
    "    train_simple_nn(fc_model, train_loader, i, optimizer, loss_fn)\n",
    "    val_rmse = test_simple_nn(fc_model, test_loader, loss_fn)\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(fc_model.state_dict(), 'models/best_bert_featurizer.pth')\n",
    "        saved = True\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break\n",
    "fc_model.load_state_dict(torch.load('models/best_bert_featurizer.pth'))\n",
    "print('this is the training statistics')\n",
    "test_simple_nn(fc_model, train_loader, loss_fn, verbose = True)\n",
    "print('this is the testing statistics')\n",
    "test_simple_nn(fc_model, test_loader, loss_fn, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets super"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1287/1287 [00:00<00:00, 11868.11it/s]\n",
      "100%|██████████| 322/322 [00:00<00:00, 10832.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from dl_toolsets.utilities import make_ecfp, make_aft_embedding, make_datalist, attentive_fp_featurizers\n",
    "train_raw = pd.read_csv(os.path.join('mini_dataset/train_alc.csv'))\n",
    "test_raw = pd.read_csv(os.path.join('mini_dataset/test_alc.csv'))\n",
    "train_ecfp = make_ecfp(train_raw['smiles'])\n",
    "test_ecfp = make_ecfp(test_raw['smiles'])\n",
    "train_afp = attentive_fp_featurizers(train_raw['smiles'])\n",
    "test_afp = attentive_fp_featurizers(test_raw['smiles'])\n",
    "train_list = make_datalist(train_afp, train_ecfp,train_raw['rt'].values)\n",
    "test_list = make_datalist(test_afp, test_ecfp, test_raw['rt'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/fanzhoukong/Documents/GitHub/Libgen_data/metlin_smrt'\n",
    "data_subset = pd.read_csv(os.path.join(data_dir, 'SMRT_dataset.csv'), delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80038/80038 [00:28<00:00, 2772.97it/s]\n"
     ]
    }
   ],
   "source": [
    "smiles = []\n",
    "for index, row in tqdm(data_subset.iterrows(), total = len(data_subset)):\n",
    "    mol = Chem.MolFromInchi(row['inchi'])\n",
    "    if mol is not None:\n",
    "        smiles.append(Chem.MolToSmiles(mol))\n",
    "    else:\n",
    "        smiles.append(np.NAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset['smiles'] = smiles\n",
    "data_subset = data_subset.dropna(subset = ['smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.to_csv(os.path.join(data_dir, 'SMRT_dataset_cleaned.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubchem</th>\n",
       "      <th>rt</th>\n",
       "      <th>inchi</th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5139</td>\n",
       "      <td>93.5</td>\n",
       "      <td>InChI=1S/C3H8N2S/c1-2-6-3(4)5/h2H2,1H3,(H3,4,5)</td>\n",
       "      <td>CCSC(=N)N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3505</td>\n",
       "      <td>687.8</td>\n",
       "      <td>InChI=1S/C19H25Cl2N3O3/c1-27-19(26)23-8-9-24(1...</td>\n",
       "      <td>COC(=O)N1CCN(C(=O)Cc2ccc(Cl)c(Cl)c2)[C@H](CN2C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2159</td>\n",
       "      <td>590.7</td>\n",
       "      <td>InChI=1S/C17H27N3O4S/c1-4-20-8-6-7-12(20)11-19...</td>\n",
       "      <td>CCN1CCC[C@@H]1CN=C(O)c1cc(S(=O)(=O)CC)c(N)cc1OC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1340</td>\n",
       "      <td>583.6</td>\n",
       "      <td>InChI=1S/C9H7NO2/c11-8-3-1-2-7-6(8)4-5-10-9(7)...</td>\n",
       "      <td>Oc1cccc2c(O)nccc12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3344</td>\n",
       "      <td>579.0</td>\n",
       "      <td>InChI=1S/C15H20N2O2/c18-14-16-12-15(19-14)7-10...</td>\n",
       "      <td>OC1=NCC2(CCN(CCc3ccccc3)CC2)O1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80033</th>\n",
       "      <td>97733655</td>\n",
       "      <td>946.4</td>\n",
       "      <td>InChI=1S/C25H29N3O6S/c1-5-24(29)26-25-17(4)27-...</td>\n",
       "      <td>CCOc1ccc(OCC)c(NS(=O)(=O)c2ccc(/C=C/c3onc(C)c3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80034</th>\n",
       "      <td>98666786</td>\n",
       "      <td>653.1</td>\n",
       "      <td>InChI=1S/C17H24FN3O5S/c1-25-7-3-6-19-17(22)20-...</td>\n",
       "      <td>COCCCN=C(O)N1C[C@@H]2CN(S(=O)(=O)c3cccc(F)c3)C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80035</th>\n",
       "      <td>98670835</td>\n",
       "      <td>648.2</td>\n",
       "      <td>InChI=1S/C17H25N3O5S/c1-13-4-3-5-16(8-13)26(22...</td>\n",
       "      <td>COCCN=C(O)N1C[C@@H]2CN(S(=O)(=O)c3cccc(C)c3)C[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80036</th>\n",
       "      <td>98779314</td>\n",
       "      <td>783.9</td>\n",
       "      <td>InChI=1S/C21H25N3O4S/c1-15-7-9-19(10-8-15)29(2...</td>\n",
       "      <td>Cc1ccc(S(=O)(=O)N2C[C@@H]3CN(C(O)=Nc4ccccc4C)C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80037</th>\n",
       "      <td>99905419</td>\n",
       "      <td>603.7</td>\n",
       "      <td>InChI=1S/C14H16N2O2/c17-13-11-8-15-9-12(11)14(...</td>\n",
       "      <td>O=C1[C@H]2CNC[C@H]2C(=O)N1CCc1ccccc1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79957 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pubchem     rt                                              inchi  \\\n",
       "0          5139   93.5    InChI=1S/C3H8N2S/c1-2-6-3(4)5/h2H2,1H3,(H3,4,5)   \n",
       "1          3505  687.8  InChI=1S/C19H25Cl2N3O3/c1-27-19(26)23-8-9-24(1...   \n",
       "2          2159  590.7  InChI=1S/C17H27N3O4S/c1-4-20-8-6-7-12(20)11-19...   \n",
       "3          1340  583.6  InChI=1S/C9H7NO2/c11-8-3-1-2-7-6(8)4-5-10-9(7)...   \n",
       "4          3344  579.0  InChI=1S/C15H20N2O2/c18-14-16-12-15(19-14)7-10...   \n",
       "...         ...    ...                                                ...   \n",
       "80033  97733655  946.4  InChI=1S/C25H29N3O6S/c1-5-24(29)26-25-17(4)27-...   \n",
       "80034  98666786  653.1  InChI=1S/C17H24FN3O5S/c1-25-7-3-6-19-17(22)20-...   \n",
       "80035  98670835  648.2  InChI=1S/C17H25N3O5S/c1-13-4-3-5-16(8-13)26(22...   \n",
       "80036  98779314  783.9  InChI=1S/C21H25N3O4S/c1-15-7-9-19(10-8-15)29(2...   \n",
       "80037  99905419  603.7  InChI=1S/C14H16N2O2/c17-13-11-8-15-9-12(11)14(...   \n",
       "\n",
       "                                                  smiles  \n",
       "0                                              CCSC(=N)N  \n",
       "1      COC(=O)N1CCN(C(=O)Cc2ccc(Cl)c(Cl)c2)[C@H](CN2C...  \n",
       "2        CCN1CCC[C@@H]1CN=C(O)c1cc(S(=O)(=O)CC)c(N)cc1OC  \n",
       "3                                     Oc1cccc2c(O)nccc12  \n",
       "4                         OC1=NCC2(CCN(CCc3ccccc3)CC2)O1  \n",
       "...                                                  ...  \n",
       "80033  CCOc1ccc(OCC)c(NS(=O)(=O)c2ccc(/C=C/c3onc(C)c3...  \n",
       "80034  COCCCN=C(O)N1C[C@@H]2CN(S(=O)(=O)c3cccc(F)c3)C...  \n",
       "80035  COCCN=C(O)N1C[C@@H]2CN(S(=O)(=O)c3cccc(C)c3)C[...  \n",
       "80036  Cc1ccc(S(=O)(=O)N2C[C@@H]3CN(C(O)=Nc4ccccc4C)C...  \n",
       "80037               O=C1[C@H]2CNC[C@H]2C(=O)N1CCc1ccccc1  \n",
       "\n",
       "[79957 rows x 4 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_toolsets.utilities import predict_super\n",
    "from dl_toolsets.super_model import Super, train_super, test_super"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2048])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.ecfp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_toolsets.super_model import Super, Super_2\n",
    "model = Super_2(\n",
    "    node_feat_size=node_feat_size,\n",
    "    edge_feat_size=edge_feat_size,\n",
    "    num_layers=2,   \n",
    "    num_timesteps=2,\n",
    "    graph_feat_size=512,\n",
    "    feat_size = 256,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = predict_super(model, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, RMSE: 137.2999304722866\n",
      "Epoch 1, RMSE: 89.31967626617396\n",
      "Epoch 2, RMSE: 47.49396444092854\n",
      "Epoch 3, RMSE: 44.889405688223334\n",
      "Epoch 4, RMSE: 43.759443157577465\n",
      "Epoch 5, RMSE: 42.36692281570951\n",
      "Epoch 6, RMSE: 40.06884695461606\n",
      "Epoch 7, RMSE: 37.15672413914334\n",
      "Epoch 8, RMSE: 33.20362154691785\n",
      "Epoch 9, RMSE: 31.95407658915447\n",
      "Epoch 10, RMSE: 27.78972640137796\n",
      "Epoch 11, RMSE: 26.52137635454736\n",
      "Epoch 12, RMSE: 26.495725796691364\n",
      "Epoch 13, RMSE: 24.64473687817694\n",
      "Epoch 14, RMSE: 25.683790540434146\n",
      "Epoch 15, RMSE: 24.30784288228249\n",
      "Epoch 16, RMSE: 22.374993369596062\n",
      "Epoch 17, RMSE: 21.861748511059865\n",
      "Epoch 18, RMSE: 21.58254204972658\n",
      "Epoch 19, RMSE: 20.420013072478508\n",
      "Epoch 20, RMSE: 20.007457242473567\n",
      "Epoch 21, RMSE: 20.489749657786184\n",
      "Epoch 22, RMSE: 18.971629456012128\n",
      "Epoch 23, RMSE: 18.583820557416942\n",
      "Epoch 24, RMSE: 18.508366928431784\n",
      "Epoch 25, RMSE: 18.136229347904244\n",
      "Epoch 26, RMSE: 18.395138966959248\n",
      "Epoch 27, RMSE: 18.03172215571598\n",
      "Epoch 28, RMSE: 18.332009973799163\n",
      "Epoch 29, RMSE: 19.06525526401666\n",
      "Epoch 30, RMSE: 17.515892961089733\n",
      "Epoch 31, RMSE: 18.412029603374823\n",
      "Epoch 32, RMSE: 17.94262830918699\n",
      "Epoch 33, RMSE: 17.541992899887166\n",
      "Epoch 34, RMSE: 17.244845541633804\n",
      "Epoch 35, RMSE: 17.085374459654727\n",
      "Epoch 36, RMSE: 17.393462591127957\n",
      "Epoch 37, RMSE: 17.02597211417602\n",
      "Epoch 38, RMSE: 16.89352161865804\n",
      "Epoch 39, RMSE: 17.11356961783934\n",
      "Epoch 40, RMSE: 17.4368845918976\n",
      "Epoch 41, RMSE: 16.917189201029103\n",
      "Epoch 42, RMSE: 16.980328952305946\n",
      "Epoch 43, RMSE: 17.517842986245462\n",
      "Epoch 44, RMSE: 16.808869717987854\n",
      "Epoch 45, RMSE: 16.43280548067679\n",
      "Epoch 46, RMSE: 17.086316624416703\n",
      "Epoch 47, RMSE: 16.260133317264955\n",
      "Epoch 48, RMSE: 16.047933584016203\n",
      "Epoch 49, RMSE: 16.065854799765237\n",
      "Epoch 50, RMSE: 17.522618707073327\n",
      "Epoch 51, RMSE: 16.377133980853955\n",
      "Epoch 52, RMSE: 16.166582869798706\n",
      "Epoch 53, RMSE: 16.269208338549294\n",
      "Epoch 54, RMSE: 15.94843696100423\n",
      "Epoch 55, RMSE: 15.734596964425256\n",
      "Epoch 56, RMSE: 15.782865098778053\n",
      "Epoch 57, RMSE: 15.502154611403226\n",
      "Epoch 58, RMSE: 16.496899787901302\n",
      "Epoch 59, RMSE: 15.718965707154563\n",
      "Epoch 60, RMSE: 15.50488711951698\n",
      "Epoch 61, RMSE: 16.05676453265686\n",
      "Epoch 62, RMSE: 16.162611199987026\n",
      "Epoch 63, RMSE: 15.434424928861764\n",
      "early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from dl_toolsets.super_model import Super, Super_2\n",
    "# from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_list, batch_size=64, shuffle=True, \n",
    "                        #   collate_fn=collate\n",
    "                          )\n",
    "test_loader = DataLoader(test_list, batch_size=64, shuffle=False,)\n",
    "model = Super_2(\n",
    "    node_feat_size=node_feat_size,\n",
    "    edge_feat_size=edge_feat_size,\n",
    "    num_layers=2,   \n",
    "    num_timesteps=2,\n",
    "    graph_feat_size=512,\n",
    "    feat_size = 512,\n",
    ")\n",
    "\n",
    "model = model.to('cpu')\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "max_epoch = 500\n",
    "best_val_loss = float('inf')\n",
    "for i in range(max_epoch):\n",
    "    train_super( i,model, train_loader, loss_fn,optimizer)\n",
    "    val_rmse = test_super(model, test_loader)\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(model.state_dict(), 'models/best_super_2.pth')\n",
    "        saved = True\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the training statistics\n",
      "the testing rmse is 15.81350168580622, the mae is 5.657388687133789\n",
      "this is the testing statistics\n",
      "the testing rmse is 15.320936373520071, the mae is 5.836612701416016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(15.3209, dtype=torch.float64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('models/best_super_2.pth'))\n",
    "print('this is the training statistics')\n",
    "test_super(model, train_loader, verbose = True)\n",
    "print('this is the testing statistics')\n",
    "test_super(model, test_loader, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the training statistics\n",
      "the testing rmse is 15.097013631241326, the mae is 5.775764465332031\n",
      "this is the testing statistics\n",
      "the testing rmse is 14.391841980411714, the mae is 6.10150146484375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14.3918, dtype=torch.float64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.load_state_dict(torch.load('models/best_super_2.pth'))\n",
    "print('this is the training statistics')\n",
    "test_super(model, train_loader, verbose = True)\n",
    "print('this is the testing statistics')\n",
    "test_super(model, test_loader, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_toolsets.super_model import TransforerBlock\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.input_dim = input_dim  \n",
    "        self.conv1d = TransforerBlock(in_channel=self.input_dim, out_channel=self.input_dim)\n",
    "        self.feat_size = input_dim\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Linear(self.feat_size, self.feat_size // 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.BatchNorm1d(self.feat_size // 8),\n",
    "            nn.Linear(self.feat_size // 8, self.feat_size // 64),\n",
    "            # nn.BatchNorm1d(self.feat_size // 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.feat_size // 64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        return self.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_toolsets.super_model import train_super"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleMLP(input_dim=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, RMSE: 140.58605576658894\n",
      "Epoch 1, RMSE: 116.44050657074675\n",
      "Epoch 2, RMSE: 121.03515497466495\n",
      "Epoch 3, RMSE: 120.26551485801367\n",
      "Epoch 4, RMSE: 118.81958623579054\n",
      "Epoch 5, RMSE: 120.99330583059783\n",
      "Epoch 6, RMSE: 120.84813183974633\n",
      "Epoch 7, RMSE: 123.53865834598925\n",
      "Epoch 8, RMSE: 118.30198368856038\n",
      "Epoch 9, RMSE: 117.0678088123619\n",
      "Epoch 10, RMSE: 119.88779949804278\n",
      "Epoch 11, RMSE: 119.25275407936823\n",
      "Epoch 12, RMSE: 116.16510034898526\n",
      "Epoch 13, RMSE: 116.06474447613729\n",
      "Epoch 14, RMSE: 114.65902725044253\n",
      "Epoch 15, RMSE: 124.00225282498499\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[572], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epoch):\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mtrain_super\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     val_rmse \u001b[38;5;241m=\u001b[39m test_super(model, test_loader, \u001b[38;5;28msuper\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_rmse \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "File \u001b[0;32m~/Documents/GitHub/Rhoeto/dl_toolsets/super_model.py:23\u001b[0m, in \u001b[0;36mtrain_super\u001b[0;34m(epoch, model, data_loader, loss_criterion, optimizer, device, super)\u001b[0m\n\u001b[1;32m     21\u001b[0m     total_items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m epoch_rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(total_loss\u001b[38;5;241m/\u001b[39mtotal_items)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = model.to('cpu')\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "max_epoch = 1000\n",
    "best_val_loss = float('inf')\n",
    "for i in range(max_epoch):\n",
    "    train_super( i,model, train_loader, loss_fn,optimizer, super = False)\n",
    "    val_rmse = test_super(model, test_loader, super = False)\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(model.state_dict(), 'models/best_super.pth')\n",
    "        saved = True\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the training statistics\n",
      "the testing rmse is 15.406994999095351, the mae is 6.999534606933594\n",
      "this is the testing statistics\n",
      "the testing rmse is 16.190607035764003, the mae is 6.488086700439453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(16.1906, dtype=torch.float64)"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/best_super.pth'))\n",
    "print('this is the training statistics')\n",
    "test_super(model, train_loader, verbose = True, super = False)\n",
    "print('this is the testing statistics')\n",
    "test_super(model, test_loader, verbose = True, super= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training rmse: 137.46802568033874\n",
      "Epoch 1, training rmse: 120.83760954015821\n",
      "Epoch 2, training rmse: 75.56717185693189\n",
      "Epoch 3, training rmse: 58.73008380625916\n",
      "Epoch 4, training rmse: 52.54098131974022\n",
      "Epoch 5, training rmse: 47.99334741634914\n",
      "Epoch 6, training rmse: 44.197181469263995\n",
      "Epoch 7, training rmse: 40.32111023617902\n",
      "Epoch 8, training rmse: 36.9559742046515\n",
      "Epoch 9, training rmse: 34.16508074293935\n",
      "Epoch 10, training rmse: 31.793066285341027\n",
      "Epoch 11, training rmse: 29.884330364050292\n",
      "Epoch 12, training rmse: 28.422565152469264\n",
      "Epoch 13, training rmse: 26.95229546940466\n",
      "Epoch 14, training rmse: 25.836184311329312\n",
      "Epoch 15, training rmse: 24.74795703408616\n",
      "Epoch 16, training rmse: 23.973106368501675\n",
      "Epoch 17, training rmse: 23.210600356839137\n",
      "Epoch 18, training rmse: 22.751336055708794\n",
      "Epoch 19, training rmse: 22.263768090925712\n",
      "Epoch 20, training rmse: 21.742844470096163\n",
      "Epoch 21, training rmse: 21.36221382596325\n",
      "Epoch 22, training rmse: 21.022285974879576\n",
      "Epoch 23, training rmse: 20.912300427369285\n",
      "Epoch 24, training rmse: 20.846792963329946\n",
      "Epoch 25, training rmse: 20.5261196476238\n",
      "Epoch 26, training rmse: 20.36471042829116\n",
      "Epoch 27, training rmse: 20.142267657641437\n",
      "Epoch 28, training rmse: 20.005248983170066\n",
      "Epoch 29, training rmse: 19.892116280817085\n",
      "Epoch 30, training rmse: 19.927394442664465\n",
      "Epoch 31, training rmse: 19.74498724612629\n",
      "Epoch 32, training rmse: 19.810269171763736\n",
      "Epoch 33, training rmse: 19.420581038488702\n",
      "Epoch 34, training rmse: 19.406161094616817\n",
      "Epoch 35, training rmse: 19.27055969455441\n",
      "Epoch 36, training rmse: 19.453556302933052\n",
      "Epoch 37, training rmse: 19.370973841863545\n",
      "Epoch 38, training rmse: 19.104577009506325\n",
      "Epoch 39, training rmse: 19.060058089359355\n",
      "Epoch 40, training rmse: 19.17436979674161\n",
      "Epoch 41, training rmse: 18.985381122396916\n",
      "Epoch 42, training rmse: 18.999221690187863\n",
      "Epoch 43, training rmse: 18.88034526478055\n",
      "Epoch 44, training rmse: 18.76638095835354\n",
      "Epoch 45, training rmse: 18.692461752499778\n",
      "Epoch 46, training rmse: 18.862904615593248\n",
      "Epoch 47, training rmse: 18.66676902965231\n",
      "Epoch 48, training rmse: 18.594674171557628\n",
      "Epoch 49, training rmse: 18.583062937208783\n",
      "Epoch 50, training rmse: 18.65981230362198\n",
      "Epoch 51, training rmse: 18.648239383757133\n",
      "Epoch 52, training rmse: 18.533192022067986\n",
      "Epoch 53, training rmse: 18.43519364912609\n",
      "Epoch 54, training rmse: 18.44974847408546\n",
      "Epoch 55, training rmse: 18.577881102673608\n",
      "Epoch 56, training rmse: 18.46678256311896\n",
      "early stopping triggered\n",
      "this is the training statistics\n",
      "the rmse is 18.478744254038556, the mae is 7.2935638427734375\n",
      "this is the testing statistics\n",
      "the rmse is 22.63047663114798, the mae is 10.67609453201294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(22.6305, dtype=torch.float64)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model = SimpleMLP(input_dim=train_df.tensors[0].shape[1])\n",
    "fc_model = fc_model.to('cpu')\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(fc_model.parameters(), lr=0.001)\n",
    "max_epoch = 500\n",
    "best_val_loss = float('inf')\n",
    "for i in range(max_epoch):\n",
    "    train_simple_nn(fc_model, train_loader, i, optimizer, loss_fn)\n",
    "    val_rmse = test_simple_nn(fc_model, test_loader, loss_fn)\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(fc_model.state_dict(), 'models/best_ecfp_featurizer.pth')\n",
    "        saved = True\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break\n",
    "fc_model.load_state_dict(torch.load('models/best_ecfp_featurizer.pth'))\n",
    "print('this is the training statistics')\n",
    "test_simple_nn(fc_model, train_loader, loss_fn, verbose = True)\n",
    "print('this is the testing statistics')\n",
    "test_simple_nn(fc_model, test_loader, loss_fn, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputNN(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, input_dim3, hidden_dim1, hidden_dim2, hidden_dim3, final_hidden_dim):\n",
    "        super(MultiInputNN, self).__init__()\n",
    "        \n",
    "        # Fully connected layers for each input set\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_dim1, hidden_dim1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(input_dim2, hidden_dim2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(input_dim3, hidden_dim3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer to fuse the outputs\n",
    "        self.fc_final = nn.Sequential(\n",
    "            nn.Linear(hidden_dim1 + hidden_dim2 + hidden_dim3, final_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_hidden_dim, 1)  # Assuming a regression task, change output accordingly\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        # Forward pass for each set of features\n",
    "        out1 = self.fc1(x1)\n",
    "        out2 = self.fc2(x2)\n",
    "        out3 = self.fc3(x3)\n",
    "        \n",
    "        # Concatenate the outputs\n",
    "        combined = torch.cat((out1, out2, out3), dim=1)\n",
    "        \n",
    "        # Pass through the final fully connected layer\n",
    "        output = self.fc_final(combined)\n",
    "        \n",
    "        return output\n",
    "input_dim1, input_dim2, input_dim3 = x1.shape[1], x2.shape[1], x3.shape[1]\n",
    "hidden_dim1, hidden_dim2, hidden_dim3 = 512, 512, 512\n",
    "final_hidden_dim = 256\n",
    "multi_model = MultiInputNN(input_dim1, input_dim2, input_dim3, hidden_dim1, hidden_dim2, hidden_dim3, final_hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_train(model, train_loader, i, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    for data in (train_loader):\n",
    "        \n",
    "        x1, x2, x3, y = data\n",
    "        batch_n = len(y)\n",
    "        x1, x2,x3,y = x1.to('cpu'), x2.to('cpu'), x3.to('cpu'), y.to('cpu')\n",
    "        out = model(x1, x2, x3)\n",
    "        loss = loss_fn(out, y.view(-1, 1))\n",
    "        total_loss += loss.item()*batch_n\n",
    "        total_items += batch_n\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    median_loss = total_loss / total_items\n",
    "    print('Epoch: {}, Loss: {:.4f}'.format(i, np.sqrt(median_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_test(model, test_loader,loss_criterion, return_values = False, verbose = False, return_rmse = True):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    refs = []\n",
    "    with torch.no_grad():\n",
    "        for data in (test_loader):\n",
    "            x1, x2, x3, y = data\n",
    "            batch_n = len(y)\n",
    "            x1, x2,x3,y = x1.to('cpu'), x2.to('cpu'), x3.to('cpu'), y.to('cpu')\n",
    "            refs.extend(y.view(1,-1).detach().cpu().numpy()[0])\n",
    "            \n",
    "            out = model(x1, x2, x3)\n",
    "            predictions.extend(out.view(1,-1).detach().cpu().numpy()[0])\n",
    "    # mse = loss_criterion(torch.tensor(predictions), torch.tensor(refs))\n",
    "    # mae = \n",
    "    ref = np.array(refs)\n",
    "    predictions = np.array(predictions)\n",
    "    if return_rmse:\n",
    "        return np.sqrt(mse.item())\n",
    "    if verbose:\n",
    "        print('RMSE: {:.4f}'.format(get_rmse(predictions, refs)))\n",
    "        print('MAE: {:.4f}'.format(get_mae(predictions, refs)))\n",
    "    if return_values:\n",
    "        return refs, predictions\n",
    "    \n",
    "    return ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Subclasses of Dataset should implement __getitem__.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[271], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m (train_loader):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/utils/data/dataset.py:60\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_co:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubclasses of Dataset should implement __getitem__.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Subclasses of Dataset should implement __getitem__."
     ]
    }
   ],
   "source": [
    "for data in (train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 16.4725\n",
      "Epoch: 1, Loss: 15.9498\n",
      "Epoch: 2, Loss: 16.4371\n",
      "Epoch: 3, Loss: 15.9361\n",
      "Epoch: 4, Loss: 15.6434\n",
      "Epoch: 5, Loss: 15.7950\n",
      "Epoch: 6, Loss: 15.5161\n",
      "Epoch: 7, Loss: 15.6414\n",
      "Epoch: 8, Loss: 15.7681\n",
      "Epoch: 9, Loss: 15.4031\n",
      "Epoch: 10, Loss: 15.2031\n",
      "Epoch: 11, Loss: 15.4906\n",
      "Epoch: 12, Loss: 15.4010\n",
      "Epoch: 13, Loss: 15.0755\n",
      "Epoch: 14, Loss: 14.9934\n",
      "Epoch: 15, Loss: 14.9015\n",
      "Epoch: 16, Loss: 15.0209\n",
      "Epoch: 17, Loss: 14.9867\n",
      "early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "multi_model = multi_model.to('cpu')\n",
    "optimizer = torch.optim.Adam(multi_model.parameters(), lr=0.0001, weight_decay=0.0003126662000605776)\n",
    "max_epoch = 500\n",
    "best_val_loss = float('inf')\n",
    "saved = False\n",
    "\n",
    "for i in range(0, max_epoch):\n",
    "    multi_train(multi_model, train_loader, i, optimizer, loss_fn)\n",
    "    val_rmse = multi_test(multi_model, test_loader, loss_fn, return_rmse = True)\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(multi_model.state_dict(), 'models/best_multi_model.pth')\n",
    "        saved = True\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break\n",
    "    # val_rmse = test(model, test_loader, loss_fn).item()\n",
    "    # if val_rmse < best_val_loss:\n",
    "    #     best_val_loss = val_rmse\n",
    "    #     torch.save(model.state_dict(), 'models/multimodel.pth')\n",
    "    #     saved = True\n",
    "    #     patience = 10\n",
    "    # else:\n",
    "    #     patience -= 1\n",
    "    #     if patience == 0:\n",
    "    #         print('early stopping triggered')\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 15.9418\n",
      "MAE: 7.8669\n",
      "RMSE: 21.0632\n",
      "MAE: 15.5596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_model.load_state_dict(torch.load('models/best_multi_model.pth'))\n",
    "multi_test(multi_model, train_loader, loss_fn, return_rmse = False, verbose=True)\n",
    "multi_test(multi_model, test_loader, loss_fn, return_rmse = False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_nn(model, data_loader,epoch, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    for data in data_loader:\n",
    "        x ,y = data\n",
    "        x, y = x.to('cpu'), y.to('cpu')\n",
    "        out = model(x)\n",
    "        loss = (loss_fn(out, y.view(-1,1)))\n",
    "        total_loss += loss.item()*len(y)\n",
    "        total_items += len(y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    median_loss = total_loss/total_items\n",
    "    print(f'this is eopch {epoch} and the rmse is {np.sqrt(median_loss)}')\n",
    "    # return total_loss / len(data_loader)\n",
    "def test_simple_nn(model, data_loader, loss_fn, verbose = False, return_values = False, return_rmse = True):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    refs = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            x, y = data\n",
    "            x, y = x.to('cpu'), y.to('cpu')\n",
    "            out = model(x)\n",
    "            predictions.extend(out)\n",
    "            refs.extend(y)\n",
    "    predictions = np.array([i.item() for i in predictions])\n",
    "    refs = np.array([i.item() for i in refs])\n",
    "    mse = loss_fn(torch.tensor(predictions), torch.tensor(refs))\n",
    "    mae = np.median(abs(predictions - refs))\n",
    "    if verbose:\n",
    "        print(f'the rmse is {np.sqrt(mse)}, the mae is {mae}')\n",
    "    if return_values:\n",
    "        return predictions, refs\n",
    "    if return_rmse:\n",
    "        return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naive attentivefp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is epoch 0,the loss is 134.69096098358034\n",
      "this is epoch 1,the loss is 115.18462688041235\n",
      "this is epoch 2,the loss is 71.61897914433274\n",
      "this is epoch 3,the loss is 46.587294314529565\n",
      "this is epoch 4,the loss is 45.21098128557371\n",
      "this is epoch 5,the loss is 44.694036842174796\n",
      "this is epoch 6,the loss is 44.05779860818883\n",
      "this is epoch 7,the loss is 43.26362143937483\n",
      "this is epoch 8,the loss is 42.27931006136783\n",
      "this is epoch 9,the loss is 41.0304647866085\n",
      "this is epoch 10,the loss is 39.05981149329075\n",
      "this is epoch 11,the loss is 37.558453800485985\n",
      "this is epoch 12,the loss is 35.16514399268827\n",
      "this is epoch 13,the loss is 32.570484328220715\n",
      "this is epoch 14,the loss is 28.479286431008198\n",
      "this is epoch 15,the loss is 26.65709916924575\n",
      "this is epoch 16,the loss is 25.026698716098053\n",
      "this is epoch 17,the loss is 24.57520639901532\n",
      "this is epoch 18,the loss is 23.478316154587816\n",
      "this is epoch 19,the loss is 24.25536729447341\n",
      "this is epoch 20,the loss is 22.439020271090204\n",
      "this is epoch 21,the loss is 22.23766576164113\n",
      "this is epoch 22,the loss is 22.016320252140872\n",
      "this is epoch 23,the loss is 21.052656772670836\n",
      "this is epoch 24,the loss is 20.425051633202152\n",
      "this is epoch 25,the loss is 20.25542279470731\n",
      "this is epoch 26,the loss is 20.09727204353584\n",
      "this is epoch 27,the loss is 20.44727355434847\n",
      "this is epoch 28,the loss is 21.09163602253823\n",
      "this is epoch 29,the loss is 19.69339066160356\n",
      "this is epoch 30,the loss is 19.40837669008406\n",
      "this is epoch 31,the loss is 19.387388185386726\n",
      "this is epoch 32,the loss is 19.075400006056427\n",
      "this is epoch 33,the loss is 18.60344614975558\n",
      "this is epoch 34,the loss is 18.385687427535537\n",
      "this is epoch 35,the loss is 19.015281972780457\n",
      "this is epoch 36,the loss is 18.69153685812477\n",
      "this is epoch 37,the loss is 18.198152633569375\n",
      "this is epoch 38,the loss is 18.222148306696507\n",
      "this is epoch 39,the loss is 18.004901418562532\n",
      "this is epoch 40,the loss is 18.022668467377045\n",
      "this is epoch 41,the loss is 18.04859800461064\n",
      "this is epoch 42,the loss is 18.012956135881176\n",
      "this is epoch 43,the loss is 18.180764333035707\n",
      "this is epoch 44,the loss is 18.08323341022317\n",
      "this is epoch 45,the loss is 17.52592085715809\n",
      "this is epoch 46,the loss is 17.703416165117137\n",
      "this is epoch 47,the loss is 17.576900866507298\n",
      "early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "model = AttentiveFPPredictor(node_feat_size=node_feat_size,\n",
    "                                  edge_feat_size=edge_feat_size,\n",
    "                                  num_layers=2,\n",
    "                                  num_timesteps=2,\n",
    "                                  graph_feat_size=256,\n",
    "                                  n_tasks=1,\n",
    "                                  dropout=0.0)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "model = model.to('cpu')\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0003126662000605776)\n",
    "max_epoch = 500\n",
    "best_val_loss = float('inf')\n",
    "saved = False\n",
    "\n",
    "for i in range(0, max_epoch):\n",
    "    train(model, train_loader, i, optimizer, loss_fn)\n",
    "    val_rmse = test(model, test_loader, loss_fn).item()\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(model.state_dict(), 'models/naive_attentivefp.pth')\n",
    "        saved = True\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training statistics are: \n",
      "the rmse is 18.092162453693163, the mae is 7.616363525390625\n",
      "the testing statistics are: \n",
      "the rmse is 16.783491785338907, the mae is 7.748635768890381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/naive_attentivefp.pth'))\n",
    "print('the training statistics are: ')\n",
    "test(model, train_loader, loss_fn, verbose = True, return_rmse=False)\n",
    "print('the testing statistics are: ')\n",
    "test(model, test_loader, loss_fn, verbose = True,return_rmse=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
