{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import dgl\n",
    "from collections import defaultdict\n",
    "from dgl.nn.pytorch.glob import AvgPooling\n",
    "from dgllife.model import load_pretrained\n",
    "from dgllife.utils import mol_to_bigraph, PretrainAtomFeaturizer, PretrainBondFeaturizer, AttentiveFPAtomFeaturizer,AttentiveFPBondFeaturizer, CanonicalAtomFeaturizer, CanonicalBondFeaturizer\n",
    "from rdkit import Chem\n",
    "from dgllife.model import AttentiveFPPredictor\n",
    "from dgllife.utils import Meter, EarlyStopping, SMILESToBigraph\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from dgllife.model import load_pretrained\n",
    "from dgllife.utils import CanonicalAtomFeaturizer, CanonicalBondFeaturizer\n",
    "import toolsets.torch_utilities as tu\n",
    "import dgl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alc = pd.read_csv('mini_dataset/train_alc.csv')\n",
    "test_alc = pd.read_csv('mini_dataset/test_alc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_construction_and_featurization(smiles):\n",
    "    \"\"\"Construct graphs from SMILES and featurize them\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : list of str\n",
    "        SMILES of molecules for embedding computation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of DGLGraph\n",
    "        List of graphs constructed and featurized\n",
    "    list of bool\n",
    "        Indicators for whether the SMILES string can be\n",
    "        parsed by RDKit\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    success = []\n",
    "    for smi in smiles:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol is None:\n",
    "                success.append(False)\n",
    "                continue\n",
    "            g = mol_to_bigraph(mol, add_self_loop=True,\n",
    "                               node_featurizer=PretrainAtomFeaturizer(),\n",
    "                               edge_featurizer=PretrainBondFeaturizer(),\n",
    "                               canonical_atom_order=False)\n",
    "            graphs.append(g)\n",
    "            success.append(True)\n",
    "        except:\n",
    "            success.append(False)\n",
    "\n",
    "    return graphs, success\n",
    "\n",
    "def collate(graphs):\n",
    "    return dgl.batch(graphs)\n",
    "def test(model, test_loader, verbose = False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for i, (X_batch, y_batch) in enumerate(test_loader):\n",
    "            X_batch = X_batch.to('cpu')\n",
    "            y_batch = y_batch.to('cpu')\n",
    "            output = model(X_batch)\n",
    "            y_true.append(y_batch)\n",
    "            y_pred.append(output)\n",
    "        y_true = torch.cat(y_true, dim=0)\n",
    "        y_pred = torch.cat(y_pred, dim=0)\n",
    "    if verbose:\n",
    "        print(f'the rmse is {get_rmse(y_true.numpy(), y_pred.numpy()):.4f}')\n",
    "        print(f'the mae is {get_mae(y_true.numpy(), y_pred.numpy()):.4f}')\n",
    "    return y_true.numpy(), y_pred.numpy()\n",
    "def train(model, train_loader,epoch, optimizer, criterion, scheduler = None):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_ns = 0\n",
    "    for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        X_batch = X_batch.to('cpu')\n",
    "        y_batch = y_batch.to('cpu')\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += loss.detach().item()*y_batch.shape[0]\n",
    "        epoch_ns += y_batch.shape[0]\n",
    "    print(f'Epoch {epoch}, Loss: {np.sqrt(epoch_loss/epoch_ns) :.4f}')\n",
    "def get_mae(y_true, y_pred):\n",
    "    mae = np.median(np.abs(y_pred - y_true))\n",
    "    return mae\n",
    "def get_rmse(y_true, y_pred):\n",
    "    rmse = np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
    "    return rmse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_weights = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, last_hidden_state):\n",
    "        # last_hidden_state shape: (batch_size, sequence_length, hidden_size)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_scores = self.attention_weights(last_hidden_state)  # (batch_size, sequence_length, 1)\n",
    "        attn_scores = attn_scores.squeeze(-1)  # (batch_size, sequence_length)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)  # (batch_size, sequence_length)\n",
    "        \n",
    "        # Multiply hidden states by the attention weights\n",
    "        weighted_hidden_state = last_hidden_state * attn_weights.unsqueeze(-1)  # (batch_size, sequence_length, hidden_size)\n",
    "        \n",
    "        # Sum the weighted hidden states to get the final representation\n",
    "        pooled_output = weighted_hidden_state.sum(dim=1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        return pooled_output\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "def make_clf_embeddings(smiles, attention_pooling=False, default_pooler = False):\n",
    "    checkpoint = 'unikei/bert-base-smiles'\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(checkpoint)\n",
    "    model = BertModel.from_pretrained(checkpoint)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings_all = torch.empty(0)\n",
    "        for s in tqdm(smiles):\n",
    "            tokens = tokenizer(s, return_tensors='pt')\n",
    "            predictions = model(**tokens)\n",
    "            if attention_pooling == True:\n",
    "                attention_pooling = AttentionPooling(hidden_size=768)\n",
    "                embeddings = attention_pooling(predictions.last_hidden_state)\n",
    "            elif default_pooler == True:\n",
    "                embeddings = predictions.pooler_output\n",
    "            else:\n",
    "                embeddings = predictions[0][:,0]\n",
    "            embeddings_all = torch.cat((embeddings_all, embeddings[0].unsqueeze(0)), dim=0)\n",
    "        \n",
    "    return embeddings_all.detach().numpy()\n",
    "def make_dataset(embeddings, labels):\n",
    "    labels = torch.from_numpy(labels.astype(np.float32))\n",
    "    labels  = labels.view(labels.shape[0],1)\n",
    "    embeddings = torch.from_numpy(embeddings.astype(np.float32))\n",
    "    dataset = TensorDataset( embeddings, labels)\n",
    "    return dataset\n",
    "def gin_featurizers(smiles):\n",
    "    \"\"\"Construct graphs from SMILES and featurize them\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : list of str\n",
    "        SMILES of molecules for embedding computation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of DGLGraph\n",
    "        List of graphs constructed and featurized\n",
    "    list of bool\n",
    "        Indicators for whether the SMILES string can be\n",
    "        parsed by RDKit\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    success = []\n",
    "    for smi in smiles:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol is None:\n",
    "                success.append(False)\n",
    "                continue\n",
    "            g = mol_to_bigraph(mol, add_self_loop=True,\n",
    "                               node_featurizer=PretrainAtomFeaturizer(),\n",
    "                               edge_featurizer=PretrainBondFeaturizer(),\n",
    "                               canonical_atom_order=False)\n",
    "            graphs.append(g)\n",
    "            success.append(True)\n",
    "        except:\n",
    "            success.append(False)\n",
    "\n",
    "    return graphs, success\n",
    "\n",
    "def collate(graphs):\n",
    "    return dgl.batch(graphs)\n",
    "def make_gin_embedding(smiles, model_name='gin_supervised_contextpred'):\n",
    "    gin_feats = gin_featurizers(smiles)\n",
    "    dataset = [gin_feats[0][x] for x in range(len(gin_feats[1])) if gin_feats[1][x] == True]# if there is fail?\n",
    "    args = {\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'model': model_name,\n",
    "        'batch_size': 128\n",
    "    }\n",
    "    data_loader = DataLoader(dataset, batch_size=args['batch_size'],\n",
    "                             collate_fn=collate, shuffle=False)\n",
    "    \n",
    "    model = load_pretrained(args['model']).to(args['device'])\n",
    "    model.eval()\n",
    "    readout = AvgPooling()\n",
    "    mol_emb = []\n",
    "    for batch_id, bg in enumerate(data_loader):\n",
    "        print('Processing batch {:d}/{:d}'.format(batch_id + 1, len(data_loader)))\n",
    "        bg = bg.to(args['device'])\n",
    "        nfeats = [bg.ndata.pop('atomic_number').to(args['device']),\n",
    "                    bg.ndata.pop('chirality_type').to(args['device'])]\n",
    "        efeats = [bg.edata.pop('bond_type').to(args['device']),\n",
    "                    bg.edata.pop('bond_direction_type').to(args['device'])]\n",
    "        with torch.no_grad():\n",
    "            node_repr = model(bg, nfeats, efeats)\n",
    "        mol_emb.append(readout(bg, node_repr))\n",
    "    mol_emb = torch.cat(mol_emb, dim=0).detach().cpu().numpy()\n",
    "    return mol_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rdkit_featurzier(smiles, use_BCUT = False):\n",
    "#     descriptors_all = []\n",
    "#     for s in tqdm(smiles):\n",
    "#         mol = Chem.MolFromSmiles(s)\n",
    "#         descriptor_single = []\n",
    "#         for nm,fn in Descriptors._descList:\n",
    "#             # some of the descriptor fucntions can throw errors if they fail, catch those here:\n",
    "#             if 'BCUT' in nm and use_BCUT == False:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 val = fn(mol)\n",
    "#             if np.isinf(val):\n",
    "#                 print(s, nm)\n",
    "#                 return\n",
    "#             descriptor_single.append(val)\n",
    "#         descriptors_all.append(descriptor_single)\n",
    "#     return np.array(descriptors_all)\n",
    "\n",
    "\n",
    "def rdkit_featurzier(smiles, use_BCUT = False):\n",
    "    import deepchem as dc\n",
    "    all_descriptors = {name: func for name, func in Descriptors.descList}\n",
    "    all_names = list((all_descriptors.keys()))\n",
    "    if use_BCUT == False:\n",
    "        all_names = [x for x in all_names if x.startswith('BCUT2D_')== False]\n",
    "    f2 = dc.feat.RDKitDescriptors(use_bcut2d= use_BCUT)\n",
    "    feats2 = f2.featurize(smiles)\n",
    "    return(feats2)\n",
    "    # f2 = dc.feat.RDKitDescriptors(use_bcut2d= use_BCUT)\n",
    "    # feats2 = f2.featurize(smiles)\n",
    "    # return(feats2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_retip_md = pd.read_csv('mini_dataset/train_alc_descriptors.csv')\n",
    "test_retip_md = pd.read_csv('mini_dataset/test_alc_descriptors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_retip_md_numpy = train_retip_md.drop(\"rt\", axis=1).values\n",
    "test_retip_md_numpy = test_retip_md.drop(\"rt\", axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gin_supervised_contextpred_pre_trained.pth from https://data.dgl.ai/dgllife/pre_trained/gin_supervised_contextpred.pth...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97b29b9562a42e78fc7d2cd7ca83171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gin_supervised_contextpred_pre_trained.pth:   0%|          | 0.00/7.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded\n",
      "Processing batch 1/11\n",
      "Processing batch 2/11\n",
      "Processing batch 3/11\n",
      "Processing batch 4/11\n",
      "Processing batch 5/11\n",
      "Processing batch 6/11\n",
      "Processing batch 7/11\n",
      "Processing batch 8/11\n",
      "Processing batch 9/11\n",
      "Processing batch 10/11\n",
      "Processing batch 11/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1287/1287 [00:49<00:00, 25.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gin_supervised_contextpred_pre_trained.pth from https://data.dgl.ai/dgllife/pre_trained/gin_supervised_contextpred.pth...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c49c64e79cb4561bede738e2ac9f3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gin_supervised_contextpred_pre_trained.pth:   0%|          | 0.00/7.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded\n",
      "Processing batch 1/3\n",
      "Processing batch 2/3\n",
      "Processing batch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:12<00:00, 26.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# rdkit_descriptor = rdkit_featurzier(train_alc['smiles'], use_BCUT=False)\n",
    "gin_embedding = make_gin_embedding(train_alc['smiles'])\n",
    "cls_embedding = make_clf_embeddings(train_alc['smiles'])\n",
    "features_all = np.concatenate((\n",
    "                                # rdkit_descriptor,\n",
    "                                train_retip_md_numpy, \n",
    "                                gin_embedding, cls_embedding\n",
    "                                ), axis=1)\n",
    "train_data = make_dataset(features_all, train_alc['rt'].values)\n",
    "# rdkit_descriptor = rdkit_featurzier(test_alc['smiles'], use_BCUT=False)\n",
    "gin_embedding = make_gin_embedding(test_alc['smiles'])\n",
    "cls_embedding = make_clf_embeddings(test_alc['smiles'])\n",
    "features_all = np.concatenate((\n",
    "                                # rdkit_descriptor, \n",
    "                                test_retip_md_numpy, \n",
    "                                gin_embedding, cls_embedding\n",
    "                                ), axis=1)\n",
    "test_data = make_dataset(features_all, test_alc['rt'].values)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunedModel(nn.Module):\n",
    "    def __init__(self, input_size=300, num_classes=1):\n",
    "        super(FineTunedModel, self).__init__()\n",
    "        self.meta_fc = nn.Sequential(\n",
    "                    nn.Linear(in_features=input_size, out_features=1024),\n",
    "                    # nn.BatchNorm1d(512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(in_features=1024, out_features=512),\n",
    "                    # nn.BatchNorm1d(256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(in_features=512, out_features=num_classes),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.meta_fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 26060.6113\n",
      "Epoch 1, Loss: 10366622.2252\n",
      "Epoch 2, Loss: 447279.3774\n",
      "Epoch 3, Loss: 740489.7345\n",
      "Epoch 4, Loss: 808359.1314\n",
      "Epoch 5, Loss: 1020205.7600\n",
      "Epoch 6, Loss: 546457.3390\n",
      "Epoch 7, Loss: 450216.9465\n",
      "Epoch 8, Loss: 504111.1495\n",
      "Epoch 9, Loss: 622998.0685\n",
      "Epoch 10, Loss: 703014.4047\n",
      "Epoch 11, Loss: 528765.0723\n",
      "Epoch 12, Loss: 401842.8069\n",
      "Epoch 13, Loss: 490998.7073\n",
      "Epoch 14, Loss: 632344.6830\n",
      "Epoch 15, Loss: 525511.2521\n",
      "Epoch 16, Loss: 233556.9044\n",
      "Epoch 17, Loss: 135611.5181\n",
      "Epoch 18, Loss: 202717.0058\n",
      "Epoch 19, Loss: 310452.8113\n",
      "Epoch 20, Loss: 503593.8544\n",
      "Epoch 21, Loss: 1875361.1944\n",
      "Epoch 22, Loss: 2766546.7761\n",
      "Epoch 23, Loss: 1595969.7974\n",
      "Epoch 24, Loss: 1192356.2094\n",
      "Epoch 25, Loss: 799468.7590\n",
      "Epoch 26, Loss: 359062.7437\n",
      "Epoch 27, Loss: 637151.6425\n",
      "Epoch 28, Loss: 830489.2283\n",
      "Epoch 29, Loss: 107620.4224\n",
      "Epoch 30, Loss: 2789312.7664\n",
      "Epoch 31, Loss: 8488167.5079\n",
      "Epoch 32, Loss: 3981049.7845\n",
      "Epoch 33, Loss: 16320662.7781\n",
      "Epoch 34, Loss: 7093952.0794\n",
      "Epoch 35, Loss: 10447055.3462\n",
      "Epoch 36, Loss: 41080400.7771\n",
      "Epoch 37, Loss: 3989825.5321\n",
      "Epoch 38, Loss: 21176.2043\n",
      "Epoch 39, Loss: 3065379.5794\n",
      "Epoch 40, Loss: 3802440.1905\n",
      "early stopping triggered\n",
      "the rmse is 1464196.2500\n",
      "the mae is 13.1895\n",
      "the rmse is 23.8522\n",
      "the mae is 12.6337\n"
     ]
    }
   ],
   "source": [
    "model = FineTunedModel(test_data.tensors[0].shape[1], 1)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 500\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and loss\n",
    "    train(model, train_loader,epoch, optimizer, criterion, scheduler=scheduler)\n",
    "    test_result = test(model, test_loader)\n",
    "    test_loss = get_rmse(test_result[0], test_result[1])\n",
    "    if test_loss < best_val_loss:\n",
    "        best_val_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience = 20\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "train_result = test(model, train_loader, verbose=True)\n",
    "test_result = test(model, test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rmse is 15.4126\n",
      "the mae is 6.5254\n"
     ]
    }
   ],
   "source": [
    "train_result = test(model, train_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rmse is 15.9331\n",
      "the mae is 7.3023\n"
     ]
    }
   ],
   "source": [
    "test_result = test(model, test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 137.0893\n",
      "Epoch 1, Loss: 127.2398\n",
      "Epoch 2, Loss: 103.8848\n",
      "Epoch 3, Loss: 78.9562\n",
      "Epoch 4, Loss: 71.0809\n",
      "Epoch 5, Loss: 68.5411\n",
      "Epoch 6, Loss: 65.9107\n",
      "Epoch 7, Loss: 63.4931\n",
      "Epoch 8, Loss: 61.0713\n",
      "Epoch 9, Loss: 58.8775\n",
      "Epoch 10, Loss: 56.9683\n",
      "Epoch 11, Loss: 55.2881\n",
      "Epoch 12, Loss: 53.9064\n",
      "Epoch 13, Loss: 52.6679\n",
      "Epoch 14, Loss: 51.3747\n",
      "Epoch 15, Loss: 50.2107\n",
      "Epoch 16, Loss: 48.9567\n",
      "Epoch 17, Loss: 47.8767\n",
      "Epoch 18, Loss: 46.7002\n",
      "Epoch 19, Loss: 45.4988\n",
      "Epoch 20, Loss: 44.3301\n",
      "Epoch 21, Loss: 43.0625\n",
      "Epoch 22, Loss: 41.7812\n",
      "Epoch 23, Loss: 40.5730\n",
      "Epoch 24, Loss: 39.3767\n",
      "Epoch 25, Loss: 38.4582\n",
      "Epoch 26, Loss: 37.3449\n",
      "Epoch 27, Loss: 36.3271\n",
      "Epoch 28, Loss: 35.2775\n",
      "Epoch 29, Loss: 34.4186\n",
      "Epoch 30, Loss: 33.4520\n",
      "Epoch 31, Loss: 32.6798\n",
      "Epoch 32, Loss: 31.9098\n",
      "Epoch 33, Loss: 31.2167\n",
      "Epoch 34, Loss: 30.4721\n",
      "Epoch 35, Loss: 29.8006\n",
      "Epoch 36, Loss: 29.1896\n",
      "Epoch 37, Loss: 28.6136\n",
      "Epoch 38, Loss: 28.0820\n",
      "Epoch 39, Loss: 27.4681\n",
      "Epoch 40, Loss: 27.0260\n",
      "Epoch 41, Loss: 26.6218\n",
      "Epoch 42, Loss: 26.1218\n",
      "Epoch 43, Loss: 25.7981\n",
      "Epoch 44, Loss: 25.3374\n",
      "Epoch 45, Loss: 25.0672\n",
      "Epoch 46, Loss: 24.8030\n",
      "Epoch 47, Loss: 24.4582\n",
      "Epoch 48, Loss: 24.1663\n",
      "Epoch 49, Loss: 23.8368\n",
      "Epoch 50, Loss: 23.4816\n",
      "Epoch 51, Loss: 23.2541\n",
      "Epoch 52, Loss: 23.0055\n",
      "Epoch 53, Loss: 22.8377\n",
      "Epoch 54, Loss: 22.7114\n",
      "Epoch 55, Loss: 22.5828\n",
      "Epoch 56, Loss: 22.1355\n",
      "Epoch 57, Loss: 22.1209\n",
      "Epoch 58, Loss: 21.8040\n",
      "Epoch 59, Loss: 21.6247\n",
      "Epoch 60, Loss: 21.8797\n",
      "Epoch 61, Loss: 21.4345\n",
      "Epoch 62, Loss: 21.0792\n",
      "Epoch 63, Loss: 20.9185\n",
      "Epoch 64, Loss: 20.7788\n",
      "Epoch 65, Loss: 20.6539\n",
      "Epoch 66, Loss: 20.4640\n",
      "Epoch 67, Loss: 20.4959\n",
      "Epoch 68, Loss: 20.2281\n",
      "Epoch 69, Loss: 20.0397\n",
      "Epoch 70, Loss: 19.9005\n",
      "Epoch 71, Loss: 19.8694\n",
      "Epoch 72, Loss: 19.7610\n",
      "Epoch 73, Loss: 19.5643\n",
      "Epoch 74, Loss: 19.5242\n",
      "Epoch 75, Loss: 19.4839\n",
      "Epoch 76, Loss: 19.3896\n",
      "Epoch 77, Loss: 19.2910\n",
      "Epoch 78, Loss: 19.1551\n",
      "Epoch 79, Loss: 19.0374\n",
      "Epoch 80, Loss: 18.8442\n",
      "Epoch 81, Loss: 19.0293\n",
      "Epoch 82, Loss: 18.6399\n",
      "Epoch 83, Loss: 18.6604\n",
      "Epoch 84, Loss: 18.6782\n",
      "Epoch 85, Loss: 18.6001\n",
      "Epoch 86, Loss: 18.6087\n",
      "Epoch 87, Loss: 18.3386\n",
      "Epoch 88, Loss: 18.2608\n",
      "Epoch 89, Loss: 18.0820\n",
      "Epoch 90, Loss: 18.0613\n",
      "Epoch 91, Loss: 17.9396\n",
      "Epoch 92, Loss: 17.7934\n",
      "Epoch 93, Loss: 17.8076\n",
      "Epoch 94, Loss: 17.7085\n",
      "Epoch 95, Loss: 17.5918\n",
      "Epoch 96, Loss: 17.5696\n",
      "Epoch 97, Loss: 17.6468\n",
      "Epoch 98, Loss: 17.5008\n",
      "Epoch 99, Loss: 17.3556\n",
      "Epoch 100, Loss: 17.2936\n",
      "Epoch 101, Loss: 17.6473\n",
      "Epoch 102, Loss: 17.2005\n",
      "Epoch 103, Loss: 17.2159\n",
      "Epoch 104, Loss: 17.0219\n",
      "Epoch 105, Loss: 16.9229\n",
      "Epoch 106, Loss: 16.9692\n",
      "Epoch 107, Loss: 16.8274\n",
      "Epoch 108, Loss: 16.8369\n",
      "Epoch 109, Loss: 16.7599\n",
      "Epoch 110, Loss: 16.6787\n",
      "Epoch 111, Loss: 16.6079\n",
      "Epoch 112, Loss: 16.6335\n",
      "Epoch 113, Loss: 16.5391\n",
      "Epoch 114, Loss: 16.4035\n",
      "Epoch 115, Loss: 16.3766\n",
      "Epoch 116, Loss: 16.3501\n",
      "Epoch 117, Loss: 16.2724\n",
      "Epoch 118, Loss: 16.3199\n",
      "Epoch 119, Loss: 16.2690\n",
      "Epoch 120, Loss: 16.2313\n",
      "Epoch 121, Loss: 16.0678\n",
      "Epoch 122, Loss: 16.0349\n",
      "Epoch 123, Loss: 16.0010\n",
      "Epoch 124, Loss: 15.9885\n",
      "Epoch 125, Loss: 16.0986\n",
      "Epoch 126, Loss: 15.9101\n",
      "Epoch 127, Loss: 15.8831\n",
      "Epoch 128, Loss: 15.8614\n",
      "Epoch 129, Loss: 15.8301\n",
      "Epoch 130, Loss: 15.7266\n",
      "Epoch 131, Loss: 15.6976\n",
      "Epoch 132, Loss: 15.6294\n",
      "Epoch 133, Loss: 15.6700\n",
      "Epoch 134, Loss: 15.5799\n",
      "Epoch 135, Loss: 15.5418\n",
      "Epoch 136, Loss: 15.4830\n",
      "Epoch 137, Loss: 15.5875\n",
      "Epoch 138, Loss: 15.3633\n",
      "Epoch 139, Loss: 15.3581\n",
      "Epoch 140, Loss: 15.3902\n",
      "Epoch 141, Loss: 15.3403\n",
      "Epoch 142, Loss: 15.2883\n",
      "Epoch 143, Loss: 15.2791\n",
      "Epoch 144, Loss: 15.1307\n",
      "Epoch 145, Loss: 15.0974\n",
      "Epoch 146, Loss: 15.1384\n",
      "Epoch 147, Loss: 15.1468\n",
      "Epoch 148, Loss: 15.1823\n",
      "Epoch 149, Loss: 15.3100\n",
      "Epoch 150, Loss: 15.0882\n",
      "Epoch 151, Loss: 14.8903\n",
      "Epoch 152, Loss: 14.8507\n",
      "Epoch 153, Loss: 14.7978\n",
      "Epoch 154, Loss: 14.8685\n",
      "Epoch 155, Loss: 14.8974\n",
      "Epoch 156, Loss: 15.1961\n",
      "Epoch 157, Loss: 14.7255\n",
      "Epoch 158, Loss: 14.7456\n",
      "Epoch 159, Loss: 14.7171\n",
      "Epoch 160, Loss: 14.7099\n",
      "Epoch 161, Loss: 14.5996\n",
      "Epoch 162, Loss: 14.5289\n",
      "Epoch 163, Loss: 14.6872\n",
      "Epoch 164, Loss: 14.7075\n",
      "Epoch 165, Loss: 14.4690\n",
      "Epoch 166, Loss: 14.5578\n",
      "Epoch 167, Loss: 14.4413\n",
      "Epoch 168, Loss: 14.4682\n",
      "Epoch 169, Loss: 14.5235\n",
      "Epoch 170, Loss: 14.4357\n",
      "Epoch 171, Loss: 14.3489\n",
      "Epoch 172, Loss: 14.3849\n",
      "Epoch 173, Loss: 14.4408\n",
      "Epoch 174, Loss: 14.2585\n",
      "Epoch 175, Loss: 14.2963\n",
      "Epoch 176, Loss: 14.2087\n",
      "Epoch 177, Loss: 14.1997\n",
      "Epoch 178, Loss: 14.2297\n",
      "Epoch 179, Loss: 14.0776\n",
      "Epoch 180, Loss: 14.1421\n",
      "Epoch 181, Loss: 14.1235\n",
      "Epoch 182, Loss: 14.3802\n",
      "Epoch 183, Loss: 14.1959\n",
      "Epoch 184, Loss: 13.9540\n",
      "Epoch 185, Loss: 13.9638\n",
      "Epoch 186, Loss: 14.0854\n",
      "Epoch 187, Loss: 14.1053\n",
      "Epoch 188, Loss: 13.9219\n",
      "Epoch 189, Loss: 13.9112\n",
      "Epoch 190, Loss: 13.9158\n",
      "Epoch 191, Loss: 14.5854\n",
      "Epoch 192, Loss: 14.0863\n",
      "Epoch 193, Loss: 13.9027\n",
      "Epoch 194, Loss: 14.1596\n",
      "Epoch 195, Loss: 13.8019\n",
      "Epoch 196, Loss: 13.6996\n",
      "Epoch 197, Loss: 13.7421\n",
      "Epoch 198, Loss: 13.8152\n",
      "Epoch 199, Loss: 13.7120\n",
      "Epoch 200, Loss: 13.6243\n",
      "Epoch 201, Loss: 13.6110\n",
      "Epoch 202, Loss: 13.7950\n",
      "Epoch 203, Loss: 13.5273\n",
      "Epoch 204, Loss: 13.6240\n",
      "Epoch 205, Loss: 13.6326\n",
      "Epoch 206, Loss: 13.6306\n",
      "Epoch 207, Loss: 13.8678\n",
      "Epoch 208, Loss: 13.9609\n",
      "Epoch 209, Loss: 13.5759\n",
      "Epoch 210, Loss: 13.5821\n",
      "Epoch 211, Loss: 13.5267\n",
      "Epoch 212, Loss: 13.4203\n",
      "Epoch 213, Loss: 13.3753\n",
      "Epoch 214, Loss: 13.4111\n",
      "Epoch 215, Loss: 13.3545\n",
      "Epoch 216, Loss: 13.4082\n",
      "Epoch 217, Loss: 13.5516\n",
      "Epoch 218, Loss: 13.3514\n",
      "Epoch 219, Loss: 13.2639\n",
      "Epoch 220, Loss: 13.3034\n",
      "Epoch 221, Loss: 13.3566\n",
      "Epoch 222, Loss: 13.4856\n",
      "Epoch 223, Loss: 13.4235\n",
      "Epoch 224, Loss: 13.1593\n",
      "Epoch 225, Loss: 13.3581\n",
      "Epoch 226, Loss: 13.2700\n",
      "Epoch 227, Loss: 13.1765\n",
      "Epoch 228, Loss: 13.1518\n",
      "Epoch 229, Loss: 13.1312\n",
      "Epoch 230, Loss: 13.0804\n",
      "Epoch 231, Loss: 13.1279\n",
      "Epoch 232, Loss: 13.1731\n",
      "Epoch 233, Loss: 13.1279\n",
      "Epoch 234, Loss: 13.2607\n",
      "Epoch 235, Loss: 13.0164\n",
      "Epoch 236, Loss: 13.0341\n",
      "Epoch 237, Loss: 13.0288\n",
      "Epoch 238, Loss: 13.0762\n",
      "Epoch 239, Loss: 12.9314\n",
      "Epoch 240, Loss: 12.9969\n",
      "Epoch 241, Loss: 13.0528\n",
      "Epoch 242, Loss: 13.0365\n",
      "Epoch 243, Loss: 13.0529\n",
      "Epoch 244, Loss: 12.9061\n",
      "Epoch 245, Loss: 12.9418\n",
      "Epoch 246, Loss: 12.9432\n",
      "Epoch 247, Loss: 12.9523\n",
      "Epoch 248, Loss: 12.7926\n",
      "Epoch 249, Loss: 12.7925\n",
      "Epoch 250, Loss: 12.7515\n",
      "Epoch 251, Loss: 12.9176\n",
      "Epoch 252, Loss: 12.7609\n",
      "Epoch 253, Loss: 12.7331\n",
      "Epoch 254, Loss: 12.8627\n",
      "early stopping triggered\n",
      "the rmse is 12.7933\n",
      "the mae is 5.9182\n",
      "the rmse is 14.7645\n",
      "the mae is 6.6388\n"
     ]
    }
   ],
   "source": [
    "model = FineTunedModel(1068, 1)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and loss\n",
    "    train(model, train_loader,epoch, optimizer, criterion)\n",
    "    test_result = test(model, test_loader)\n",
    "    test_loss = get_rmse(test_result[0], test_result[1])\n",
    "    if test_loss < best_val_loss:\n",
    "        best_val_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "train_result = test(model, train_loader, verbose=True)\n",
    "test_result = test(model, test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:08<00:00, 36.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gin_supervised_contextpred_pre_trained.pth from https://data.dgl.ai/dgllife/pre_trained/gin_supervised_contextpred.pth...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c6f59958234fbbb1b061f4b3ea081a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gin_supervised_contextpred_pre_trained.pth:   0%|          | 0.00/7.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded\n",
      "Processing batch 1/3\n",
      "Processing batch 2/3\n",
      "Processing batch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:12<00:00, 24.87it/s]\n"
     ]
    }
   ],
   "source": [
    "rdkit_descriptor = rdkit_featurzier(test_alc['smiles'], use_BCUT=False)\n",
    "gin_embedding = make_gin_embedding(test_alc['smiles'])\n",
    "cls_embedding = make_clf_embeddings(test_alc['smiles'])\n",
    "features_all = np.concatenate((rdkit_descriptor, gin_embedding, cls_embedding), axis=1)\n",
    "test_data = make_dataset(features_all, test_alc['rt'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: nan\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[329], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Forward pass and loss\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     train(model, train_loader,epoch, optimizer, criterion)\n\u001b[0;32m---> 11\u001b[0m     test_result \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m get_rmse(test_result[\u001b[38;5;241m0\u001b[39m], test_result[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[0;32mIn[173], line 46\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_loader, verbose)\u001b[0m\n\u001b[1;32m     44\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m y_true\u001b[38;5;241m.\u001b[39mappend(y_batch)\n\u001b[1;32m     48\u001b[0m y_pred\u001b[38;5;241m.\u001b[39mappend(output)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[173], line 84\u001b[0m, in \u001b[0;36mFineTunedModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 84\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     85\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     86\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "model = FineTunedModel(1270, 1)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and loss\n",
    "    train(model, train_loader,epoch, optimizer, criterion)\n",
    "    test_result = test(model, test_loader)\n",
    "    test_loss = get_rmse(test_result[0], test_result[1])\n",
    "    if test_loss < best_val_loss:\n",
    "        best_val_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "train_result = test(model, train_loader, verbose=True)\n",
    "test_result = test(model, test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rmse is 42.5045\n",
      "the mae is 25.1233\n"
     ]
    }
   ],
   "source": [
    "tt = test(model, train_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rmse is 43.4710\n",
      "the mae is 26.6265\n"
     ]
    }
   ],
   "source": [
    "tt = test(model, test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgllife.model.model_zoo.attentivefp_predictor import AttentiveFPPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgllife.data import MoleculeCSVDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alc = pd.read_csv('mini_dataset/train_alc.csv')\n",
    "test_alc = pd.read_csv('mini_dataset/test_alc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# below is test code for pretrained GIN representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_construction_and_featurization(smiles):\n",
    "    \"\"\"Construct graphs from SMILES and featurize them\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : list of str\n",
    "        SMILES of molecules for embedding computation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of DGLGraph\n",
    "        List of graphs constructed and featurized\n",
    "    list of bool\n",
    "        Indicators for whether the SMILES string can be\n",
    "        parsed by RDKit\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    success = []\n",
    "    for smi in smiles:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol is None:\n",
    "                success.append(False)\n",
    "                continue\n",
    "            g = mol_to_bigraph(mol, add_self_loop=True,\n",
    "                               node_featurizer=PretrainAtomFeaturizer(),\n",
    "                               edge_featurizer=PretrainBondFeaturizer(),\n",
    "                               canonical_atom_order=False)\n",
    "            graphs.append(g)\n",
    "            success.append(True)\n",
    "        except:\n",
    "            success.append(False)\n",
    "\n",
    "    return graphs, success\n",
    "\n",
    "def collate(graphs):\n",
    "    return dgl.batch(graphs)\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for i, (X_batch, y_batch) in enumerate(test_loader):\n",
    "            X_batch = X_batch.to('cpu')\n",
    "            y_batch = y_batch.to('cpu')\n",
    "            output = model(X_batch)\n",
    "            y_true.append(y_batch)\n",
    "            y_pred.append(output)\n",
    "        y_true = torch.cat(y_true, dim=0)\n",
    "        y_pred = torch.cat(y_pred, dim=0)\n",
    "    return y_true.numpy(), y_pred.numpy()\n",
    "def train(model, train_loader,epoch, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_ns = 0\n",
    "    for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        X_batch = X_batch.to('cpu')\n",
    "        y_batch = y_batch.to('cpu')\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()*y_batch.shape[0]\n",
    "        epoch_ns += y_batch.shape[0]\n",
    "    print(f'Epoch {epoch}, Loss: {np.sqrt(epoch_loss/epoch_ns) :.4f}')\n",
    "class FineTunedModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=300, num_classes=1):\n",
    "        super(FineTunedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.output = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, success = graph_construction_and_featurization(train_alc['smiles'])\n",
    "test_dataset, test_success = graph_construction_and_featurization(test_alc['smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preloader = DataLoader(train_dataset, batch_size=args['batch_size'],\n",
    "                             collate_fn=collate, shuffle=False)\n",
    "test_preloader = DataLoader(test_dataset, batch_size=args['batch_size'],\n",
    "                             collate_fn=collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gin_supervised_masking_pre_trained.pth from https://data.dgl.ai/dgllife/pre_trained/gin_supervised_masking.pth...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e90b6d3f9ce4803961f83dadcf89951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gin_supervised_masking_pre_trained.pth:   0%|          | 0.00/7.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:01, 18.15it/s]\n",
      "6it [00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 137.5318\n",
      "Epoch 1, Loss: 121.5152\n",
      "Epoch 2, Loss: 66.6739\n",
      "Epoch 3, Loss: 48.7676\n",
      "Epoch 4, Loss: 44.2514\n",
      "Epoch 5, Loss: 42.5255\n",
      "Epoch 6, Loss: 41.1153\n",
      "Epoch 7, Loss: 39.9270\n",
      "Epoch 8, Loss: 38.7852\n",
      "Epoch 9, Loss: 37.6807\n",
      "Epoch 10, Loss: 36.9888\n",
      "Epoch 11, Loss: 35.6272\n",
      "Epoch 12, Loss: 34.7508\n",
      "Epoch 13, Loss: 33.9030\n",
      "Epoch 14, Loss: 33.0634\n",
      "Epoch 15, Loss: 32.4630\n",
      "Epoch 16, Loss: 31.7320\n",
      "Epoch 17, Loss: 30.9437\n",
      "Epoch 18, Loss: 30.4970\n",
      "Epoch 19, Loss: 29.7131\n",
      "Epoch 20, Loss: 29.0241\n",
      "Epoch 21, Loss: 28.5882\n",
      "Epoch 22, Loss: 28.0253\n",
      "Epoch 23, Loss: 27.3701\n",
      "Epoch 24, Loss: 26.8970\n",
      "Epoch 25, Loss: 26.8466\n",
      "Epoch 26, Loss: 26.2393\n",
      "Epoch 27, Loss: 25.5648\n",
      "Epoch 28, Loss: 25.1493\n",
      "Epoch 29, Loss: 24.8814\n",
      "Epoch 30, Loss: 24.1900\n",
      "Epoch 31, Loss: 23.8637\n",
      "Epoch 32, Loss: 23.6587\n",
      "Epoch 33, Loss: 23.2250\n",
      "Epoch 34, Loss: 23.3641\n",
      "Epoch 35, Loss: 22.6768\n",
      "Epoch 36, Loss: 22.3339\n",
      "Epoch 37, Loss: 22.0280\n",
      "Epoch 38, Loss: 21.8413\n",
      "Epoch 39, Loss: 21.4737\n",
      "Epoch 40, Loss: 21.4144\n",
      "Epoch 41, Loss: 21.1396\n",
      "Epoch 42, Loss: 20.7003\n",
      "Epoch 43, Loss: 20.8054\n",
      "Epoch 44, Loss: 20.3659\n",
      "Epoch 45, Loss: 20.1931\n",
      "Epoch 46, Loss: 19.8122\n",
      "Epoch 47, Loss: 19.7501\n",
      "Epoch 48, Loss: 19.6755\n",
      "Epoch 49, Loss: 19.1232\n",
      "Epoch 50, Loss: 19.3827\n",
      "Epoch 51, Loss: 19.4089\n",
      "Epoch 52, Loss: 19.0103\n",
      "Epoch 53, Loss: 18.5508\n",
      "Epoch 54, Loss: 18.4203\n",
      "Epoch 55, Loss: 18.3085\n",
      "Epoch 56, Loss: 18.0432\n",
      "Epoch 57, Loss: 17.8022\n",
      "Epoch 58, Loss: 17.7394\n",
      "Epoch 59, Loss: 17.5902\n",
      "Epoch 60, Loss: 17.3432\n",
      "Epoch 61, Loss: 17.6295\n",
      "Epoch 62, Loss: 17.1030\n",
      "Epoch 63, Loss: 17.1252\n",
      "Epoch 64, Loss: 16.7770\n",
      "Epoch 65, Loss: 16.7924\n",
      "Epoch 66, Loss: 16.8336\n",
      "Epoch 67, Loss: 17.0176\n",
      "Epoch 68, Loss: 16.4617\n",
      "Epoch 69, Loss: 16.2348\n",
      "Epoch 70, Loss: 16.0729\n",
      "Epoch 71, Loss: 15.8746\n",
      "Epoch 72, Loss: 15.7031\n",
      "Epoch 73, Loss: 15.8988\n",
      "Epoch 74, Loss: 15.5160\n",
      "Epoch 75, Loss: 15.8022\n",
      "Epoch 76, Loss: 15.6674\n",
      "Epoch 77, Loss: 15.8850\n",
      "Epoch 78, Loss: 15.3814\n",
      "Epoch 79, Loss: 14.9391\n",
      "Epoch 80, Loss: 14.9063\n",
      "Epoch 81, Loss: 14.8807\n",
      "Epoch 82, Loss: 14.6933\n",
      "Epoch 83, Loss: 14.7383\n",
      "Epoch 84, Loss: 14.4285\n",
      "Epoch 85, Loss: 14.3763\n",
      "Epoch 86, Loss: 14.2202\n",
      "Epoch 87, Loss: 14.3288\n",
      "Epoch 88, Loss: 14.0536\n",
      "Epoch 89, Loss: 14.2071\n",
      "Epoch 90, Loss: 14.0445\n",
      "Epoch 91, Loss: 14.2879\n",
      "Epoch 92, Loss: 13.8928\n",
      "Epoch 93, Loss: 13.4690\n",
      "Epoch 94, Loss: 13.3991\n",
      "Epoch 95, Loss: 13.4458\n",
      "Epoch 96, Loss: 13.8311\n",
      "Epoch 97, Loss: 13.4245\n",
      "Epoch 98, Loss: 12.9941\n",
      "Epoch 99, Loss: 13.1335\n",
      "Epoch 100, Loss: 12.8091\n",
      "Epoch 101, Loss: 12.9932\n",
      "Epoch 102, Loss: 12.9946\n",
      "Epoch 103, Loss: 12.6303\n",
      "Epoch 104, Loss: 13.0398\n",
      "Epoch 105, Loss: 12.5953\n",
      "Epoch 106, Loss: 12.3424\n",
      "Epoch 107, Loss: 12.5572\n",
      "Epoch 108, Loss: 12.5542\n",
      "Epoch 109, Loss: 12.5307\n",
      "Epoch 110, Loss: 12.3169\n",
      "Epoch 111, Loss: 13.2907\n",
      "Epoch 112, Loss: 12.3806\n",
      "Epoch 113, Loss: 12.1161\n",
      "Epoch 114, Loss: 11.8883\n",
      "Epoch 115, Loss: 11.9678\n",
      "Epoch 116, Loss: 11.8676\n",
      "Epoch 117, Loss: 11.6002\n",
      "Epoch 118, Loss: 11.7907\n",
      "Epoch 119, Loss: 11.9253\n",
      "Epoch 120, Loss: 11.6796\n",
      "Epoch 121, Loss: 11.7125\n",
      "Epoch 122, Loss: 11.6109\n",
      "Epoch 123, Loss: 11.5416\n",
      "Epoch 124, Loss: 11.2562\n",
      "Epoch 125, Loss: 11.3056\n",
      "Epoch 126, Loss: 11.3879\n",
      "Epoch 127, Loss: 11.8312\n",
      "Epoch 128, Loss: 11.1839\n",
      "Epoch 129, Loss: 11.2574\n",
      "Epoch 130, Loss: 10.9930\n",
      "Epoch 131, Loss: 10.9110\n",
      "Epoch 132, Loss: 11.0013\n",
      "Epoch 133, Loss: 11.1696\n",
      "Epoch 134, Loss: 11.2181\n",
      "Epoch 135, Loss: 10.9443\n",
      "Epoch 136, Loss: 10.9045\n",
      "Epoch 137, Loss: 11.7109\n",
      "Epoch 138, Loss: 10.5964\n",
      "Epoch 139, Loss: 10.4952\n",
      "Epoch 140, Loss: 11.1816\n",
      "Epoch 141, Loss: 10.6601\n",
      "Epoch 142, Loss: 10.4529\n",
      "Epoch 143, Loss: 10.6673\n",
      "Epoch 144, Loss: 10.5254\n",
      "Epoch 145, Loss: 10.3052\n",
      "Epoch 146, Loss: 10.3226\n",
      "Epoch 147, Loss: 10.0903\n",
      "Epoch 148, Loss: 10.2723\n",
      "Epoch 149, Loss: 10.1607\n",
      "Epoch 150, Loss: 10.3256\n",
      "Epoch 151, Loss: 10.4392\n",
      "Epoch 152, Loss: 10.4632\n",
      "Epoch 153, Loss: 12.0894\n",
      "Epoch 154, Loss: 11.2292\n",
      "Epoch 155, Loss: 10.9542\n",
      "Epoch 156, Loss: 10.5181\n",
      "Epoch 157, Loss: 10.0519\n",
      "Epoch 158, Loss: 10.4248\n",
      "Epoch 159, Loss: 9.8955\n",
      "Epoch 160, Loss: 9.6718\n",
      "Epoch 161, Loss: 9.7902\n",
      "Epoch 162, Loss: 10.0589\n",
      "Epoch 163, Loss: 9.6389\n",
      "early stopping triggered\n",
      "the train rmse is 10.067578315734863\n",
      "the train mae is 4.48370361328125\n",
      "the test rmse is 17.493791580200195\n",
      "the test mae is 7.314727783203125\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'batch_size': 64,\n",
    "    'device': 'cpu',\n",
    "    'model': 'gin_supervised_masking',\n",
    "}\n",
    "model = load_pretrained(args['model']).to(args['device'])\n",
    "train_df = make_pretrained_datasets(model,train_preloader, train_alc)\n",
    "test_df = make_pretrained_datasets(model,test_preloader, test_alc)\n",
    "train_loader = DataLoader(train_df, batch_size=args['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_df, batch_size=args['batch_size'], shuffle=False)\n",
    "ft_model = FineTunedModel()\n",
    "learning_rate = 0.001\n",
    "# step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(ft_model.parameters(), lr=learning_rate)  \n",
    "optimizer = torch.optim.Adam(ft_model.parameters(), lr=0.001, \n",
    "                            #  weight_decay=1e-4\n",
    "                             )\n",
    "best_val_loss = float('inf')\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and loss\n",
    "    train(ft_model, train_loader,epoch, optimizer, criterion)\n",
    "    test_result = test(ft_model, test_loader)\n",
    "    test_loss = calculate_rmse(test_result[0], test_result[1])\n",
    "    if test_loss < best_val_loss:\n",
    "        best_val_loss = test_loss\n",
    "        torch.save(ft_model.state_dict(), 'best_model.pth')\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break\n",
    "ft_model.load_state_dict(torch.load('best_model.pth'))\n",
    "train_result = test(ft_model, train_loader)\n",
    "test_result = test(ft_model, test_loader)\n",
    "print(f'the train rmse is {calculate_rmse(train_result[0], train_result[1])}')\n",
    "print(f'the train mae is {calculate_mae(train_result[0], train_result[1])}')\n",
    "print(f'the test rmse is {calculate_rmse(test_result[0], test_result[1])}')\n",
    "print(f'the test mae is {calculate_mae(test_result[0], test_result[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets load some pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is attentivefp implementation in dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading AttentiveFP_Aromaticity_pre_trained.pth from https://data.dgl.ai/dgllife/pre_trained/attentivefp_aromaticity.pth...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad43089c88aa4535bdbe06e7f1bdfd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AttentiveFP_Aromaticity_pre_trained.pth:   0%|          | 0.00/4.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded\n"
     ]
    }
   ],
   "source": [
    "model = load_pretrained('AttentiveFP_Aromaticity') # Pretrained model loaded\n",
    "\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.predict = nn.Sequential(\n",
    "  nn.Dropout(p=0.2, inplace=False),\n",
    "  nn.Linear(in_features=200, out_features=1, bias=True)\n",
    ")\n",
    "model = model.to('cpu')\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, \n",
    "                            #  weight_decay=1e-4\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_feat = AttentiveFPAtomFeaturizer()\n",
    "bond_feat = AttentiveFPBondFeaturizer()\n",
    "\n",
    "def load_dataset(df, atom_feat, bond_feat):\n",
    "    smiles_to_g = SMILESToBigraph(add_self_loop=False, node_featurizer=atom_feat,\n",
    "                                  edge_featurizer=bond_feat)\n",
    "    dataset = MoleculeCSVDataset(df=df,\n",
    "                                 smiles_to_graph=smiles_to_g,\n",
    "                                 smiles_column='smiles',\n",
    "                                 cache_file_path= 'models/graph.bin',\n",
    "                                 task_names='rt',\n",
    "                                 n_jobs=6)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done 100 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done 1276 out of 1287 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 1287 out of 1287 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 100 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done 322 out of 322 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(train_alc, atom_feat, bond_feat)\n",
    "\n",
    "test_dataset = load_dataset(test_alc, atom_feat, bond_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_molgraphs(data):\n",
    "    \"\"\"Batching a list of datapoints for dataloader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list of 4-tuples.\n",
    "        Each tuple is for a single datapoint, consisting of\n",
    "        a SMILES, a DGLGraph, all-task labels and a binary\n",
    "        mask indicating the existence of labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    smiles : list\n",
    "        List of smiles\n",
    "    bg : DGLGraph\n",
    "        The batched DGLGraph.\n",
    "    labels : Tensor of dtype float32 and shape (B, T)\n",
    "        Batched datapoint labels. B is len(data) and\n",
    "        T is the number of total tasks.\n",
    "    masks : Tensor of dtype float32 and shape (B, T)\n",
    "        Batched datapoint binary mask, indicating the\n",
    "        existence of labels.\n",
    "    \"\"\"\n",
    "    smiles, graphs, labels, masks = map(list, zip(*data))\n",
    "\n",
    "    bg = dgl.batch(graphs)\n",
    "    bg.set_n_initializer(dgl.init.zero_initializer)\n",
    "    bg.set_e_initializer(dgl.init.zero_initializer)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "\n",
    "    if masks is None:\n",
    "        masks = torch.ones(labels.shape)\n",
    "    else:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "\n",
    "    return smiles, bg, labels, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True,\n",
    "                              collate_fn=collate_molgraphs)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64,shuffle=False,\n",
    "                            collate_fn=collate_molgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_id, batch_data in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles, bg, labels, masks=batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=3074, num_edges=6068,\n",
       "      ndata_schemes={'h': Scheme(shape=(39,), dtype=torch.float32)}\n",
       "      edata_schemes={'e': Scheme(shape=(10,), dtype=torch.float32)})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict( model, bg):\n",
    "    bg = bg.to('cpu')\n",
    "    \n",
    "    \n",
    "    node_feats = bg.ndata.pop('h').to('cpu')\n",
    "    edge_feats = bg.edata.pop('e').to('cpu')\n",
    "        # return model(bg, node_feats, edge_feats)\n",
    "    return model(bg, node_feats, edge_feats)\n",
    "def run_a_train_epoch( epoch, model, data_loader, loss_criterion, optimizer, scheduler = None):\n",
    "    model.train()\n",
    "    # train_meter = Meter()\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        batch_n = len(batch_data[0])\n",
    "        batch_loss = 0\n",
    "        smiles, bg, labels, masks = batch_data\n",
    "        if len(smiles) == 1:\n",
    "            # Avoid potential issues with batch normalization\n",
    "            continue\n",
    "\n",
    "        labels, masks = labels.to('cpu'), masks.to('cpu')\n",
    "        prediction = predict( model, bg)\n",
    "        loss = (loss_criterion(prediction, labels) )\n",
    "        total_loss += loss.item()*batch_n\n",
    "        total_items += batch_n\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "    median_loss = total_loss / total_items\n",
    "    print(f'the loss is {np.sqrt(median_loss)}')\n",
    "def run_an_eval_epoch(model, data_loader, loss_criterion, mode = 'Attentive_FP'):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    refs = []\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            smiles, bg, labels, masks = batch_data\n",
    "            refs.extend(labels)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                bg.to(torch.device('cpu'))\n",
    "                labels = labels.to('cpu')\n",
    "                masks = masks.to('cpu')\n",
    "            if mode == 'GAT' or mode == 'GCN':\n",
    "                prediction = model(bg, bg.ndata['h'])# this is GAT, only node feature required\n",
    "            else:\n",
    "                prediction = model(bg, bg.ndata['h'], bg.edata['e'])\n",
    "            predictions.extend(prediction)\n",
    "            loss = (loss_criterion(prediction, labels).float())\n",
    "            #loss = loss_criterion(prediction, labels)\n",
    "            losses.append(loss.data.item())\n",
    "    predictions= np.array([i.item() for i in predictions])\n",
    "    refs = np.array([i.item() for i in refs])\n",
    "    mse = loss_criterion(torch.tensor(predictions), torch.tensor(refs))\n",
    "    mae = np.median(abs(predictions-refs))\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading AttentiveFP_Aromaticity_pre_trained.pth from https://data.dgl.ai/dgllife/pre_trained/attentivefp_aromaticity.pth...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19892bbd4594e218a84a20e7d7639af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AttentiveFP_Aromaticity_pre_trained.pth:   0%|          | 0.00/4.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss is 134.29402565122504\n",
      "epoch 0, val_rmse 130.82691299024117\n",
      "the loss is 114.9535773867753\n",
      "epoch 1, val_rmse 100.04459448023646\n",
      "the loss is 85.38903845279766\n",
      "epoch 2, val_rmse 74.03280613766817\n",
      "the loss is 76.13608845337625\n",
      "epoch 3, val_rmse 74.56561863367233\n",
      "the loss is 76.04307481702618\n",
      "epoch 4, val_rmse 76.04688529704741\n",
      "the loss is 75.83854771989255\n",
      "epoch 5, val_rmse 75.61223313218494\n",
      "the loss is 75.89030984020361\n",
      "epoch 6, val_rmse 75.44847988426986\n",
      "the loss is 75.81546521506517\n",
      "epoch 7, val_rmse 75.38103272030811\n",
      "the loss is 75.85113120573428\n",
      "epoch 8, val_rmse 75.25285650868307\n",
      "the loss is 75.8690601282864\n",
      "epoch 9, val_rmse 74.97371120470247\n",
      "the loss is 75.83100844906646\n",
      "epoch 10, val_rmse 75.33707420944404\n",
      "the loss is 75.86019742180684\n",
      "epoch 11, val_rmse 74.7128143595593\n",
      "the loss is 75.94596913346864\n",
      "epoch 12, val_rmse 74.74965593365646\n",
      "early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "model = load_pretrained('AttentiveFP_Aromaticity') # Pretrained model loaded\n",
    "\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.predict = nn.Sequential(\n",
    "  nn.Linear(in_features=200, out_features=512,bias=True),\n",
    "  nn.ReLU(),\n",
    "\n",
    "\n",
    "  nn.Linear(in_features=512, out_features=1, bias=True)\n",
    ")\n",
    "model = model.to('cpu')\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, \n",
    "                            #  weight_decay=1e-4\n",
    "                             )\n",
    "max_epoch = 500\n",
    "best_val_loss = float('inf')\n",
    "# step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "for epoch in range(max_epoch):\n",
    "    \n",
    "    run_a_train_epoch(epoch, model, train_loader, criterion, optimizer)\n",
    "    val_rmse = run_an_eval_epoch(model, test_loader, criterion)\n",
    "    print(f'epoch {epoch}, val_rmse {val_rmse}')\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(model.state_dict(), 'best_pretrained.pth')\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss is 19180.439416703088\n",
      "epoch 0, val_rmse 142.6000648394769\n",
      "the loss is 18831.88536658654\n",
      "epoch 1, val_rmse 141.26483822911078\n",
      "the loss is 18488.862283441384\n",
      "epoch 2, val_rmse 139.9487519517861\n",
      "the loss is 18154.909683144546\n",
      "epoch 3, val_rmse 138.6516341838341\n",
      "the loss is 17837.873118960033\n",
      "epoch 4, val_rmse 137.40029579893522\n",
      "the loss is 17514.79140564297\n",
      "epoch 5, val_rmse 136.141143988239\n",
      "the loss is 17199.248628108002\n",
      "epoch 6, val_rmse 134.85920208219378\n",
      "the loss is 16886.659329169095\n",
      "epoch 7, val_rmse 133.5840082559893\n",
      "the loss is 16587.514324434247\n",
      "epoch 8, val_rmse 132.33844976825915\n",
      "the loss is 16295.920103984557\n",
      "epoch 9, val_rmse 131.13530411652042\n",
      "the loss is 16002.182224893162\n",
      "epoch 10, val_rmse 129.88643915043139\n",
      "the loss is 15710.008053795164\n",
      "epoch 11, val_rmse 128.70046735102142\n",
      "the loss is 15436.68330237471\n",
      "epoch 12, val_rmse 127.51026688603586\n",
      "the loss is 15172.164575441919\n",
      "epoch 13, val_rmse 126.34515688619975\n",
      "the loss is 14896.167782087705\n",
      "epoch 14, val_rmse 125.1714257889514\n",
      "the loss is 14631.849808177933\n",
      "epoch 15, val_rmse 124.04070467922624\n",
      "the loss is 14380.43520238442\n",
      "epoch 16, val_rmse 122.91242295393748\n",
      "the loss is 14133.190431963869\n",
      "epoch 17, val_rmse 121.79382415220715\n",
      "the loss is 13898.962869189187\n",
      "epoch 18, val_rmse 120.66998831625615\n",
      "the loss is 13641.18702955031\n",
      "epoch 19, val_rmse 119.61501396953264\n",
      "the loss is 13427.924646859217\n",
      "epoch 20, val_rmse 118.52020301383513\n",
      "the loss is 13176.804669289044\n",
      "epoch 21, val_rmse 117.45339691527218\n",
      "the loss is 12974.721659200175\n",
      "epoch 22, val_rmse 116.43348167301556\n",
      "the loss is 12763.711841977467\n",
      "epoch 23, val_rmse 115.42016463178754\n",
      "the loss is 12544.715800583965\n",
      "epoch 24, val_rmse 114.37703524266522\n",
      "the loss is 12357.981599346835\n",
      "epoch 25, val_rmse 113.39223372584725\n",
      "the loss is 12170.84893025811\n",
      "epoch 26, val_rmse 112.4284451137996\n",
      "the loss is 11936.760726252915\n",
      "epoch 27, val_rmse 111.47224001634099\n",
      "the loss is 11766.170764131703\n",
      "epoch 28, val_rmse 110.5156527739973\n",
      "the loss is 11572.61398275119\n",
      "epoch 29, val_rmse 109.53701548097597\n",
      "the loss is 11384.541036112325\n",
      "epoch 30, val_rmse 108.62418326211012\n",
      "the loss is 11229.571424127089\n",
      "epoch 31, val_rmse 107.71319383234155\n",
      "the loss is 11055.03809124903\n",
      "epoch 32, val_rmse 106.81607182978345\n",
      "the loss is 10859.440626972853\n",
      "epoch 33, val_rmse 105.95245643696597\n",
      "the loss is 10702.29513661252\n",
      "epoch 34, val_rmse 105.11271154521683\n",
      "the loss is 10554.608585858587\n",
      "epoch 35, val_rmse 104.26194204646877\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epoch):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mrun_a_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     val_rmse \u001b[38;5;241m=\u001b[39m run_an_eval_epoch(model, test_loader, criterion)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val_rmse \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_rmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[68], line 23\u001b[0m, in \u001b[0;36mrun_a_train_epoch\u001b[0;34m(epoch, model, data_loader, loss_criterion, optimizer)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     22\u001b[0m labels, masks \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), masks\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m (loss_criterion(prediction, labels) )\n\u001b[1;32m     25\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mbatch_n\n",
      "Cell \u001b[0;32mIn[68], line 8\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, bg)\u001b[0m\n\u001b[1;32m      6\u001b[0m edge_feats \u001b[38;5;241m=\u001b[39m bg\u001b[38;5;241m.\u001b[39medata\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# return model(bg, node_feats, edge_feats)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/dgllife/model/model_zoo/attentivefp_predictor.py:87\u001b[0m, in \u001b[0;36mAttentiveFPPredictor.forward\u001b[0;34m(self, g, node_feats, edge_feats, get_node_weight)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, g, node_feats, edge_feats, get_node_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Graph-level regression/soft classification.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m        gives the node weights in the i-th update.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     node_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m get_node_weight:\n\u001b[1;32m     89\u001b[0m         g_feats, node_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadout(g, node_feats, get_node_weight)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/dgllife/model/gnn/attentivefp.py:359\u001b[0m, in \u001b[0;36mAttentiveFPGNN.forward\u001b[0;34m(self, g, node_feats, edge_feats)\u001b[0m\n\u001b[1;32m    357\u001b[0m node_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_context(g, node_feats, edge_feats)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gnn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn_layers:\n\u001b[0;32m--> 359\u001b[0m     node_feats \u001b[38;5;241m=\u001b[39m \u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node_feats\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/dgllife/model/gnn/attentivefp.py:298\u001b[0m, in \u001b[0;36mGNNLayer.forward\u001b[0;34m(self, g, node_feats)\u001b[0m\n\u001b[1;32m    296\u001b[0m g\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m node_feats\n\u001b[1;32m    297\u001b[0m g\u001b[38;5;241m.\u001b[39mapply_edges(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_edges)\n\u001b[0;32m--> 298\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattentive_gru(g, logits, node_feats)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_epoch = 500\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(max_epoch):\n",
    "    \n",
    "    run_a_train_epoch(epoch, model, train_loader, criterion, optimizer)\n",
    "    val_rmse = run_an_eval_epoch(model, test_loader, criterion)\n",
    "    print(f'epoch {epoch}, val_rmse {val_rmse}')\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience = 10\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('early stopping triggered')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done 100 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done 1276 out of 1287 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 1287 out of 1287 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 100 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done 322 out of 322 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "train_df = load_dataset(train_alc)\n",
    "test_df = load_dataset(test_alc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_name = 'AttentiveFPPredictor'\n",
    "exp_config = du.get_configure(Model_name)\n",
    "exp_config['model']=Model_name\n",
    "in_node_features = CanonicalAtomFeaturizer().feat_size()\n",
    "in_edge_features = CanonicalBondFeaturizer(self_loop=True).feat_size()\n",
    "exp_config['in_node_feats']=in_node_features\n",
    "exp_config['in_edge_feats']=in_edge_features\n",
    "exp_config['n_tasks'] = 1\n",
    "args = {\n",
    "    'device':'cpu',\n",
    "    'node_featurizer_type':'CanonicalAtomFeaturizer',\n",
    "    'edge_featurizer':'CanonicalBondFeaturizer',\n",
    "    'bond_featurizer_type':'CanonicalBondFeaturizer',\n",
    "    'num_epochs':100,\n",
    "    'metric':'rmse'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_name = 'GAT'\n",
    "exp_config = du.get_configure(Model_name)\n",
    "exp_config['model']=Model_name\n",
    "in_node_features = CanonicalAtomFeaturizer().feat_size()\n",
    "in_edge_features = CanonicalBondFeaturizer(self_loop=True).feat_size()\n",
    "exp_config['in_node_feats']=in_node_features\n",
    "# exp_config['in_edge_feats']=in_edge_features\n",
    "exp_config['n_tasks'] = 1\n",
    "args = {\n",
    "    'device':'cpu',\n",
    "    'node_featurizer_type':'CanonicalAtomFeaturizer',\n",
    "    'edge_featurizer':'CanonicalBondFeaturizer',\n",
    "    'bond_featurizer_type':'CanonicalBondFeaturizer',\n",
    "    'num_epochs':100,\n",
    "    'metric':'rmse'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_df, batch_size=64, shuffle=True,\n",
    "                              collate_fn=du.collate_molgraphs, drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_df, batch_size=64, shuffle=False,\n",
    "                              collate_fn=du.collate_molgraphs, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For metric rmse, the lower the better\n"
     ]
    }
   ],
   "source": [
    "Model_name = 'GCN'\n",
    "exp_config = du.get_configure(Model_name)\n",
    "exp_config['model']=Model_name\n",
    "in_node_features = CanonicalAtomFeaturizer().feat_size()\n",
    "in_edge_features = CanonicalBondFeaturizer(self_loop=True).feat_size()\n",
    "exp_config['in_node_feats']=in_node_features\n",
    "exp_config['in_edge_feats']=in_edge_features\n",
    "exp_config['n_tasks'] = 1\n",
    "args = {\n",
    "    'device':'cpu',\n",
    "    'node_featurizer_type':'CanonicalAtomFeaturizer',\n",
    "    'edge_featurizer':'CanonicalBondFeaturizer',\n",
    "    'bond_featurizer_type':'CanonicalBondFeaturizer',\n",
    "    'num_epochs':100,\n",
    "    'metric':'rmse'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model = du.load_model(exp_config).to('cpu')\n",
    "loss_criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), \n",
    "                #  lr=exp_config['lr'],\n",
    "                lr = 0.001,\n",
    "                     weight_decay=exp_config['weight_decay'])\n",
    "stopper = EarlyStopping(patience=exp_config['patience'],\n",
    "                        filename='models' + '/model.pth',\n",
    "                        metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100, training rmse 137.3330\n",
      "epoch 2/100, training rmse 135.4787\n",
      "epoch 3/100, training rmse 133.8532\n",
      "epoch 4/100, training rmse 131.9603\n",
      "epoch 5/100, training rmse 130.1400\n",
      "epoch 6/100, training rmse 127.9735\n",
      "epoch 7/100, training rmse 125.8501\n",
      "epoch 8/100, training rmse 122.8770\n",
      "epoch 9/100, training rmse 120.4410\n",
      "epoch 10/100, training rmse 117.6438\n",
      "epoch 11/100, training rmse 114.9163\n",
      "epoch 12/100, training rmse 111.5598\n",
      "epoch 13/100, training rmse 108.8347\n",
      "epoch 14/100, training rmse 105.2403\n",
      "epoch 15/100, training rmse 102.1658\n",
      "epoch 16/100, training rmse 99.0732\n",
      "epoch 17/100, training rmse 95.5528\n",
      "epoch 18/100, training rmse 92.1718\n",
      "epoch 19/100, training rmse 88.9633\n",
      "epoch 20/100, training rmse 85.9761\n",
      "epoch 21/100, training rmse 82.2844\n",
      "epoch 22/100, training rmse 78.6935\n",
      "epoch 23/100, training rmse 76.5099\n",
      "epoch 24/100, training rmse 73.1654\n",
      "epoch 25/100, training rmse 69.9619\n",
      "epoch 26/100, training rmse 67.3938\n",
      "epoch 27/100, training rmse 63.8981\n",
      "epoch 28/100, training rmse 60.9251\n",
      "epoch 29/100, training rmse 57.8957\n",
      "epoch 30/100, training rmse 54.6543\n",
      "epoch 31/100, training rmse 52.1365\n",
      "epoch 32/100, training rmse 49.0961\n",
      "epoch 33/100, training rmse 46.6023\n",
      "epoch 34/100, training rmse 43.6201\n",
      "epoch 35/100, training rmse 41.1584\n",
      "epoch 36/100, training rmse 38.5478\n",
      "epoch 37/100, training rmse 36.4258\n",
      "epoch 38/100, training rmse 32.8397\n",
      "epoch 39/100, training rmse 32.0097\n",
      "epoch 40/100, training rmse 29.9303\n",
      "epoch 41/100, training rmse 27.4780\n",
      "epoch 42/100, training rmse 25.4146\n",
      "epoch 43/100, training rmse 25.2617\n",
      "epoch 44/100, training rmse 22.4840\n",
      "epoch 45/100, training rmse 22.6999\n",
      "epoch 46/100, training rmse 20.4344\n",
      "epoch 47/100, training rmse 20.5895\n",
      "epoch 48/100, training rmse 18.9285\n",
      "epoch 49/100, training rmse 18.6017\n",
      "epoch 50/100, training rmse 17.6698\n",
      "epoch 51/100, training rmse 19.4968\n",
      "epoch 52/100, training rmse 18.1347\n",
      "epoch 53/100, training rmse 17.7162\n",
      "epoch 54/100, training rmse 16.6129\n",
      "epoch 55/100, training rmse 15.3982\n",
      "epoch 56/100, training rmse 14.5867\n",
      "epoch 57/100, training rmse 15.8477\n",
      "epoch 58/100, training rmse 16.2553\n",
      "epoch 59/100, training rmse 15.5524\n",
      "epoch 60/100, training rmse 16.4705\n",
      "epoch 61/100, training rmse 15.3702\n",
      "epoch 62/100, training rmse 15.7059\n",
      "epoch 63/100, training rmse 18.7337\n",
      "epoch 64/100, training rmse 14.7319\n",
      "epoch 65/100, training rmse 16.6862\n",
      "epoch 66/100, training rmse 15.8132\n",
      "epoch 67/100, training rmse 14.9155\n",
      "epoch 68/100, training rmse 18.3060\n",
      "epoch 69/100, training rmse 14.6499\n",
      "epoch 70/100, training rmse 14.8230\n",
      "epoch 71/100, training rmse 16.7924\n",
      "epoch 72/100, training rmse 16.1511\n",
      "epoch 73/100, training rmse 17.6766\n",
      "epoch 74/100, training rmse 15.7990\n",
      "epoch 75/100, training rmse 17.5442\n",
      "epoch 76/100, training rmse 15.2207\n",
      "epoch 77/100, training rmse 15.8019\n",
      "epoch 78/100, training rmse 16.8247\n",
      "epoch 79/100, training rmse 15.7957\n",
      "epoch 80/100, training rmse 16.0732\n",
      "epoch 81/100, training rmse 15.2214\n",
      "epoch 82/100, training rmse 13.3618\n",
      "epoch 83/100, training rmse 13.7102\n",
      "epoch 84/100, training rmse 14.9888\n",
      "epoch 85/100, training rmse 16.0338\n",
      "epoch 86/100, training rmse 15.9612\n",
      "epoch 87/100, training rmse 14.7079\n",
      "epoch 88/100, training rmse 16.9962\n",
      "epoch 89/100, training rmse 15.8324\n",
      "epoch 90/100, training rmse 15.8592\n",
      "epoch 91/100, training rmse 13.4893\n",
      "epoch 92/100, training rmse 13.3030\n",
      "epoch 93/100, training rmse 16.3102\n",
      "epoch 94/100, training rmse 13.7857\n",
      "epoch 95/100, training rmse 16.5431\n",
      "epoch 96/100, training rmse 14.3653\n",
      "epoch 97/100, training rmse 14.6689\n",
      "epoch 98/100, training rmse 17.0669\n",
      "epoch 99/100, training rmse 15.5027\n",
      "epoch 100/100, training rmse 14.9715\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args['num_epochs']):\n",
    "        # Train\n",
    "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer, mode='GAT')\n",
    "\n",
    "        # Validation and early stop\n",
    "        # val_score = run_an_eval_epoch(args, model, val_loader)\n",
    "        # early_stop = stopper.step(val_score, model)\n",
    "        # print('epoch {:d}/{:d}, validation {} {:.4f}, best validation {} {:.4f}'.format(\n",
    "        #     epoch + 1, args['num_epochs'], args['metric'], val_score,\n",
    "        #     args['metric'], stopper.best_score))\n",
    "\n",
    "        # if early_stop:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_afp_data(data, smiles_col = 'smiles', target_col = 'rt'):\n",
    "\n",
    "#     train_mols = [Chem.MolFromSmiles(s) for s in data[smiles_col]]\n",
    "#     train_graph = [mol_to_bigraph(mol,\n",
    "#                                 node_featurizer=AttentiveFPAtomFeaturizer(),\n",
    "#                                 edge_featurizer=AttentiveFPBondFeaturizer()) for mol in train_mols]\n",
    "#     train_y = torch.tensor(data[target_col].values, dtype=torch.float32).reshape(-1, 1)\n",
    "#     train_smiles = train_alc[smiles_col].values\n",
    "#     train_data = list(zip(train_smiles, train_graph, train_y))\n",
    "#     return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = make_afp_data(train_alc)\n",
    "# test_data = make_afp_data(test_alc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rmse is 15.000260697213859, the mae is 7.7477264404296875\n"
     ]
    }
   ],
   "source": [
    "result = run_an_eval_epoch(model, test_loader, loss_criterion, mode = 'GCN')\n",
    "print(f'the rmse is {result[0]}, the mae is {result[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rmse is 11.735922605798882, the mae is 6.880561828613281\n"
     ]
    }
   ],
   "source": [
    "result = run_an_eval_epoch(model, train_loader, loss_criterion, mode = 'GCN')\n",
    "print(f'the rmse is {result[0]}, the mae is {result[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=64, collate_fn=collate_molgraphs)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, collate_fn=collate_molgraphs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100, training rmse: 132.1964\n",
      "epoch 2/100, training rmse: 108.2541\n",
      "epoch 3/100, training rmse: 59.1010\n",
      "epoch 4/100, training rmse: 45.2814\n",
      "epoch 5/100, training rmse: 43.4932\n",
      "epoch 6/100, training rmse: 42.5811\n",
      "epoch 7/100, training rmse: 41.5329\n",
      "epoch 8/100, training rmse: 40.4257\n",
      "epoch 9/100, training rmse: 39.1514\n",
      "epoch 10/100, training rmse: 37.1230\n",
      "epoch 11/100, training rmse: 34.5917\n",
      "epoch 12/100, training rmse: 31.7194\n",
      "epoch 13/100, training rmse: 28.7641\n",
      "epoch 14/100, training rmse: 27.3798\n",
      "epoch 15/100, training rmse: 26.0786\n",
      "epoch 16/100, training rmse: 25.0581\n",
      "epoch 17/100, training rmse: 23.9964\n",
      "epoch 18/100, training rmse: 23.3594\n",
      "epoch 19/100, training rmse: 22.8261\n",
      "epoch 20/100, training rmse: 22.6727\n",
      "epoch 21/100, training rmse: 22.6882\n",
      "epoch 22/100, training rmse: 22.2658\n",
      "epoch 23/100, training rmse: 21.8008\n",
      "epoch 24/100, training rmse: 21.1478\n",
      "epoch 25/100, training rmse: 20.5799\n",
      "epoch 26/100, training rmse: 20.2989\n",
      "epoch 27/100, training rmse: 20.1153\n",
      "epoch 28/100, training rmse: 19.8662\n",
      "epoch 29/100, training rmse: 19.6184\n",
      "epoch 30/100, training rmse: 19.3803\n",
      "epoch 31/100, training rmse: 19.2294\n",
      "epoch 32/100, training rmse: 19.0845\n",
      "epoch 33/100, training rmse: 18.9706\n",
      "epoch 34/100, training rmse: 18.8499\n",
      "epoch 35/100, training rmse: 18.7393\n",
      "epoch 36/100, training rmse: 18.6401\n",
      "epoch 37/100, training rmse: 18.5419\n",
      "epoch 38/100, training rmse: 18.4567\n",
      "epoch 39/100, training rmse: 18.3880\n",
      "epoch 40/100, training rmse: 18.3421\n",
      "epoch 41/100, training rmse: 18.5227\n",
      "epoch 42/100, training rmse: 18.4084\n",
      "epoch 43/100, training rmse: 18.2462\n",
      "epoch 44/100, training rmse: 18.1351\n",
      "epoch 45/100, training rmse: 17.9829\n",
      "epoch 46/100, training rmse: 17.8862\n",
      "epoch 47/100, training rmse: 17.7947\n",
      "epoch 48/100, training rmse: 17.7399\n",
      "epoch 49/100, training rmse: 17.6343\n",
      "epoch 50/100, training rmse: 17.5660\n",
      "epoch 51/100, training rmse: 17.4765\n",
      "epoch 52/100, training rmse: 17.4226\n",
      "epoch 53/100, training rmse: 17.3522\n",
      "epoch 54/100, training rmse: 17.2792\n",
      "epoch 55/100, training rmse: 17.2395\n",
      "epoch 56/100, training rmse: 17.2062\n",
      "epoch 57/100, training rmse: 17.1595\n",
      "epoch 58/100, training rmse: 17.0987\n",
      "epoch 59/100, training rmse: 17.0591\n",
      "epoch 60/100, training rmse: 16.9830\n",
      "epoch 61/100, training rmse: 16.9275\n",
      "epoch 62/100, training rmse: 16.8477\n",
      "epoch 63/100, training rmse: 16.7585\n",
      "epoch 64/100, training rmse: 16.7254\n",
      "epoch 65/100, training rmse: 16.7110\n",
      "epoch 66/100, training rmse: 16.6938\n",
      "epoch 67/100, training rmse: 16.5621\n",
      "epoch 68/100, training rmse: 16.4544\n",
      "epoch 69/100, training rmse: 16.3810\n",
      "epoch 70/100, training rmse: 16.3466\n",
      "epoch 71/100, training rmse: 16.3317\n",
      "epoch 72/100, training rmse: 16.3141\n",
      "epoch 73/100, training rmse: 16.2348\n",
      "epoch 74/100, training rmse: 16.1013\n",
      "epoch 75/100, training rmse: 16.0574\n",
      "epoch 76/100, training rmse: 15.9942\n",
      "epoch 77/100, training rmse: 15.9636\n",
      "epoch 78/100, training rmse: 15.8944\n",
      "epoch 79/100, training rmse: 15.8372\n",
      "epoch 80/100, training rmse: 15.7714\n",
      "epoch 81/100, training rmse: 15.7221\n",
      "epoch 82/100, training rmse: 15.6590\n",
      "epoch 83/100, training rmse: 15.6204\n",
      "epoch 84/100, training rmse: 15.5693\n",
      "epoch 85/100, training rmse: 15.5444\n",
      "epoch 86/100, training rmse: 15.5665\n",
      "epoch 87/100, training rmse: 15.4191\n",
      "epoch 88/100, training rmse: 15.3918\n",
      "epoch 89/100, training rmse: 15.2850\n",
      "epoch 90/100, training rmse: 15.3281\n",
      "epoch 91/100, training rmse: 15.2588\n",
      "epoch 92/100, training rmse: 15.2347\n",
      "epoch 93/100, training rmse: 15.0826\n",
      "epoch 94/100, training rmse: 15.1389\n",
      "epoch 95/100, training rmse: 15.0179\n",
      "epoch 96/100, training rmse: 15.0597\n",
      "epoch 97/100, training rmse: 14.9342\n",
      "epoch 98/100, training rmse: 14.9509\n",
      "epoch 99/100, training rmse: 14.8681\n",
      "epoch 100/100, training rmse: 14.8845\n"
     ]
    }
   ],
   "source": [
    "model = AttentiveFPPredictor(node_feat_size=39,\n",
    "                                  edge_feat_size=10,\n",
    "                                  num_layers=2,\n",
    "                                  num_timesteps=2,\n",
    "                                  graph_feat_size=256,\n",
    "                                  n_tasks=1,\n",
    "                                  dropout=0.0)\n",
    "model = model.to('cpu')\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0003126662000605776,)\n",
    "n_epochs = 100\n",
    "epochs = []\n",
    "scores = []\n",
    "for e in range(n_epochs):\n",
    "    score = run_a_train_epoch(n_epochs, e, model, train_loader, loss_fn, optimizer)\n",
    "    epochs.append(e)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 14.62560367150402, MAE: 5.33612060546875\n"
     ]
    }
   ],
   "source": [
    "rmse, mae, preds, refs = run_an_eval_epoch(model, train_loader, loss_fn)\n",
    "print(f'training RMSE: {rmse}, MAE: {mae}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
